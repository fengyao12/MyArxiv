{"2025-04-21T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2504.15280v1","updated":"2025-04-21T17:59:53Z","published":"2025-04-21T17:59:53Z","title":"Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs","summary":"  Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.\n","authors":["Chun-Hsiao Yeh","Chenyu Wang","Shengbang Tong","Ta-Ying Cheng","Rouyu Wang","Tianzhe Chu","Yuexiang Zhai","Yubei Chen","Shenghua Gao","Yi Ma"],"pdf_url":"https://arxiv.org/pdf/2504.15280v1.pdf","comment":"Project page: https://danielchyeh.github.io/All-Angles-Bench/"},{"id":"http://arxiv.org/abs/2504.15270v1","updated":"2025-04-21T17:57:21Z","published":"2025-04-21T17:57:21Z","title":"An LMM for Efficient Video Understanding via Reinforced Compression of\n  Video Cubes","summary":"  Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present \\textbf{Quicksviewer}, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.\n","authors":["Ji Qi","Yuan Yao","Yushi Bai","Bin Xu","Juanzi Li","Zhiyuan Liu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2504.15270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11794v4","updated":"2025-04-21T17:48:15Z","published":"2024-06-17T17:42:57Z","title":"DataComp-LM: In search of the next generation of training sets for\n  language models","summary":"  We introduce DataComp for Language Models (DCLM), a testbed for controlled\ndataset experiments with the goal of improving language models. As part of\nDCLM, we provide a standardized corpus of 240T tokens extracted from Common\nCrawl, effective pretraining recipes based on the OpenLM framework, and a broad\nsuite of 53 downstream evaluations. Participants in the DCLM benchmark can\nexperiment with data curation strategies such as deduplication, filtering, and\ndata mixing at model scales ranging from 412M to 7B parameters. As a baseline\nfor DCLM, we conduct extensive experiments and find that model-based filtering\nis key to assembling a high-quality training set. The resulting dataset,\nDCLM-Baseline enables training a 7B parameter language model from scratch to\n64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the\nprevious state-of-the-art in open-data language models, DCLM-Baseline\nrepresents a 6.6 percentage point improvement on MMLU while being trained with\n40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and\nLlama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53\nnatural language understanding tasks while being trained with 6.6x less compute\nthan Llama 3 8B. Our results highlight the importance of dataset design for\ntraining language models and offer a starting point for further research on\ndata curation.\n","authors":["Jeffrey Li","Alex Fang","Georgios Smyrnis","Maor Ivgi","Matt Jordan","Samir Gadre","Hritik Bansal","Etash Guha","Sedrick Keh","Kushal Arora","Saurabh Garg","Rui Xin","Niklas Muennighoff","Reinhard Heckel","Jean Mercat","Mayee Chen","Suchin Gururangan","Mitchell Wortsman","Alon Albalak","Yonatan Bitton","Marianna Nezhurina","Amro Abbas","Cheng-Yu Hsieh","Dhruba Ghosh","Josh Gardner","Maciej Kilian","Hanlin Zhang","Rulin Shao","Sarah Pratt","Sunny Sanyal","Gabriel Ilharco","Giannis Daras","Kalyani Marathe","Aaron Gokaslan","Jieyu Zhang","Khyathi Chandu","Thao Nguyen","Igor Vasiljevic","Sham Kakade","Shuran Song","Sujay Sanghavi","Fartash Faghri","Sewoong Oh","Luke Zettlemoyer","Kyle Lo","Alaaeldin El-Nouby","Hadi Pouransari","Alexander Toshev","Stephanie Wang","Dirk Groeneveld","Luca Soldaini","Pang Wei Koh","Jenia Jitsev","Thomas Kollar","Alexandros G. Dimakis","Yair Carmon","Achal Dave","Ludwig Schmidt","Vaishaal Shankar"],"pdf_url":"https://arxiv.org/pdf/2406.11794v4.pdf","comment":"Project page: https://www.datacomp.ai/dclm/"},{"id":"http://arxiv.org/abs/2504.15266v1","updated":"2025-04-21T17:47:46Z","published":"2025-04-21T17:47:46Z","title":"Roll the dice & look before you leap: Going beyond the creative limits\n  of next-token prediction","summary":"  We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\ncomparatively, multi-token approaches, namely teacherless training and\ndiffusion models, excel in producing diverse and original output. Secondly, in\nour tasks, we find that to elicit randomness from the Transformer without\nhurting coherence, it is better to inject noise right at the input layer (via a\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\nthe output layer. Thus, our work offers a principled, minimal test-bed for\nanalyzing open-ended creative skills, and offers new arguments for going beyond\nnext-token learning and softmax-based sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity\n","authors":["Vaishnavh Nagarajan","Chen Henry Wu","Charles Ding","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2504.15266v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2504.15254v1","updated":"2025-04-21T17:33:33Z","published":"2025-04-21T17:33:33Z","title":"CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation","summary":"  C-to-Rust transpilation is essential for modernizing legacy C code while\nenhancing safety and interoperability with modern Rust ecosystems. However, no\ndataset currently exists for evaluating whether a system can transpile C into\nsafe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset\nof 100 C repositories, each paired with manually-written interfaces in safe\nRust as well as test cases that can be used to validate correctness of the\ntranspilation. By considering entire repositories rather than isolated\nfunctions, CRUST-Bench captures the challenges of translating complex projects\nwith dependencies across multiple files. The provided Rust interfaces provide\nexplicit specifications that ensure adherence to idiomatic, memory-safe Rust\npatterns, while the accompanying test cases enforce functional correctness. We\nevaluate state-of-the-art large language models (LLMs) on this task and find\nthat safe and idiomatic Rust generation is still a challenging problem for\nvarious state-of-the-art methods and techniques. We also provide insights into\nthe errors LLMs usually make in transpiling code from C to safe Rust. The best\nperforming model, OpenAI o1, is able to solve only 15 tasks in a single-shot\nsetting. Improvements on CRUST-Bench would lead to improved transpilation\nsystems that can reason about complex scenarios and help in migrating legacy\ncodebases from C into languages like Rust that ensure memory safety. You can\nfind the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.\n","authors":["Anirudh Khatry","Robert Zhang","Jia Pan","Ziteng Wang","Qiaochu Chen","Greg Durrett","Isil Dillig"],"pdf_url":"https://arxiv.org/pdf/2504.15254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15253v1","updated":"2025-04-21T17:33:23Z","published":"2025-04-21T17:33:23Z","title":"Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\n  Test-Time Scaling Evaluators","summary":"  Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses.\n","authors":["Yilun Zhou","Austin Xu","Peifeng Wang","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2504.15253v1.pdf","comment":"The first two authors contributed equally. The codebase is at\n  https://github.com/SalesforceAIResearch/jetts-benchmark"},{"id":"http://arxiv.org/abs/2502.05291v2","updated":"2025-04-21T17:30:23Z","published":"2025-02-07T19:50:02Z","title":"Can LLMs Rank the Harmfulness of Smaller LLMs? We are Not There Yet","summary":"  Large language models (LLMs) have become ubiquitous, thus it is important to\nunderstand their risks and limitations. Smaller LLMs can be deployed where\ncompute resources are constrained, such as edge devices, but with different\npropensity to generate harmful output. Mitigation of LLM harm typically depends\non annotating the harmfulness of LLM output, which is expensive to collect from\nhumans. This work studies two questions: How do smaller LLMs rank regarding\ngeneration of harmful content? How well can larger LLMs annotate harmfulness?\nWe prompt three small LLMs to elicit harmful content of various types, such as\ndiscriminatory language, offensive content, privacy invasion, or negative\ninfluence, and collect human rankings of their outputs. Then, we evaluate three\nstate-of-the-art large LLMs on their ability to annotate the harmfulness of\nthese responses. We find that the smaller models differ with respect to\nharmfulness. We also find that large LLMs show low to moderate agreement with\nhumans. These findings underline the need for further work on harm mitigation\nin LLMs.\n","authors":["Berk Atil","Vipul Gupta","Sarkar Snigdha Sarathi Das","Rebecca J. Passonneau"],"pdf_url":"https://arxiv.org/pdf/2502.05291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15241v1","updated":"2025-04-21T17:15:06Z","published":"2025-04-21T17:15:06Z","title":"MR. Guard: Multilingual Reasoning Guardrail using Curriculum Learning","summary":"  Large Language Models (LLMs) are susceptible to adversarial attacks such as\njailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability\nis exacerbated in multilingual setting, where multilingual safety-aligned data\nare often limited. Thus, developing a guardrail capable of detecting and\nfiltering unsafe content across diverse languages is critical for deploying\nLLMs in real-world applications. In this work, we propose an approach to build\na multilingual guardrail with reasoning. Our method consists of: (1) synthetic\nmultilingual data generation incorporating culturally and linguistically\nnuanced variants, (2) supervised fine-tuning, and (3) a curriculum-guided Group\nRelative Policy Optimization (GRPO) framework that further improves\nperformance. Experimental results demonstrate that our multilingual guardrail\nconsistently outperforms recent baselines across both in-domain and\nout-of-domain languages. The multilingual reasoning capability of our guardrail\nenables it to generate multilingual explanations, which are particularly useful\nfor understanding language-specific risks and ambiguities in multilingual\ncontent moderation.\n","authors":["Yahan Yang","Soham Dan","Shuo Li","Dan Roth","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2504.15241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15236v1","updated":"2025-04-21T17:13:16Z","published":"2025-04-21T17:13:16Z","title":"Values in the Wild: Discovering and Analyzing Values in Real-World\n  Language Model Interactions","summary":"  AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems.\n","authors":["Saffron Huang","Esin Durmus","Miles McCain","Kunal Handa","Alex Tamkin","Jerry Hong","Michael Stern","Arushi Somani","Xiuruo Zhang","Deep Ganguli"],"pdf_url":"https://arxiv.org/pdf/2504.15236v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2504.15220v1","updated":"2025-04-21T16:46:07Z","published":"2025-04-21T16:46:07Z","title":"Fully Bayesian Approaches to Topics over Time","summary":"  The Topics over Time (ToT) model captures thematic changes in timestamped\ndatasets by explicitly modeling publication dates jointly with word\nco-occurrence patterns. However, ToT was not approached in a fully Bayesian\nfashion, a flaw that makes it susceptible to stability problems. To address\nthis issue, we propose a fully Bayesian Topics over Time (BToT) model via the\nintroduction of a conjugate prior to the Beta distribution. This prior acts as\na regularization that prevents the online version of the algorithm from\nunstable updates when a topic is poorly represented in a mini-batch. The\ncharacteristics of this prior to the Beta distribution are studied here for the\nfirst time. Still, this model suffers from a difference in scale between the\nsingle-time observations and the multiplicity of words per document. A\nvariation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a\nsolution. In WBToT, publication dates are repeated a certain number of times\nper document, which balances the relative influence of words and timestamps\nalong the inference process. We have tested our models on two datasets: a\ncollection of over 200 years of US state-of-the-union (SOTU) addresses and a\nlarge-scale COVID-19 Twitter corpus of 10 million tweets. The results show that\nWBToT captures events better than Latent Dirichlet Allocation and other SOTA\ntopic models like BERTopic: the median absolute deviation of the topic presence\nover time is reduced by $51\\%$ and $34\\%$, respectively. Our experiments also\ndemonstrate the superior coherence of WBToT over BToT, which highlights the\nimportance of balancing the time and word modalities. Finally, we illustrate\nthe stability of the online optimization algorithm in WBToT, which allows the\napplication of WBToT to problems that are intractable for standard ToT.\n","authors":["Julián Cendrero","Julio Gonzalo","Ivar Zapata"],"pdf_url":"https://arxiv.org/pdf/2504.15220v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2504.15219v1","updated":"2025-04-21T16:43:50Z","published":"2025-04-21T16:43:50Z","title":"EvalAgent: Discovering Implicit Evaluation Criteria from the Web","summary":"  Evaluation of language model outputs on structured writing tasks is typically\nconducted with a number of desirable criteria presented to human evaluators or\nlarge language models (LLMs). For instance, on a prompt like \"Help me draft an\nacademic talk on coffee intake vs research productivity\", a model response may\nbe evaluated for criteria like accuracy and coherence. However, high-quality\nresponses should do more than just satisfy basic task requirements. An\neffective response to this query should include quintessential features of an\nacademic talk, such as a compelling opening, clear research questions, and a\ntakeaway. To help identify these implicit criteria, we introduce EvalAgent, a\nnovel framework designed to automatically uncover nuanced and task-specific\ncriteria. EvalAgent first mines expert-authored online guidance. It then uses\nthis evidence to propose diverse, long-tail evaluation criteria that are\ngrounded in reliable external sources. Our experiments demonstrate that the\ngrounded criteria produced by EvalAgent are often implicit (not directly stated\nin the user's prompt), yet specific (high degree of lexical precision).\nFurther, EvalAgent criteria are often not satisfied by initial responses but\nthey are actionable, such that responses can be refined to satisfy them.\nFinally, we show that combining LLM-generated and EvalAgent criteria uncovers\nmore human-valued criteria than using LLMs alone.\n","authors":["Manya Wadhwa","Zayne Sprague","Chaitanya Malaviya","Philippe Laban","Junyi Jessy Li","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2504.15219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07890v3","updated":"2025-04-21T16:43:00Z","published":"2024-07-10T17:57:58Z","title":"Training on the Test Task Confounds Evaluation and Emergence","summary":"  We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of practices that\nutilize knowledge about evaluation tasks at training time. We demonstrate that\ntraining on the test task confounds both relative model evaluations and claims\nabout emergent capabilities. We argue that the seeming superiority of one model\nfamily over another may be explained by a different degree of training on the\ntest task. To this end, we propose an effective method to adjust for the effect\nof training on the test task on benchmark evaluations. Put simply, to fine-tune\neach model under comparison on the same task-relevant data prior to evaluation.\nWe then show that instances of emergent behavior disappear gradually as models\ntrain on the test task. Our work promotes a new perspective on the evaluation\nof large language models, with broad implications for benchmarking and the\nstudy of emergent capabilities.\n","authors":["Ricardo Dominguez-Olmedo","Florian E. Dorner","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2407.07890v3.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.14744v3","updated":"2025-04-21T16:41:37Z","published":"2025-02-20T17:14:34Z","title":"HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language\n  Models via Monitoring Hidden States","summary":"  The integration of additional modalities increases the susceptibility of\nlarge vision-language models (LVLMs) to safety risks, such as jailbreak\nattacks, compared to their language-only counterparts. While existing research\nprimarily focuses on post-hoc alignment techniques, the underlying safety\nmechanisms within LVLMs remain largely unexplored. In this work , we\ninvestigate whether LVLMs inherently encode safety-relevant signals within\ntheir internal activations during inference. Our findings reveal that LVLMs\nexhibit distinct activation patterns when processing unsafe prompts, which can\nbe leveraged to detect and mitigate adversarial inputs without requiring\nextensive fine-tuning. Building on this insight, we introduce HiddenDetect, a\nnovel tuning-free framework that harnesses internal model activations to\nenhance safety. Experimental results show that {HiddenDetect} surpasses\nstate-of-the-art methods in detecting jailbreak attacks against LVLMs. By\nutilizing intrinsic safety-aware patterns, our method provides an efficient and\nscalable solution for strengthening LVLM robustness against multimodal threats.\nOur code will be released publicly at\nhttps://github.com/leigest519/HiddenDetect.\n","authors":["Yilei Jiang","Xinyan Gao","Tianshuo Peng","Yingshui Tan","Xiaoyong Zhu","Bo Zheng","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2502.14744v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15205v1","updated":"2025-04-21T16:20:43Z","published":"2025-04-21T16:20:43Z","title":"Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus\n  LLM Judges","summary":"  Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment.\n","authors":["Nandan Thakur","Ronak Pradeep","Shivani Upadhyay","Daniel Campos","Nick Craswell","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2504.15205v1.pdf","comment":"Accepted at SIGIR 2025 (short)"},{"id":"http://arxiv.org/abs/2402.01677v5","updated":"2025-04-21T15:58:38Z","published":"2024-01-20T08:44:34Z","title":"Embedding Ontologies via Incorporating Extensional and Intensional\n  Knowledge","summary":"  Ontologies contain rich knowledge within domain, which can be divided into\ntwo categories, namely extensional knowledge and intensional knowledge.\nExtensional knowledge provides information about the concrete instances that\nbelong to specific concepts in the ontology, while intensional knowledge\ndetails inherent properties, characteristics, and semantic associations among\nconcepts. However, existing ontology embedding approaches fail to take both\nextensional knowledge and intensional knowledge into fine consideration\nsimultaneously. In this paper, we propose a novel ontology embedding approach\nnamed EIKE (Extensional and Intensional Knowledge Embedding) by representing\nontologies in two spaces, called extensional space and intensional space. EIKE\npresents a unified framework for embedding instances, concepts and their\nrelations in an ontology, applying a geometry-based method to model extensional\nknowledge and a pretrained language model to model intensional knowledge, which\ncan capture both structure information and textual information. Experimental\nresults show that EIKE significantly outperforms state-of-the-art methods in\nthree datasets for both triple classification and link prediction, indicating\nthat EIKE provides a more comprehensive and representative perspective of the\ndomain.\n","authors":["Keyu Wang","Guilin Qi","Jiaoyan Chen","Yi Huang","Tianxing Wu"],"pdf_url":"https://arxiv.org/pdf/2402.01677v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06560v2","updated":"2025-04-21T15:37:15Z","published":"2024-06-02T11:54:50Z","title":"Inverse Constitutional AI: Compressing Preferences into Principles","summary":"  Feedback data is widely used for fine-tuning and evaluating state-of-the-art\nAI models. Pairwise text preferences, where human or AI annotators select the\n\"better\" of two options, are particularly common. Such preferences are used to\ntrain (reward) models or to rank models with aggregate statistics. For many\napplications it is desirable to understand annotator preferences in addition to\nmodelling them - not least because extensive prior work has shown various\nunintended biases in preference datasets. Yet, preference datasets remain\nchallenging to interpret. Neither black-box reward models nor statistics can\nanswer why one text is preferred over another. Manual interpretation of the\nnumerous (long) response pairs is usually equally infeasible. In this paper, we\nintroduce the Inverse Constitutional AI (ICAI) problem, formulating the\ninterpretation of pairwise text preference data as a compression task. In\nconstitutional AI, a set of principles (a constitution) is used to provide\nfeedback and fine-tune AI models. ICAI inverts this process: given a feedback\ndataset, we aim to extract a constitution that best enables a large language\nmodel (LLM) to reconstruct the original annotations. We propose a corresponding\nICAI algorithm and validate its generated constitutions quantitatively based on\nannotation reconstruction accuracy on several datasets: (a) synthetic feedback\ndata with known principles; (b) AlpacaEval cross-annotated human feedback data;\n(c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse\ndemographic groups. As a short and interpretable representation of the original\ndataset, generated constitutions have many potential use cases: help identify\nundesirable annotator biases, understand model performance better, scale\nfeedback to unseen data, or adapt models to individual user or group\npreferences. We release the source code at https://github.com/rdnfn/icai.\n","authors":["Arduin Findeis","Timo Kaufmann","Eyke Hüllermeier","Samuel Albanie","Robert Mullins"],"pdf_url":"https://arxiv.org/pdf/2406.06560v2.pdf","comment":"Accepted at ICLR 2025, v2 is camera-ready version; Main changes from\n  v1: extended experiments, additional baselines"},{"id":"http://arxiv.org/abs/2504.15168v1","updated":"2025-04-21T15:22:21Z","published":"2025-04-21T15:22:21Z","title":"On true empty category","summary":"  According to Chomsky (1981, 1986), empty categories consist of PRO, pro,\ntrace, and variable. However, some empty object positions seem to be\nincompatible with extant empty categories. Given this, Li (2007a, 2007b, 2014)\nand Li & Wei (2014) raise the true empty category hypothesis, which holds that\ntrue empty category is only an empty position with category and Case features.\nAs a last resort option, it is used mainly to meet the subcatgorization of a\nverb. This assumption is ingenious, and if proved to be true, it will exert a\ngreat impact on the study of UG. In this paper, we evaluate their evidence from\ntopicalization and demonstrate that it can be accounted for without invoking\ntrue empty category.\n","authors":["Qilin Tian"],"pdf_url":"https://arxiv.org/pdf/2504.15168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14866v2","updated":"2025-04-21T15:13:44Z","published":"2025-02-20T18:59:52Z","title":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention","summary":"  Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.\n","authors":["Shang Yang","Junxian Guo","Haotian Tang","Qinghao Hu","Guangxuan Xiao","Jiaming Tang","Yujun Lin","Zhijian Liu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2502.14866v2.pdf","comment":"Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve"},{"id":"http://arxiv.org/abs/2504.02438v3","updated":"2025-04-21T15:12:34Z","published":"2025-04-03T09:55:09Z","title":"Scaling Video-Language Models to 10K Frames via Hierarchical\n  Differential Distillation","summary":"  Long-form video processing fundamentally challenges vision-language models\n(VLMs) due to the high computational costs of handling extended temporal\nsequences. Existing token pruning and feature merging methods often sacrifice\ncritical temporal dependencies or dilute semantic information. We introduce\ndifferential distillation, a principled approach that systematically preserves\ntask-relevant information while suppressing redundancy. Based on this\nprinciple, we develop ViLaMP, a hierarchical video-language model that\nprocesses hour-long videos at ``mixed precision'' through two key mechanisms:\n(1) differential keyframe selection that maximizes query relevance while\nmaintaining temporal distinctiveness at the frame level and (2) differential\nfeature merging that preserves query-salient features in non-keyframes at the\npatch level. Hence, ViLaMP retains full information in keyframes while reducing\nnon-keyframes to their most salient features, resembling mixed-precision\ntraining. Extensive experiments demonstrate ViLaMP's superior performance\nacross four video understanding benchmarks, particularly on long-form content.\nNotably, ViLaMP can process ultra-long videos (up to 10K frames) on a single\nNVIDIA A100 GPU, achieving substantial computational efficiency while\nmaintaining state-of-the-art performance.\n","authors":["Chuanqi Cheng","Jian Guan","Wei Wu","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2504.02438v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15160v1","updated":"2025-04-21T15:07:26Z","published":"2025-04-21T15:07:26Z","title":"The Synthetic Imputation Approach: Generating Optimal Synthetic Texts\n  For Underrepresented Categories In Supervised Classification Tasks","summary":"  Encoder-decoder Large Language Models (LLMs), such as BERT and RoBERTa,\nrequire that all categories in an annotation task be sufficiently represented\nin the training data for optimal performance. However, it is often difficult to\nfind sufficient examples for all categories in a task when building a\nhigh-quality training set. In this article, I describe this problem and propose\na solution, the synthetic imputation approach. Leveraging a generative LLM\n(GPT-4o), this approach generates synthetic texts based on careful prompting\nand five original examples drawn randomly with replacement from the sample.\nThis approach ensures that new synthetic texts are sufficiently different from\nthe original texts to reduce overfitting, but retain the underlying substantive\nmeaning of the examples to maximize out-of-sample performance. With 75 original\nexamples or more, synthetic imputation's performance is on par with a full\nsample of original texts, and overfitting remains low, predictable and\ncorrectable with 50 original samples. The synthetic imputation approach\nprovides a novel role for generative LLMs in research and allows applied\nresearchers to balance their datasets for best performance.\n","authors":["Joan C. Timoneda"],"pdf_url":"https://arxiv.org/pdf/2504.15160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15135v1","updated":"2025-04-21T14:38:44Z","published":"2025-04-21T14:38:44Z","title":"KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking","summary":"  Entity linking (EL) aligns textual mentions with their corresponding entities\nin a knowledge base, facilitating various applications such as semantic search\nand question answering. Recent advances in multimodal entity linking (MEL) have\nshown that combining text and images can reduce ambiguity and improve alignment\naccuracy. However, most existing MEL methods overlook the rich structural\ninformation available in the form of knowledge-graph (KG) triples. In this\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\nhigh-quality triples for each mention by employing vision-language models based\non its text and images. (2) Retrieval: Learns joint mention-entity\nrepresentations, via contrastive learning, that integrate text, images, and\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\nReranking: Refines the KG triples of the candidate entities and employs large\nlanguage models to identify the best-matching entity for the mention. Extensive\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\nmethods. Our code and datasets are available at:\nhttps://github.com/juyeonnn/KGMEL.\n","authors":["Juyeon Kim","Geon Lee","Taeuk Kim","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2504.15135v1.pdf","comment":"SIGIR 2025 (Short)"},{"id":"http://arxiv.org/abs/2504.15133v1","updated":"2025-04-21T14:33:55Z","published":"2025-04-21T14:33:55Z","title":"EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models","summary":"  In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.\n","authors":["Ziwen Xu","Shuxun Wang","Kewei Xu","Haoming Xu","Mengru Wang","Xinle Deng","Yunzhi Yao","Guozhou Zheng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.15133v1.pdf","comment":"Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2504.15120v1","updated":"2025-04-21T14:17:25Z","published":"2025-04-21T14:17:25Z","title":"Kuwain 1.5B: An Arabic SLM via Language Injection","summary":"  Enhancing existing models with new knowledge is a crucial aspect of AI\ndevelopment. This paper introduces a novel method for integrating a new\nlanguage into a large language model (LLM). Our approach successfully\nincorporates a previously unseen target language into an existing LLM without\ncompromising its prior knowledge. We trained a tiny model with 1.5 billion\nparameters named Kuwain by injecting the Arabic language into a small\nopen-source model mainly trained in English. Our method demonstrates\nsignificant improvements in Arabic language performance, with an average 8%\nimprovement across various benchmarks, while retaining the model's existing\nknowledge with a minimum amount of the original model's data. This offers a\ncost-effective alternative to training a comprehensive model in both English\nand Arabic. The results highlight the potential for efficient, targeted\nlanguage model expansion without extensive retraining or resource-intensive\nprocesses.\n","authors":["Khalil Hennara","Sara Chrouf","Mohamed Motaism Hamed","Zeina Aldallal","Omar Hadid","Safwan AlModhayan"],"pdf_url":"https://arxiv.org/pdf/2504.15120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05414v2","updated":"2025-04-21T13:50:28Z","published":"2025-01-09T18:16:55Z","title":"LongProc: Benchmarking Long-Context Language Models on Long Procedural\n  Generation","summary":"  Existing benchmarks for evaluating long-context language models (LCLMs)\nprimarily focus on long-context recall, requiring models to produce short\nresponses based on a few critical snippets while processing thousands of\nirrelevant tokens. We introduce LongProc (Long Procedural Generation), a new\nbenchmark that requires both the integration of highly dispersed information\nand long-form generation. LongProc consists of six diverse procedural\ngeneration tasks, such as extracting structured information from HTML pages\ninto a TSV format and executing complex search procedures to create travel\nplans. These tasks challenge LCLMs by testing their ability to follow detailed\nprocedural instructions, synthesize and reason over dispersed information, and\ngenerate structured, long-form outputs (up to 8K tokens). Furthermore, as these\ntasks adhere to deterministic procedures and yield structured outputs, they\nenable reliable rule-based evaluation. We evaluated 23 LCLMs, including\ninstruction-tuned models and recent reasoning models, on LongProc at three\ndifficulty levels, with the maximum number of output tokens set at 500, 2K, and\n8K. Notably, while all tested models claim a context window size above 32K\ntokens, open-weight models typically falter on 2K-token tasks, and\nclosed-source models like GPT-4o show significant degradation on 8K-token\ntasks. Reasoning models achieve stronger overall performance in long-form\ngeneration, benefiting from long CoT training. Further analysis reveals that\nLCLMs struggle to maintain long-range coherence in long-form generations. These\nfindings highlight critical limitations in current LCLMs and suggest\nsubstantial room for improvement. Data and code available at:\nhttps://princeton-pli.github.io/LongProc.\n","authors":["Xi Ye","Fangcong Yin","Yinghui He","Joie Zhang","Howard Yen","Tianyu Gao","Greg Durrett","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2501.05414v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15093v1","updated":"2025-04-21T13:25:55Z","published":"2025-04-21T13:25:55Z","title":"Rethinking the Potential of Multimodality in Collaborative Problem\n  Solving Diagnosis with Large Language Models","summary":"  Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts.\n","authors":["K. Wong","B. Wu","S. Bulathwela","M. Cukurova"],"pdf_url":"https://arxiv.org/pdf/2504.15093v1.pdf","comment":"Accepted for 26th International Conference on Artificial Intelligence\n  in Education (AIED 2025), 22 - 26 July 2025, Palermo, Italy. 17 pages, 1\n  figure"},{"id":"http://arxiv.org/abs/2503.10664v2","updated":"2025-04-21T13:04:29Z","published":"2025-03-09T08:23:31Z","title":"Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism","summary":"  Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding.\n","authors":["Timo Aukusti Laine"],"pdf_url":"https://arxiv.org/pdf/2503.10664v2.pdf","comment":"29 pages, 4 figures. Some corrections added"},{"id":"http://arxiv.org/abs/2504.15072v1","updated":"2025-04-21T13:02:30Z","published":"2025-04-21T13:02:30Z","title":"Rhythm of Opinion: A Hawkes-Graph Framework for Dynamic Propagation\n  Analysis","summary":"  The rapid development of social media has significantly reshaped the dynamics\nof public opinion, resulting in complex interactions that traditional models\nfail to effectively capture. To address this challenge, we propose an\ninnovative approach that integrates multi-dimensional Hawkes processes with\nGraph Neural Network, modeling opinion propagation dynamics among nodes in a\nsocial network while considering the intricate hierarchical relationships\nbetween comments. The extended multi-dimensional Hawkes process captures the\nhierarchical structure, multi-dimensional interactions, and mutual influences\nacross different topics, forming a complex propagation network. Moreover,\nrecognizing the lack of high-quality datasets capable of comprehensively\ncapturing the evolution of public opinion dynamics, we introduce a new dataset,\nVISTA. It includes 159 trending topics, corresponding to 47,207 posts, 327,015\nsecond-level comments, and 29,578 third-level comments, covering diverse\ndomains such as politics, entertainment, sports, health, and medicine. The\ndataset is annotated with detailed sentiment labels across 11 categories and\nclearly defined hierarchical relationships. When combined with our method, it\noffers strong interpretability by linking sentiment propagation to the comment\nhierarchy and temporal evolution. Our approach provides a robust baseline for\nfuture research.\n","authors":["Yulong Li","Zhixiang Lu","Feilong Tang","Simin Lai","Ming Hu","Yuxuan Zhang","Haochen Xue","Zhaodong Wu","Imran Razzak","Qingxia Li","Jionglong Su"],"pdf_url":"https://arxiv.org/pdf/2504.15072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15068v1","updated":"2025-04-21T12:55:06Z","published":"2025-04-21T12:55:06Z","title":"The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation\n  with Large Language Models","summary":"  Large Language Models (LLMs) have significantly enhanced the capabilities of\ninformation access systems, especially with retrieval-augmented generation\n(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to\ncontinued progress, a challenge we tackle in this work by proposing an\nautomatic evaluation framework that is validated against human annotations. We\nbelieve that the nugget evaluation methodology provides a solid foundation for\nevaluating RAG systems. This approach, originally developed for the TREC\nQuestion Answering (QA) Track in 2003, evaluates systems based on atomic facts\nthat should be present in good answers. Our efforts focus on \"refactoring\" this\nmethodology, where we describe the AutoNuggetizer framework that specifically\napplies LLMs to both automatically create nuggets and automatically assign\nnuggets to system answers. In the context of the TREC 2024 RAG Track, we\ncalibrate a fully automatic approach against strategies where nuggets are\ncreated manually or semi-manually by human assessors and then assigned manually\nto system answers. Based on results from a community-wide evaluation, we\nobserve strong agreement at the run level between scores derived from fully\nautomatic nugget evaluation and human-based variants. The agreement is stronger\nwhen individual framework components such as nugget assignment are automated\nindependently. This suggests that our evaluation framework provides tradeoffs\nbetween effort and quality that can be used to guide the development of future\nRAG systems. However, further research is necessary to refine our approach,\nparticularly in establishing robust per-topic agreement to diagnose system\nfailures effectively.\n","authors":["Ronak Pradeep","Nandan Thakur","Shivani Upadhyay","Daniel Campos","Nick Craswell","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2504.15068v1.pdf","comment":"To appear in SIGIR 2025. Significant updates and revisions to\n  arXiv:2411.09607"},{"id":"http://arxiv.org/abs/2403.10258v3","updated":"2025-04-21T12:52:49Z","published":"2024-03-15T12:47:39Z","title":"Is Translation All You Need? A Study on Solving Multilingual Tasks with\n  Large Language Models","summary":"  Large language models (LLMs) have demonstrated multilingual capabilities, yet\nthey are mostly English-centric due to the imbalanced training corpora. While\nprior works have leveraged this bias to enhance multilingual performance\nthrough translation, they have been largely limited to natural language\nprocessing (NLP) tasks. In this work, we extend the evaluation to real-world\nuser queries and non-English-centric LLMs, offering a broader examination of\nmultilingual performance. Our key contribution lies in demonstrating that while\ntranslation into English can boost the performance of English-centric LLMs on\nNLP tasks, it is not universally optimal. For culture-related tasks that need\ndeep language understanding, prompting in the native language proves more\neffective as it better captures the nuances of culture and language. Our\nexperiments expose varied behaviors across LLMs and tasks in the multilingual\ncontext, underscoring the need for a more comprehensive approach to\nmultilingual evaluation. Therefore, we call for greater efforts in developing\nand evaluating LLMs that go beyond English-centric paradigms.\n","authors":["Chaoqun Liu","Wenxuan Zhang","Yiran Zhao","Anh Tuan Luu","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2403.10258v3.pdf","comment":"Accepted to NAACL 2025"},{"id":"http://arxiv.org/abs/2504.15052v1","updated":"2025-04-21T12:21:37Z","published":"2025-04-21T12:21:37Z","title":"Testing LLMs' Capabilities in Annotating Translations Based on an Error\n  Typology Designed for LSP Translation: First Experiments with ChatGPT","summary":"  This study investigates the capabilities of large language models (LLMs),\nspecifically ChatGPT, in annotating MT outputs based on an error typology. In\ncontrast to previous work focusing mainly on general language, we explore\nChatGPT's ability to identify and categorise errors in specialised\ntranslations. By testing two different prompts and based on a customised error\ntypology, we compare ChatGPT annotations with human expert evaluations of\ntranslations produced by DeepL and ChatGPT itself. The results show that, for\ntranslations generated by DeepL, recall and precision are quite high. However,\nthe degree of accuracy in error categorisation depends on the prompt's specific\nfeatures and its level of detail, ChatGPT performing very well with a detailed\nprompt. When evaluating its own translations, ChatGPT achieves significantly\npoorer results, revealing limitations with self-assessment. These results\nhighlight both the potential and the limitations of LLMs for translation\nevaluation, particularly in specialised domains. Our experiments pave the way\nfor future research on open-source LLMs, which could produce annotations of\ncomparable or even higher quality. In the future, we also aim to test the\npractical effectiveness of this automated evaluation in the context of\ntranslation training, particularly by optimising the process of human\nevaluation by teachers and by exploring the impact of annotations by LLMs on\nstudents' post-editing and translation learning.\n","authors":["Joachim Minder","Guillaume Wisniewski","Natalie Kübler"],"pdf_url":"https://arxiv.org/pdf/2504.15052v1.pdf","comment":"Accepted for publication in the proceedings of MT Summit 2025"},{"id":"http://arxiv.org/abs/2504.15047v1","updated":"2025-04-21T12:04:57Z","published":"2025-04-21T12:04:57Z","title":"RainbowPlus: Enhancing Adversarial Prompt Generation via Evolutionary\n  Quality-Diversity Search","summary":"  Large Language Models (LLMs) exhibit remarkable capabilities but are\nsusceptible to adversarial prompts that exploit vulnerabilities to produce\nunsafe or biased outputs. Existing red-teaming methods often face scalability\nchallenges, resource-intensive requirements, or limited diversity in attack\nstrategies. We propose RainbowPlus, a novel red-teaming framework rooted in\nevolutionary computation, enhancing adversarial prompt generation through an\nadaptive quality-diversity (QD) search that extends classical evolutionary\nalgorithms like MAP-Elites with innovations tailored for language models. By\nemploying a multi-element archive to store diverse high-quality prompts and a\ncomprehensive fitness function to evaluate multiple prompts concurrently,\nRainbowPlus overcomes the constraints of single-prompt archives and pairwise\ncomparisons in prior QD methods like Rainbow Teaming. Experiments comparing\nRainbowPlus to QD methods across six benchmark datasets and four open-source\nLLMs demonstrate superior attack success rate (ASR) and diversity\n(Diverse-Score $\\approx 0.84$), generating up to 100 times more unique prompts\n(e.g., 10,418 vs. 100 for Ministral-8B-Instruct-2410). Against nine\nstate-of-the-art methods on the HarmBench dataset with twelve LLMs (ten\nopen-source, two closed-source), RainbowPlus achieves an average ASR of 81.1%,\nsurpassing AutoDAN-Turbo by 3.9%, and is 9 times faster (1.45 vs. 13.50 hours).\nOur open-source implementation fosters further advancements in LLM safety,\noffering a scalable tool for vulnerability assessment. Code and resources are\npublicly available at https://github.com/knoveleng/rainbowplus, supporting\nreproducibility and future research in LLM red-teaming.\n","authors":["Quy-Anh Dang","Chris Ngo","Truong-Son Hy"],"pdf_url":"https://arxiv.org/pdf/2504.15047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15027v1","updated":"2025-04-21T11:26:02Z","published":"2025-04-21T11:26:02Z","title":"DistilQwen2.5: Industrial Practices of Training Distilled Open\n  Lightweight Language Models","summary":"  Enhancing computational efficiency and reducing deployment costs for large\nlanguage models (LLMs) have become critical challenges in various\nresource-constrained scenarios. In this work, we present DistilQwen2.5, a\nfamily of distilled, lightweight LLMs derived from the public Qwen2.5 models.\nThese distilled models exhibit enhanced instruction-following capabilities\ncompared to the original models based on a series of distillation techniques\nthat incorporate knowledge from much larger LLMs. In our industrial practice,\nwe first leverage powerful proprietary LLMs with varying capacities as\nmulti-agent teachers to select, rewrite, and refine instruction-response pairs\nthat are more suitable for student LLMs to learn. After standard fine-tuning,\nwe further leverage a computationally efficient model fusion approach that\nenables student models to progressively integrate fine-grained hidden knowledge\nfrom their teachers. Experimental evaluations demonstrate that the distilled\nmodels possess significantly stronger capabilities than their original\ncheckpoints. Additionally, we present use cases to illustrate the applications\nof our framework in real-world scenarios. To facilitate practical use, we have\nreleased all the DistilQwen2.5 models to the open-source community.\n","authors":["Chengyu Wang","Junbing Yan","Yuanhao Yue","Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2504.15027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15022v1","updated":"2025-04-21T11:11:07Z","published":"2025-04-21T11:11:07Z","title":"LLMs as Data Annotators: How Close Are We to Human Performance","summary":"  In NLP, fine-tuning LLMs is effective for various applications but requires\nhigh-quality annotated data. However, manual annotation of data is\nlabor-intensive, time-consuming, and costly. Therefore, LLMs are increasingly\nused to automate the process, often employing in-context learning (ICL) in\nwhich some examples related to the task are given in the prompt for better\nperformance. However, manually selecting context examples can lead to\ninefficiencies and suboptimal model performance. This paper presents\ncomprehensive experiments comparing several LLMs, considering different\nembedding models, across various datasets for the Named Entity Recognition\n(NER) task. The evaluation encompasses models with approximately $7$B and $70$B\nparameters, including both proprietary and non-proprietary models. Furthermore,\nleveraging the success of Retrieval-Augmented Generation (RAG), it also\nconsiders a method that addresses the limitations of ICL by automatically\nretrieving contextual examples, thereby enhancing performance. The results\nhighlight the importance of selecting the appropriate LLM and embedding model,\nunderstanding the trade-offs between LLM sizes and desired performance, and the\nnecessity to direct research efforts towards more challenging datasets.\n","authors":["Muhammad Uzair Ul Haq","Davide Rigoni","Alessandro Sperduti"],"pdf_url":"https://arxiv.org/pdf/2504.15022v1.pdf","comment":"27 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.13828v2","updated":"2025-04-21T10:38:44Z","published":"2025-04-18T17:55:58Z","title":"Generative AI Act II: Test Time Scaling Drives Cognition Engineering","summary":"  The first generation of Large Language Models - what might be called \"Act I\"\nof generative AI (2020-2023) - achieved remarkable success through massive\nparameter and data scaling, yet exhibited fundamental limitations such as\nknowledge latency, shallow reasoning, and constrained cognitive processes.\nDuring this era, prompt engineering emerged as our primary interface with AI,\nenabling dialogue-level communication through natural language. We now witness\nthe emergence of \"Act II\" (2024-present), where models are transitioning from\nknowledge-retrieval systems (in latent space) to thought-construction engines\nthrough test-time scaling techniques. This new paradigm establishes a\nmind-level connection with AI through language-based thoughts. In this paper,\nwe clarify the conceptual foundations of cognition engineering and explain why\nthis moment is critical for its development. We systematically break down these\nadvanced approaches through comprehensive tutorials and optimized\nimplementations, democratizing access to cognition engineering and enabling\nevery practitioner to participate in AI's second act. We provide a regularly\nupdated collection of papers on test-time scaling in the GitHub Repository:\nhttps://github.com/GAIR-NLP/cognition-engineering\n","authors":["Shijie Xia","Yiwei Qin","Xuefeng Li","Yan Ma","Run-Ze Fan","Steffi Chern","Haoyang Zou","Fan Zhou","Xiangkun Hu","Jiahe Jin","Yanheng He","Yixin Ye","Yixiu Liu","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2504.13828v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15013v1","updated":"2025-04-21T10:35:48Z","published":"2025-04-21T10:35:48Z","title":"Stay Hungry, Stay Foolish: On the Extended Reading Articles Generation\n  with LLMs","summary":"  The process of creating educational materials is both time-consuming and\ndemanding for educators. This research explores the potential of Large Language\nModels (LLMs) to streamline this task by automating the generation of extended\nreading materials and relevant course suggestions. Using the TED-Ed Dig Deeper\nsections as an initial exploration, we investigate how supplementary articles\ncan be enriched with contextual knowledge and connected to additional learning\nresources. Our method begins by generating extended articles from video\ntranscripts, leveraging LLMs to include historical insights, cultural examples,\nand illustrative anecdotes. A recommendation system employing semantic\nsimilarity ranking identifies related courses, followed by an LLM-based\nrefinement process to enhance relevance. The final articles are tailored to\nseamlessly integrate these recommendations, ensuring they remain cohesive and\ninformative. Experimental evaluations demonstrate that our model produces\nhigh-quality content and accurate course suggestions, assessed through metrics\nsuch as Hit Rate, semantic similarity, and coherence. Our experimental analysis\nhighlight the nuanced differences between the generated and existing materials,\nunderscoring the model's capacity to offer more engaging and accessible\nlearning experiences. This study showcases how LLMs can bridge the gap between\ncore content and supplementary learning, providing students with additional\nrecommended resources while also assisting teachers in designing educational\nmaterials.\n","authors":["Yow-Fu Liou","Yu-Chien Tang","An-Zi Yen"],"pdf_url":"https://arxiv.org/pdf/2504.15013v1.pdf","comment":"Accepted by iRAISE@AAAI2025"},{"id":"http://arxiv.org/abs/2410.10796v3","updated":"2025-04-21T10:19:21Z","published":"2024-10-14T17:57:09Z","title":"Context-Parametric Inversion: Why Instruction Finetuning Can Worsen\n  Context Reliance","summary":"  A standard practice when using large language models is for users to\nsupplement their instruction with an input context containing new information\nfor the model to process. However, models struggle to reliably follow the input\ncontext, especially when it conflicts with their parametric knowledge from\npretraining. In-principle, one would expect models to adapt to the user context\nbetter after instruction finetuning, particularly when handling knowledge\nconflicts. However, we observe a surprising failure mode: during instruction\ntuning, the context reliance under knowledge conflicts initially increases as\nexpected, but then gradually decreases as instruction finetuning progresses.\nThis happens while the performance on standard benchmarks keeps on increasing\nfar after this drop. We call this phenomenon context-parametric inversion and\nobserve it across multiple general purpose instruction tuning datasets such as\nTULU, Alpaca and Ultrachat, across different model families like Llama,\nMistral, and Pythia. We perform various controlled studies and theoretical\nanalysis to show that context-parametric inversion occurs due to examples in\nthe instruction finetuning data where the input context provides information\nthat aligns with model's parametric knowledge. Our analysis suggests some\nnatural mitigation strategies with limited but insightful gains, and serves as\na useful starting point in addressing this deficiency in instruction\nfinetuning.\n","authors":["Sachin Goyal","Christina Baek","J. Zico Kolter","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2410.10796v3.pdf","comment":"Published at ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2409.06635v4","updated":"2025-04-21T09:48:05Z","published":"2024-09-10T16:46:18Z","title":"MoWE-Audio: Multitask AudioLLMs with Mixture of Weak Encoders","summary":"  The rapid advancements in large language models (LLMs) have significantly\nenhanced natural language processing capabilities, facilitating the development\nof AudioLLMs that process and understand speech and audio inputs alongside\ntext. Existing AudioLLMs typically combine a pre-trained audio encoder with a\npre-trained LLM, which are subsequently finetuned on specific audio tasks.\nHowever, the pre-trained audio encoder has constrained capacity to capture\nfeatures for new tasks and datasets. To address this, we propose to incorporate\nmixtures of `weak' encoders (MoWE) into the AudioLLM framework. MoWE\nsupplements a base encoder with a pool of relatively light weight encoders,\nselectively activated based on the audio input to enhance feature extraction\nwithout significantly increasing model size. Our empirical results demonstrate\nthat MoWE effectively improves multi-task performance, broadening the\napplicability of AudioLLMs to more diverse audio tasks.\n","authors":["Wenyu Zhang","Shuo Sun","Bin Wang","Xunlong Zou","Zhuohan Liu","Yingxu He","Geyu Lin","Nancy F. Chen","Ai Ti Aw"],"pdf_url":"https://arxiv.org/pdf/2409.06635v4.pdf","comment":"ICASSP 2025"},{"id":"http://arxiv.org/abs/2504.14992v1","updated":"2025-04-21T09:41:26Z","published":"2025-04-21T09:41:26Z","title":"Efficient Pretraining Length Scaling","summary":"  Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.\n","authors":["Bohong Wu","Shen Yan","Sijun Zhang","Jianqiao Lu","Yutao Zeng","Ya Wang","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.14992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14969v1","updated":"2025-04-21T08:56:23Z","published":"2025-04-21T08:56:23Z","title":"Evaluating LLMs on Chinese Topic Constructions: A Research Proposal\n  Inspired by Tian et al. (2024)","summary":"  This paper proposes a framework for evaluating large language models (LLMs)\non Chinese topic constructions, focusing on their sensitivity to island\nconstraints. Drawing inspiration from Tian et al. (2024), we outline an\nexperimental design for testing LLMs' grammatical knowledge of Mandarin syntax.\nWhile no experiments have been conducted yet, this proposal aims to provide a\nfoundation for future studies and invites feedback on the methodology.\n","authors":["Xiaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2504.14969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14963v1","updated":"2025-04-21T08:44:33Z","published":"2025-04-21T08:44:33Z","title":"Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in\n  Multiparty Dialogues","summary":"  Speaker identification using voice recordings leverages unique acoustic\nfeatures, but this approach fails when only textual data is available. Few\napproaches have attempted to tackle the problem of identifying speakers solely\nfrom text, and the existing ones have primarily relied on traditional methods.\nIn this work, we explore the use of fuzzy fingerprints from large pre-trained\nmodels to improve text-based speaker identification. We integrate\nspeaker-specific tokens and context-aware modeling, demonstrating that\nconversational context significantly boosts accuracy, reaching 70.6% on the\nFriends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show\nthat fuzzy fingerprints can approximate full fine-tuning performance with fewer\nhidden units, offering improved interpretability. Finally, we analyze ambiguous\nutterances and propose a mechanism to detect speaker-agnostic lines. Our\nfindings highlight key challenges and provide insights for future improvements\nin text-based speaker identification.\n","authors":["Rui Ribeiro","Luísa Coheur","Joao P. Carvalho"],"pdf_url":"https://arxiv.org/pdf/2504.14963v1.pdf","comment":"Paper accepted at the FUZZY IEEE 2025 conference"},{"id":"http://arxiv.org/abs/2504.14945v1","updated":"2025-04-21T08:09:13Z","published":"2025-04-21T08:09:13Z","title":"Learning to Reason under Off-Policy Guidance","summary":"  Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.\n","authors":["Jianhao Yan","Yafu Li","Zican Hu","Zhi Wang","Ganqu Cui","Xiaoye Qu","Yu Cheng","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.14945v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2504.14928v1","updated":"2025-04-21T07:48:20Z","published":"2025-04-21T07:48:20Z","title":"EducationQ: Evaluating LLMs' Teaching Capabilities Through Multi-Agent\n  Dialogue Framework","summary":"  Large language models (LLMs) increasingly serve as educational tools, yet\nevaluating their teaching capabilities remains challenging due to the\nresource-intensive, context-dependent, and methodologically complex nature of\nteacher-student interactions. We introduce EducationQ, a multi-agent dialogue\nframework that efficiently assesses teaching capabilities through simulated\ndynamic educational scenarios, featuring specialized agents for teaching,\nlearning, and evaluation. Testing 14 LLMs across major AI Organizations\n(OpenAI, Meta, Google, Anthropic, and others) on 1,498 questions spanning 13\ndisciplines and 10 difficulty levels reveals that teaching effectiveness does\nnot correlate linearly with model scale or general reasoning capabilities -\nwith some smaller open-source models outperforming larger commercial\ncounterparts in teaching contexts. This finding highlights a critical gap in\ncurrent evaluations that prioritize knowledge recall over interactive pedagogy.\nOur mixed-methods evaluation, combining quantitative metrics with qualitative\nanalysis and expert case studies, identifies distinct pedagogical strengths\nemployed by top-performing models (e.g., sophisticated questioning strategies,\nadaptive feedback mechanisms). Human expert evaluations show 78% agreement with\nour automated qualitative analysis of effective teaching behaviors, validating\nour methodology. EducationQ demonstrates that LLMs-as-teachers require\nspecialized optimization beyond simple scaling, suggesting next-generation\neducational AI prioritize targeted enhancement of specific pedagogical\neffectiveness.\n","authors":["Yao Shi","Rongkeng Liang","Yong Xu"],"pdf_url":"https://arxiv.org/pdf/2504.14928v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12322v2","updated":"2025-04-21T07:29:28Z","published":"2025-04-11T06:13:43Z","title":"A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis","summary":"  While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.\n","authors":["Xin Gao","Qizhi Pei","Zinan Tang","Yu Li","Honglin Lin","Jiang Wu","Lijun Wu","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2504.12322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14905v1","updated":"2025-04-21T07:20:31Z","published":"2025-04-21T07:20:31Z","title":"CRAVE: A Conflicting Reasoning Approach for Explainable Claim\n  Verification Using LLMs","summary":"  The rapid spread of misinformation, driven by digital media and AI-generated\ncontent, has made automatic claim verification essential. Traditional methods,\nwhich depend on expert-annotated evidence, are labor-intensive and not\nscalable. Although recent automated systems have improved, they still struggle\nwith complex claims that require nuanced reasoning. To address this, we propose\nCRAVE, a Conflicting Reasoning Approach for explainable claim VErification,\nthat verify the complex claims based on the conflicting rationales reasoned by\nlarge language models (LLMs). Specifically, CRAVE introduces a three-module\nframework. Ambiguity Elimination enchanced Evidence Retrieval module performs\nambiguity elimination and entity-based search to gather relevant evidence\nrelated to claim verification from external sources like Wikipedia. Conflicting\nPerspective Reasoning and Preliminary Judgment module with LLMs adopts LLMs to\nreason rationales with conflicting stances about claim verification from\nretrieved evidence across four dimensions, i.e., direct evidence, semantic\nrelationships, linguistic patterns, and logical reasoning and make a\npreliminary judgment. Finally, Small Language Model (SLM) based Judge module is\nfine-tuned to make use of preliminary judgment from LLMs to assess the\nconfidence of the conflicting rationales and make a final authenticity\njudgment. This methodology allows CRAVE to capture subtle inconsistencies in\ncomplex claims, improving both the accuracy and transparency of claim\nverification. Extensive experiments on two public claim verification datasets\ndemonstrate that our CRAVE model achieves much better performance than\nstate-of-the-art methods and exhibits a superior capacity for finding relevant\nevidence and explaining the model predictions. The code is provided at\nhttps://github.com/8zym/CRAVE.\n","authors":["Yingming Zheng","Xiaoliang Liu","Peng Wu","Li Pan"],"pdf_url":"https://arxiv.org/pdf/2504.14905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14904v1","updated":"2025-04-21T07:20:19Z","published":"2025-04-21T07:20:19Z","title":"VLM as Policy: Common-Law Content Moderation Framework for Short Video\n  Platform","summary":"  Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io.\n","authors":["Xingyu Lu","Tianke Zhang","Chang Meng","Xiaobei Wang","Jinpeng Wang","YiFan Zhang","Shisong Tang","Changyi Liu","Haojie Ding","Kaiyu Jiang","Kaiyu Tang","Bin Wen","Hai-Tao Zheng","Fan Yang","Tingting Gao","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2504.14904v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.09602v2","updated":"2025-04-21T07:04:57Z","published":"2025-04-13T14:35:30Z","title":"Fine-tuning a Large Language Model for Automating Computational Fluid\n  Dynamics Simulations","summary":"  Configuring computational fluid dynamics (CFD) simulations typically demands\nextensive domain expertise, limiting broader access. Although large language\nmodels (LLMs) have advanced scientific computing, their use in automating CFD\nworkflows is underdeveloped. We introduce a novel approach centered on\ndomain-specific LLM adaptation. By fine-tuning Qwen2.5-7B-Instruct on NL2FOAM,\nour custom dataset of 28716 natural language-to-OpenFOAM configuration pairs\nwith chain-of-thought (CoT) annotations, we enable direct translation from\nnatural language descriptions to executable CFD setups. A multi-agent framework\norchestrates the process, autonomously verifying inputs, generating\nconfigurations, running simulations, and correcting errors. Evaluation on a\nbenchmark of 21 diverse flow cases demonstrates state-of-the-art performance,\nachieving 88.7% solution accuracy and 82.6% first-attempt success rate. This\nsignificantly outperforms larger general-purpose models like\nQwen2.5-72B-Instruct, DeepSeek-R1, and Llama3.3-70B-Instruct, while also\nrequiring fewer correction iterations and maintaining high computational\nefficiency. The results highlight the critical role of domain-specific\nadaptation in deploying LLM assistants for complex engineering workflows. Our\ncode and fine-tuned model have been deposited at\nhttps://github.com/YYgroup/AutoCFD.\n","authors":["Zhehao Dong","Zhen Lu","Yue Yang"],"pdf_url":"https://arxiv.org/pdf/2504.09602v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14891v1","updated":"2025-04-21T06:39:47Z","published":"2025-04-21T06:39:47Z","title":"Retrieval Augmented Generation Evaluation in the Era of Large Language\n  Models: A Comprehensive Survey","summary":"  Recent advancements in Retrieval-Augmented Generation (RAG) have\nrevolutionized natural language processing by integrating Large Language Models\n(LLMs) with external information retrieval, enabling accurate, up-to-date, and\nverifiable text generation across diverse applications. However, evaluating RAG\nsystems presents unique challenges due to their hybrid architecture that\ncombines retrieval and generation components, as well as their dependence on\ndynamic knowledge sources in the LLM era. In response, this paper provides a\ncomprehensive survey of RAG evaluation methods and frameworks, systematically\nreviewing traditional and emerging evaluation approaches, for system\nperformance, factual accuracy, safety, and computational efficiency in the LLM\nera. We also compile and categorize the RAG-specific datasets and evaluation\nframeworks, conducting a meta-analysis of evaluation practices in high-impact\nRAG research. To the best of our knowledge, this work represents the most\ncomprehensive survey for RAG evaluation, bridging traditional and LLM-driven\nmethods, and serves as a critical resource for advancing RAG development.\n","authors":["Aoran Gan","Hao Yu","Kai Zhang","Qi Liu","Wenyu Yan","Zhenya Huang","Shiwei Tong","Guoping Hu"],"pdf_url":"https://arxiv.org/pdf/2504.14891v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.14871v1","updated":"2025-04-21T05:48:52Z","published":"2025-04-21T05:48:52Z","title":"Natural Fingerprints of Large Language Models","summary":"  Large language models (LLMs) often exhibit biases -- systematic deviations\nfrom expected norms -- in their outputs. These range from overt issues, such as\nunfair responses, to subtler patterns that can reveal which model produced\nthem. We investigate the factors that give rise to identifiable characteristics\nin LLMs. Since LLMs model training data distribution, it is reasonable that\ndifferences in training data naturally lead to the characteristics. However,\nour findings reveal that even when LLMs are trained on the exact same data, it\nis still possible to distinguish the source model based on its generated text.\nWe refer to these unintended, distinctive characteristics as natural\nfingerprints. By systematically controlling training conditions, we show that\nthe natural fingerprints can emerge from subtle differences in the training\nprocess, such as parameter sizes, optimization settings, and even random seeds.\nWe believe that understanding natural fingerprints offers new insights into the\norigins of unintended bias and ways for improving control over LLM behavior.\n","authors":["Teppei Suzuki","Ryokan Ri","Sho Takase"],"pdf_url":"https://arxiv.org/pdf/2504.14871v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14870v1","updated":"2025-04-21T05:40:05Z","published":"2025-04-21T05:40:05Z","title":"OTC: Optimal Tool Calls via Reinforcement Learning","summary":"  Tool-integrated reasoning (TIR) augments large language models (LLMs) with\nthe ability to invoke external tools, such as search engines and code\ninterpreters, to solve tasks beyond the capabilities of language-only\nreasoning. While reinforcement learning (RL) has shown promise in improving TIR\nby optimizing final answer correctness, existing approaches often overlook the\nefficiency and cost associated with tool usage. This can lead to suboptimal\nbehavior, including excessive tool calls that increase computational and\nfinancial overhead, or insufficient tool use that compromises answer quality.\nIn this work, we propose Optimal Tool Call-controlled Policy Optimization\n(OTC-PO), a simple yet effective RL-based framework that encourages models to\nproduce accurate answers with minimal tool calls. Our method introduces a\ntool-integrated reward that jointly considers correctness and tool efficiency,\npromoting high tool productivity. We instantiate this framework within both\nProximal Policy Optimization (PPO) and Group Relative Preference Optimization\n(GRPO), resulting in OTC-PPO and OTC-GRPO. Experiments with Qwen-2.5 and\nQwen-Math across multiple QA benchmarks show that our approach reduces tool\ncalls by up to 73.1\\% and improves tool productivity by up to 229.4\\%, while\nmaintaining comparable answer accuracy. To the best of our knowledge, this is\nthe first RL-based framework that explicitly optimizes tool-use efficiency in\nTIR.\n","authors":["Hongru Wang","Cheng Qian","Wanjun Zhong","Xiusi Chen","Jiahao Qiu","Shijue Huang","Bowen Jin","Mengdi Wang","Kam-Fai Wong","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2504.14870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23512v2","updated":"2025-04-21T05:40:00Z","published":"2025-03-30T16:48:27Z","title":"SCORE: Story Coherence and Retrieval Enhancement for AI Narratives","summary":"  Large Language Models (LLMs) can generate creative and engaging narratives\nfrom user-specified input, but maintaining coherence and emotional depth\nthroughout these AI-generated stories remains a challenge. In this work, we\npropose SCORE, a framework for Story Coherence and Retrieval Enhancement,\ndesigned to detect and resolve narrative inconsistencies. By tracking key item\nstatuses and generating episode summaries, SCORE uses a Retrieval-Augmented\nGeneration (RAG) approach, incorporating TF-IDF and cosine similarity to\nidentify related episodes and enhance the overall story structure. Results from\ntesting multiple LLM-generated stories demonstrate that SCORE significantly\nimproves the consistency and stability of narrative coherence compared to\nbaseline GPT models, providing a more robust method for evaluating and refining\nAI-generated narratives.\n","authors":["Qiang Yi","Yangfan He","Jianhui Wang","Xinyuan Song","Shiyao Qian","Xinhang Yuan","Miao Zhang","Li Sun","Keqin Li","Kuan Lu","Menghao Huo","Jiaqi Chen","Tianyu Shi"],"pdf_url":"https://arxiv.org/pdf/2503.23512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14815v2","updated":"2025-04-21T05:29:01Z","published":"2024-10-18T18:35:19Z","title":"Adapting Multilingual LLMs to Low-Resource Languages using Continued\n  Pre-training and Synthetic Corpus","summary":"  Multilingual LLMs support a variety of languages; however, their performance\nis suboptimal for low-resource languages. In this work, we emphasize the\nimportance of continued pre-training of multilingual LLMs and the use of\ntranslation-based synthetic pre-training corpora for improving LLMs in\nlow-resource languages. We conduct our study in the context of the low-resource\nIndic language Hindi. We introduce Nemotron-Mini-Hindi 4B, a bilingual SLM\nsupporting both Hindi and English, based on Nemotron-Mini 4B. The model is\ntrained using a mix of real and synthetic Hindi + English tokens, with\ncontinuous pre-training performed on 400B tokens. We demonstrate that both the\nbase and instruct models achieve state-of-the-art results on Hindi benchmarks\nwhile remaining competitive on English tasks. Additionally, we observe that the\ncontinued pre-training approach enhances the model's overall factual accuracy.\nWe perform an ablation study to highlight the impact of Hindi pre-training,\nshowing significant improvements in Hindi chat capabilities and factual\naccuracy, which cannot be achieved through Hindi alignment alone.\n","authors":["Raviraj Joshi","Kanishk Singla","Anusha Kamath","Raunak Kalani","Rakesh Paul","Utkarsh Vaidya","Sanjay Singh Chauhan","Niranjan Wartikar","Eileen Long"],"pdf_url":"https://arxiv.org/pdf/2410.14815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09407v2","updated":"2025-04-21T05:22:55Z","published":"2025-04-13T02:34:22Z","title":"UXAgent: A System for Simulating Usability Testing of Web Design with\n  LLM Agents","summary":"  Usability testing is a fundamental research method that user experience (UX)\nresearchers use to evaluate and iterate a web design, but\\textbf{ how to\nevaluate and iterate the usability testing study design } itself? Recent\nadvances in Large Language Model-simulated Agent (\\textbf{LLM Agent}) research\ninspired us to design \\textbf{UXAgent} to support UX researchers in evaluating\nand reiterating their usability testing study design before they conduct the\nreal human-subject study. Our system features a Persona Generator module, an\nLLM Agent module, and a Universal Browser Connector module to automatically\ngenerate thousands of simulated users to interactively test the target website.\nThe system also provides an Agent Interview Interface and a Video Replay\nInterface so that the UX researchers can easily review and analyze the\ngenerated qualitative and quantitative log data. Through a heuristic\nevaluation, five UX researcher participants praised the innovation of our\nsystem but also expressed concerns about the future of LLM Agent usage in UX\nstudies.\n","authors":["Yuxuan Lu","Bingsheng Yao","Hansu Gu","Jing Huang","Jessie Wang","Yang Li","Jiri Gesi","Qi He","Toby Jia-Jun Li","Dakuo Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20749v4","updated":"2025-04-21T05:12:56Z","published":"2025-03-26T17:33:27Z","title":"LLM Agents That Act Like Us: Accurate Human Behavior Simulation with\n  Real-World Data","summary":"  Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating and improving LLM's objective ``accuracy'' rather than the\nsubjective ``believability'' in the web action generation task, leveraging a\nlarge-scale, real-world dataset collected from online shopping human actions.\nWe present the first comprehensive quantitative evaluation of state-of-the-art\nLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web action\ngeneration. Our results show that fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate actions compared to\nprompt-only methods. Furthermore, incorporating synthesized reasoning traces\ninto model training leads to additional performance gains, demonstrating the\nvalue of explicit rationale in behavior modeling. This work establishes a new\nbenchmark for evaluating LLMs in behavior simulation and offers actionable\ninsights into how real-world action data and reasoning augmentation can enhance\nthe fidelity of LLM agents.\n","authors":["Yuxuan Lu","Jing Huang","Yan Han","Bennet Bei","Yaochen Xie","Dakuo Wang","Jessie Wang","Qi He"],"pdf_url":"https://arxiv.org/pdf/2503.20749v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10663v2","updated":"2025-04-21T05:07:13Z","published":"2025-04-14T19:30:30Z","title":"Characterizing Knowledge Manipulation in a Russian Wikipedia Fork","summary":"  Wikipedia is powered by MediaWiki, a free and open-source software that is\nalso the infrastructure for many other wiki-based online encyclopedias. These\ninclude the recently launched website Ruwiki, which has copied and modified the\noriginal Russian Wikipedia content to conform to Russian law. To identify\npractices and narratives that could be associated with different forms of\nknowledge manipulation, this article presents an in-depth analysis of this\nRussian Wikipedia fork. We propose a methodology to characterize the main\nchanges with respect to the original version. The foundation of this study is a\ncomprehensive comparative analysis of more than 1.9M articles from Russian\nWikipedia and its fork. Using meta-information and geographical, temporal,\ncategorical, and textual features, we explore the changes made by Ruwiki\neditors. Furthermore, we present a classification of the main topics of\nknowledge manipulation in this fork, including a numerical estimation of their\nscope. This research not only sheds light on significant changes within Ruwiki,\nbut also provides a methodology that could be applied to analyze other\nWikipedia forks and similar collaborative projects.\n","authors":["Mykola Trokhymovych","Oleksandr Kosovan","Nathan Forrester","Pablo Aragón","Diego Saez-Trumper","Ricardo Baeza-Yates"],"pdf_url":"https://arxiv.org/pdf/2504.10663v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14858v1","updated":"2025-04-21T04:56:47Z","published":"2025-04-21T04:56:47Z","title":"AlignRAG: An Adaptable Framework for Resolving Misalignments in\n  Retrieval-Aware Reasoning of RAG","summary":"  Retrieval-augmented generation (RAG) has emerged as a foundational paradigm\nfor knowledge-grounded text generation. However, existing RAG pipelines often\nfail to ensure that the reasoning trajectories align with the evidential\nconstraints imposed by retrieved content. In this paper, we reframe RAG as a\nproblem of retrieval-aware reasoning and identify a core challenge: reasoning\nmisalignment-the mismatch between a model's reasoning trajectory and the\nretrieved evidence. To address this challenge, we propose AlignRAG, a novel\ntest-time framework that mitigates reasoning misalignment through iterative\nCritique-Driven Alignment (CDA) steps. In contrast to prior approaches that\nrely on static training or post-hoc selection, AlignRAG actively refines\nreasoning trajectories during inference by enforcing fine-grained alignment\nwith evidence. Our framework introduces a new paradigm for retrieval-aware\nreasoning by: (1) constructing context-rich training corpora; (2) generating\ncontrastive critiques from preference-aware reasoning trajectories; (3)\ntraining a dedicated \\textit{Critic Language Model (CLM)} to identify reasoning\nmisalignments; and (4) applying CDA steps to optimize reasoning trajectories\niteratively. Empirical results demonstrate that AlignRAG consistently\noutperforms all baselines and could integrate as a plug-and-play module into\nexisting RAG pipelines without further changes. By reconceptualizing RAG as a\nstructured reasoning trajectory and establishing the test-time framework for\ncorrecting reasoning misalignments in RAG, AlignRAG provides practical\nadvancements for retrieval-aware generation.\n","authors":["Jiaqi Wei","Hao Zhou","Xiang Zhang","Di Zhang","Zijie Qiu","Wei Wei","Jinzhe Li","Wanli Ouyang","Siqi Sun"],"pdf_url":"https://arxiv.org/pdf/2504.14858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14856v1","updated":"2025-04-21T04:50:16Z","published":"2025-04-21T04:50:16Z","title":"Transparentize the Internal and External Knowledge Utilization in LLMs\n  with Trustworthy Citation","summary":"  While hallucinations of large language models could been alleviated through\nretrieval-augmented generation and citation generation, how the model utilizes\ninternal knowledge is still opaque, and the trustworthiness of its generated\nanswers remains questionable. In this work, we introduce Context-Prior\nAugmented Citation Generation task, requiring models to generate citations\nconsidering both external and internal knowledge while providing trustworthy\nreferences, with 5 evaluation metrics focusing on 3 aspects: answer\nhelpfulness, citation faithfulness, and trustworthiness. We introduce RAEL, the\nparadigm for our task, and also design INTRALIGN, an integrated method\ncontaining customary data generation and an alignment algorithm. Our\nexperimental results show that our method achieves a better cross-scenario\nperformance with regard to other baselines. Our extended experiments further\nreveal that retrieval quality, question types, and model knowledge have\nconsiderable influence on the trustworthiness in citation generation.\n","authors":["Jiajun Shen","Tong Zhou","Yubo Chen","Delai Qiu","Shengping Liu","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.14856v1.pdf","comment":"19 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.12300v3","updated":"2025-04-21T04:16:29Z","published":"2024-12-16T19:11:55Z","title":"Unanswerability Evaluation for Retrieval Augmented Generation","summary":"  Existing evaluation frameworks for retrieval-augmented generation (RAG)\nsystems focus on answerable queries, but they overlook the importance of\nappropriately rejecting unanswerable requests. In this paper, we introduce\nUAEval4RAG, a framework designed to evaluate whether RAG systems can handle\nunanswerable queries effectively. We define a taxonomy with six unanswerable\ncategories, and UAEval4RAG automatically synthesizes diverse and challenging\nqueries for any given knowledge base with unanswered ratio and acceptable ratio\nmetrics. We conduct experiments with various RAG components, including\nretrieval models, rewriting methods, rerankers, language models, and prompting\nstrategies, and reveal hidden trade-offs in performance of RAG systems. Our\nfindings highlight the critical role of component selection and prompt design\nin optimizing RAG systems to balance the accuracy of answerable queries with\nhigh rejection rates of unanswerable ones. UAEval4RAG provides valuable\ninsights and tools for developing more robust and reliable RAG systems.\n","authors":["Xiangyu Peng","Prafulla Kumar Choubey","Caiming Xiong","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2412.12300v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11192v3","updated":"2025-04-21T04:11:32Z","published":"2024-06-17T03:57:35Z","title":"Beyond Boundaries: Learning a Universal Entity Taxonomy across Datasets\n  and Languages for Open Named Entity Recognition","summary":"  Open Named Entity Recognition (NER), which involves identifying arbitrary\ntypes of entities from arbitrary domains, remains challenging for Large\nLanguage Models (LLMs). Recent studies suggest that fine-tuning LLMs on\nextensive NER data can boost their performance. However, training directly on\nexisting datasets neglects their inconsistent entity definitions and redundant\ndata, limiting LLMs to dataset-specific learning and hindering out-of-domain\nadaptation. To address this, we present B2NERD, a compact dataset designed to\nguide LLMs' generalization in Open NER under a universal entity taxonomy.\nB2NERD is refined from 54 existing English and Chinese datasets using a\ntwo-step process. First, we detect inconsistent entity definitions across\ndatasets and clarify them by distinguishable label names to construct a\nuniversal taxonomy of 400+ entity types. Second, we address redundancy using a\ndata pruning strategy that selects fewer samples with greater category and\nsemantic diversity. Comprehensive evaluation shows that B2NERD significantly\nenhances LLMs' Open NER capabilities. Our B2NER models, trained on B2NERD,\noutperform GPT-4 by 6.8-12.0 F1 points and surpass previous methods in 3\nout-of-domain benchmarks across 15 datasets and 6 languages. The data, models,\nand code are publicly available at https://github.com/UmeanNever/B2NER.\n","authors":["Yuming Yang","Wantong Zhao","Caishuang Huang","Junjie Ye","Xiao Wang","Huiyuan Zheng","Yang Nan","Yuran Wang","Xueying Xu","Kaixin Huang","Yunke Zhang","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2406.11192v3.pdf","comment":"Accepted at COLING 2025. Camera-ready version updated. Project page:\n  https://github.com/UmeanNever/B2NER"},{"id":"http://arxiv.org/abs/2504.03931v2","updated":"2025-04-21T04:07:33Z","published":"2025-04-04T20:57:41Z","title":"NAACL2025 Tutorial: Adaptation of Large Language Models","summary":"  This tutorial on adaptation of LLMs is designed to address the growing demand\nfor models that go beyond the static capabilities of generic LLMs by providing\nan overview of dynamic, domain-specific, and task-adaptive LLM adaptation\ntechniques. While general LLMs have demonstrated strong generalization across a\nvariety of tasks, they often struggle to perform well in specialized domains\nsuch as finance, healthcare, and code generation for underrepresented\nlanguages. Additionally, their static nature limits their ability to evolve\nwith the changing world, and they are often extremely large in size, making\nthem impractical and costly to deploy at scale. As a result, the adaptation of\nLLMs has drawn much attention since the birth of LLMs and is of core\nimportance, both for industry, which focuses on serving its targeted users, and\nacademia, which can greatly benefit from small but powerful LLMs. To address\nthis gap, this tutorial aims to provide an overview of the LLM adaptation\ntechniques. We start with an introduction to LLM adaptation, from both the data\nperspective and the model perspective. We then emphasize how the evaluation\nmetrics and benchmarks are different from other techniques. After establishing\nthe problems, we explore various adaptation techniques. We categorize\nadaptation techniques into two main families. The first is parametric knowledge\nadaptation, which focuses on updating the parametric knowledge within LLMs.\nAdditionally, we will discuss real-time adaptation techniques, including model\nediting, which allows LLMs to be updated dynamically in production\nenvironments. The second kind of adaptation is semi-parametric knowledge\nadaptation, where the goal is to update LLM parameters to better leverage\nexternal knowledge or tools through techniques like retrieval-augmented\ngeneration (RAG) and agent-based systems.\n","authors":["Zixuan Ke","Yifei Ming","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2504.03931v2.pdf","comment":"NAACL2025 Tutorial"},{"id":"http://arxiv.org/abs/2410.14148v4","updated":"2025-04-21T04:04:53Z","published":"2024-10-18T03:34:32Z","title":"Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment","summary":"  The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.\n","authors":["Chenhang Cui","An Zhang","Yiyang Zhou","Zhaorun Chen","Gelei Deng","Huaxiu Yao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.14148v4.pdf","comment":"23 pages; Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2409.01035v4","updated":"2025-04-21T03:58:50Z","published":"2024-09-02T08:10:51Z","title":"Task-Specific Directions: Definition, Exploration, and Utilization in\n  Parameter Efficient Fine-Tuning","summary":"  Large language models demonstrate impressive performance on downstream tasks,\nyet they require extensive resource consumption when fully fine-tuning all\nparameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT)\nstrategies, such as LoRA, have been developed. In this paper, we delve into the\nconcept of task-specific directions (TSDs), which are critical for\ntransitioning large models from pretrained states to task-specific enhancements\nin PEFT. We propose a framework to clearly define these directions and explore\ntheir properties and practical utilization challenges. We then introduce a\nnovel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the\nfine-tuning process, thereby enhancing model performance on targeted tasks.\nAdditionally, based on our exploration of TSD, we focus on an important issue\nin PEFT: the initialization of LoRA. While some works have pointed out the\nsignificance of initialization for LoRA's performance and proposed various\nstrategies, these methods are often empirical and not task-specific. To address\nthis issue, we propose LoRA-Init. Starting from TSD, we identify the directions\nthat require the most adjustment during fine-tuning for downstream tasks. By\ninitializing the matrices in LoRA with these directions, LoRA-Init\nsignificantly enhances LoRA's performance. Moreover, we can combine LoRA-Dash\nand LoRA-Init to create the final version of LoRA based on TSDs, which we refer\nto as LoRA-TSD. Extensive experiments have conclusively demonstrated the\neffectiveness of these methods, and in-depth analyses further reveal the\nunderlying mechanisms behind their success.\n","authors":["Chongjie Si","Zhiyi Shi","Shifan Zhang","Xiaokang Yang","Hanspeter Pfister","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2409.01035v4.pdf","comment":"Codes in https://github.com/Chongjie-Si/Subspace-Tuning"},{"id":"http://arxiv.org/abs/2411.05735v2","updated":"2025-04-21T03:50:23Z","published":"2024-11-08T17:50:24Z","title":"Aioli: A Unified Optimization Framework for Language Model Data Mixing","summary":"  Language model performance depends on identifying the optimal mixture of data\ngroups to train on (e.g., law, code, math). Prior work has proposed a diverse\nset of methods to efficiently learn mixture proportions, ranging from fitting\nregression models over training runs to dynamically updating proportions\nthroughout training. Surprisingly, we find that no existing method consistently\noutperforms a simple stratified sampling baseline in terms of average test\nperplexity. To understand this inconsistency, we unify existing methods into a\nstandard framework, showing they are equivalent to solving a common\noptimization problem: minimize average loss subject to a method-specific mixing\nlaw -- an implicit assumption on the relationship between loss and mixture\nproportions. This framework suggests that measuring the fidelity of a method's\nmixing law can offer insights into its performance. Empirically, we find that\nexisting methods set their mixing law parameters inaccurately, resulting in the\ninconsistent mixing performance we observe. Using this insight, we derive a new\nonline method named Aioli, which directly estimates the mixing law parameters\nthroughout training and uses them to dynamically adjust proportions. Aioli\noutperforms stratified sampling on 6 out of 6 datasets by an average of 0.27\ntest perplexity points, whereas existing methods fail to consistently beat\nstratified sampling, doing up to 6.9 points worse. Moreover, in a practical\nsetting where proportions are learned on shorter runs due to computational\nconstraints, Aioli can dynamically adjust these proportions over the full\ntraining run, consistently improving performance over existing methods by up to\n12.012 test perplexity points.\n","authors":["Mayee F. Chen","Michael Y. Hu","Nicholas Lourie","Kyunghyun Cho","Christopher Ré"],"pdf_url":"https://arxiv.org/pdf/2411.05735v2.pdf","comment":"ICLR 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2504.13592v2","updated":"2025-04-21T03:29:14Z","published":"2025-04-18T09:52:12Z","title":"Improving Generalization in Intent Detection: GRPO with Reward-Based\n  Curriculum Sampling","summary":"  Intent detection, a critical component in task-oriented dialogue (TOD)\nsystems, faces significant challenges in adapting to the rapid influx of\nintegrable tools with complex interrelationships. Existing approaches, such as\nzero-shot reformulations and LLM-based dynamic recognition, struggle with\nperformance degradation when encountering unseen intents, leading to erroneous\ntask routing. To enhance the model's generalization performance on unseen\ntasks, we employ Reinforcement Learning (RL) combined with a Reward-based\nCurriculum Sampling (RCS) during Group Relative Policy Optimization (GRPO)\ntraining in intent detection tasks. Experiments demonstrate that RL-trained\nmodels substantially outperform supervised fine-tuning (SFT) baselines in\ngeneralization. Besides, the introduction of the RCS, significantly bolsters\nthe effectiveness of RL in intent detection by focusing the model on\nchallenging cases during training. Moreover, incorporating Chain-of-Thought\n(COT) processes in RL notably improves generalization in complex intent\ndetection tasks, underscoring the importance of thought in challenging\nscenarios. This work advances the generalization of intent detection tasks,\noffering practical insights for deploying adaptable dialogue systems.\n","authors":["Zihao Feng","Xiaoxue Wang","Ziwei Bai","Donghang Su","Bowen Wu","Qun Yu","Baoxun Wang"],"pdf_url":"https://arxiv.org/pdf/2504.13592v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13775v2","updated":"2025-04-21T03:12:50Z","published":"2025-04-18T16:22:41Z","title":"BadApex: Backdoor Attack Based on Adaptive Optimization Mechanism of\n  Black-box Large Language Models","summary":"  Previous insertion-based and paraphrase-based backdoors have achieved great\nsuccess in attack efficacy, but they ignore the text quality and semantic\nconsistency between poisoned and clean texts. Although recent studies introduce\nLLMs to generate poisoned texts and improve the stealthiness, semantic\nconsistency, and text quality, their hand-crafted prompts rely on expert\nexperiences, facing significant challenges in prompt adaptability and attack\nperformance after defenses. In this paper, we propose a novel backdoor attack\nbased on adaptive optimization mechanism of black-box large language models\n(BadApex), which leverages a black-box LLM to generate poisoned text through a\nrefined prompt. Specifically, an Adaptive Optimization Mechanism is designed to\nrefine an initial prompt iteratively using the generation and modification\nagents. The generation agent generates the poisoned text based on the initial\nprompt. Then the modification agent evaluates the quality of the poisoned text\nand refines a new prompt. After several iterations of the above process, the\nrefined prompt is used to generate poisoned texts through LLMs. We conduct\nextensive experiments on three dataset with six backdoor attacks and two\ndefenses. Extensive experimental results demonstrate that BadApex significantly\noutperforms state-of-the-art attacks. It improves prompt adaptability, semantic\nconsistency, and text quality. Furthermore, when two defense methods are\napplied, the average attack success rate (ASR) still up to 96.75%.\n","authors":["Zhengxian Wu","Juan Wen","Wanli Peng","Ziwei Zhang","Yinghan Zhou","Yiming Xue"],"pdf_url":"https://arxiv.org/pdf/2504.13775v2.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2406.14191v3","updated":"2025-04-21T03:04:15Z","published":"2024-06-20T10:51:06Z","title":"Temporal Knowledge Graph Question Answering: A Survey","summary":"  Knowledge Base Question Answering (KBQA) has been a long-standing field to\nanswer questions based on knowledge bases. Recently, the evolving dynamics of\nknowledge have attracted a growing interest in Temporal Knowledge Graph\nQuestion Answering (TKGQA), an emerging task to answer temporal questions.\nHowever, this field grapples with ambiguities in defining temporal questions\nand lacks a systematic categorization of existing methods for TKGQA. In\nresponse, this paper provides a thorough survey from two perspectives: the\ntaxonomy of temporal questions and the methodological categorization for TKGQA.\nSpecifically, we first establish a detailed taxonomy of temporal questions\nengaged in prior studies. Subsequently, we provide a comprehensive review of\nTKGQA techniques of two categories: semantic parsing-based and TKG\nembedding-based. Building on this review, the paper outlines potential research\ndirections aimed at advancing the field of TKGQA. This work aims to serve as a\ncomprehensive reference for TKGQA and to stimulate further research.\n","authors":["Miao Su","Zixuan Li","Zhuo Chen","Long Bai","Xiaolong Jin","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2406.14191v3.pdf","comment":"8 pages, 3 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2504.14822v1","updated":"2025-04-21T02:57:23Z","published":"2025-04-21T02:57:23Z","title":"Completing A Systematic Review in Hours instead of Months with\n  Interactive AI Agents","summary":"  Systematic reviews (SRs) are vital for evidence-based practice in high stakes\ndisciplines, such as healthcare, but are often impeded by intensive labors and\nlengthy processes that can take months to complete. Due to the high demand for\ndomain expertise, existing automatic summarization methods fail to accurately\nidentify relevant studies and generate high-quality summaries. To that end, we\nintroduce InsightAgent, a human-centered interactive AI agent powered by large\nlanguage models that revolutionize this workflow. InsightAgent partitions a\nlarge literature corpus based on semantics and employs a multi-agent design for\nmore focused processing of literature, leading to significant improvement in\nthe quality of generated SRs. InsightAgent also provides intuitive\nvisualizations of the corpus and agent trajectories, allowing users to\neffortlessly monitor the actions of the agent and provide real-time feedback\nbased on their expertise. Our user studies with 9 medical professionals\ndemonstrate that the visualization and interaction mechanisms can effectively\nimprove the quality of synthesized SRs by 27.2%, reaching 79.7% of\nhuman-written quality. At the same time, user satisfaction is improved by\n34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather\nthan months, to complete a high-quality systematic review.\n","authors":["Rui Qiu","Shijie Chen","Yu Su","Po-Yin Yen","Han-Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2504.14822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15208v2","updated":"2025-04-21T02:43:54Z","published":"2023-11-26T06:24:25Z","title":"LongStory: Coherent, Complete and Length Controlled Long story\n  Generation","summary":"  A human author can write any length of story without losing coherence. Also,\nthey always bring the story to a proper ending, an ability that current\nlanguage models lack. In this work, we present the LongStory for coherent,\ncomplete, and length-controlled long story generation. LongStory introduces two\nnovel methodologies: (1) the long and short-term contexts weight calibrator\n(CWC) and (2) long story structural positions (LSP). The CWC adjusts weights\nfor long-term context Memory and short-term context Cheating, acknowledging\ntheir distinct roles. The LSP employs discourse tokens to convey the structural\npositions of a long story. Trained on three datasets with varied average story\nlengths, LongStory outperforms other baselines, including the strong story\ngenerator Plotmachine, in coherence, completeness, relevance, and\nrepetitiveness. We also perform zero-shot tests on each dataset to assess the\nmodel's ability to predict outcomes beyond its training data and validate our\nmethodology by comparing its performance with variants of our model.\n","authors":["Kyeongman Park","Nakyeong Yang","Kyomin Jung"],"pdf_url":"https://arxiv.org/pdf/2311.15208v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07582v2","updated":"2025-04-21T02:22:06Z","published":"2024-10-10T03:31:16Z","title":"Detecting Training Data of Large Language Models via Expectation\n  Maximization","summary":"  The advancement of large language models has grown parallel to the opacity of\ntheir training data. Membership inference attacks (MIAs) aim to determine\nwhether specific data was used to train a model. They offer valuable insights\ninto detecting data contamination and ensuring compliance with privacy and\ncopyright standards. However, MIA for LLMs is challenging due to the massive\nscale of training data and the inherent ambiguity of membership in texts.\nMoreover, creating realistic MIA evaluation benchmarks is difficult as training\nand test data distributions are often unknown. We introduce EM-MIA, a novel\nmembership inference method that iteratively refines membership scores and\nprefix scores via an expectation-maximization algorithm. Our approach leverages\nthe observation that these scores can improve each other: membership scores\nhelp identify effective prefixes for detecting training data, while prefix\nscores help determine membership. As a result, EM-MIA achieves state-of-the-art\nresults on WikiMIA. To enable comprehensive evaluation, we introduce OLMoMIA, a\nbenchmark built from OLMo resources, which allows controlling task difficulty\nthrough varying degrees of overlap between training and test data\ndistributions. Our experiments demonstrate EM-MIA is robust across different\nscenarios while also revealing fundamental limitations of current MIA\napproaches when member and non-member distributions are nearly identical.\n","authors":["Gyuwan Kim","Yang Li","Evangelia Spiliopoulou","Jie Ma","Miguel Ballesteros","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.07582v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2504.14808v1","updated":"2025-04-21T02:17:19Z","published":"2025-04-21T02:17:19Z","title":"On Self-improving Token Embeddings","summary":"  This article introduces a novel and fast method for refining pre-trained\nstatic word or, more generally, token embeddings. By incorporating the\nembeddings of neighboring tokens in text corpora, it continuously updates the\nrepresentation of each token, including those without pre-assigned embeddings.\nThis approach effectively addresses the out-of-vocabulary problem, too.\nOperating independently of large language models and shallow neural networks,\nit enables versatile applications such as corpus exploration, conceptual\nsearch, and word sense disambiguation. The method is designed to enhance token\nrepresentations within topically homogeneous corpora, where the vocabulary is\nrestricted to a specific domain, resulting in more meaningful embeddings\ncompared to general-purpose pre-trained vectors. As an example, the methodology\nis applied to explore storm events and their impacts on infrastructure and\ncommunities using narratives from a subset of the NOAA Storm Events database.\nThe article also demonstrates how the approach improves the representation of\nstorm-related terms over time, providing valuable insights into the evolving\nnature of disaster narratives.\n","authors":["Mario M. Kubek","Shiraj Pokharel","Thomas Böhme","Emma L. McDaniel","Herwig Unger","Armin R. Mikler"],"pdf_url":"https://arxiv.org/pdf/2504.14808v1.pdf","comment":"18 pages, 4 figures, 3 tables, accepted at the 2025 25th\n  International Conference on Innovations for Community Services (I4CS), June\n  11 - 13, Munich, Germany, 2025"},{"id":"http://arxiv.org/abs/2504.14804v1","updated":"2025-04-21T02:08:42Z","published":"2025-04-21T02:08:42Z","title":"Automatic Evaluation Metrics for Document-level Translation: Overview,\n  Challenges and Trends","summary":"  With the rapid development of deep learning technologies, the field of\nmachine translation has witnessed significant progress, especially with the\nadvent of large language models (LLMs) that have greatly propelled the\nadvancement of document-level translation. However, accurately evaluating the\nquality of document-level translation remains an urgent issue. This paper first\nintroduces the development status of document-level translation and the\nimportance of evaluation, highlighting the crucial role of automatic evaluation\nmetrics in reflecting translation quality and guiding the improvement of\ntranslation systems. It then provides a detailed analysis of the current state\nof automatic evaluation schemes and metrics, including evaluation methods with\nand without reference texts, as well as traditional metrics, Model-based\nmetrics and LLM-based metrics. Subsequently, the paper explores the challenges\nfaced by current evaluation methods, such as the lack of reference diversity,\ndependence on sentence-level alignment information, and the bias, inaccuracy,\nand lack of interpretability of the LLM-as-a-judge method. Finally, the paper\nlooks ahead to the future trends in evaluation methods, including the\ndevelopment of more user-friendly document-level evaluation methods and more\nrobust LLM-as-a-judge methods, and proposes possible research directions, such\nas reducing the dependency on sentence-level information, introducing\nmulti-level and multi-granular evaluation approaches, and training models\nspecifically for machine translation evaluation. This study aims to provide a\ncomprehensive analysis of automatic evaluation for document-level translation\nand offer insights into future developments.\n","authors":["Jiaxin GUO","Xiaoyu Chen","Zhiqiang Rao","Jinlong Yang","Zongyao Li","Hengchao Shang","Daimeng Wei","Hao Yang"],"pdf_url":"https://arxiv.org/pdf/2504.14804v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13406v2","updated":"2025-04-21T02:00:43Z","published":"2025-04-18T02:03:14Z","title":"LangCoop: Collaborative Driving with Language","summary":"  Multi-agent collaboration holds great promise for enhancing the safety,\nreliability, and mobility of autonomous driving systems by enabling information\nsharing among multiple connected agents. However, existing multi-agent\ncommunication approaches are hindered by limitations of existing communication\nmedia, including high bandwidth demands, agent heterogeneity, and information\nloss. To address these challenges, we introduce LangCoop, a new paradigm for\ncollaborative autonomous driving that leverages natural language as a compact\nyet expressive medium for inter-agent communication. LangCoop features two key\ninnovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured\nzero-shot vision-language reasoning and Natural Language Information Packaging\n(LangPack) for efficiently packaging information into concise, language-based\nmessages. Through extensive experiments conducted in the CARLA simulations, we\ndemonstrate that LangCoop achieves a remarkable 96\\% reduction in communication\nbandwidth (< 2KB per message) compared to image-based communication, while\nmaintaining competitive driving performance in the closed-loop evaluation. Our\nproject page and code are at https://xiangbogaobarry.github.io/LangCoop/.\n","authors":["Xiangbo Gao","Yuheng Wu","Rujia Wang","Chenxi Liu","Yang Zhou","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2504.13406v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05693v2","updated":"2025-04-21T01:24:31Z","published":"2023-12-09T22:12:52Z","title":"Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs\n  on the Edge","summary":"  Large Language Models (LLMs) stand out for their impressive performance in\nintricate language modeling tasks. However, their demanding computational and\nmemory needs pose obstacles for broad use on edge devices. Quantization is then\nintroduced to boost LLMs' on-device efficiency. Recent works show that 8-bit or\nlower weight quantization is feasible with minimal impact on end-to-end task\nperformance, while the activation is still not quantized. On the other hand,\nmainstream commodity edge devices still struggle to execute these sub-8-bit\nquantized networks effectively. In this paper, we propose Agile-Quant, an\nactivation-guided quantization framework for popular Large Language Models\n(LLMs), and implement an end-to-end accelerator on multiple edge devices for\nfaster inference. Considering the hardware profiling and activation analysis,\nwe first introduce a basic activation quantization strategy to balance the\ntrade-off of task performance and real inference speed. Then we leverage the\nactivation-aware token pruning technique to reduce the outliers and the adverse\nimpact on attentivity. Ultimately, we utilize the SIMD-based 4-bit multiplier\nand our efficient TRIP matrix multiplication to implement the accelerator for\nLLMs on the edge. We apply our framework on different scales of LLMs including\nLLaMA, OPT, and BLOOM with 4-bit or 8-bit for the activation and 4-bit for the\nweight quantization. Experiments show that Agile-Quant achieves simultaneous\nquantization of model weights and activations while maintaining task\nperformance comparable to existing weight-only quantization methods. Moreover,\nin the 8- and 4-bit scenario, Agile-Quant achieves an on-device speedup of up\nto 2.55x compared to its FP16 counterparts across multiple edge devices,\nmarking a pioneering advancement in this domain. Code:\nhttps://github.com/shawnricecake/agile-quant\n","authors":["Xuan Shen","Peiyan Dong","Lei Lu","Zhenglun Kong","Zhengang Li","Ming Lin","Chao Wu","Yanzhi Wang"],"pdf_url":"https://arxiv.org/pdf/2312.05693v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2504.14773v1","updated":"2025-04-21T00:02:50Z","published":"2025-04-21T00:02:50Z","title":"PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning\n  Capabilities","summary":"  Planning is central to agents and agentic AI. The ability to plan, e.g.,\ncreating travel itineraries within a budget, holds immense potential in both\nscientific and commercial contexts. Moreover, optimal plans tend to require\nfewer resources compared to ad-hoc methods. To date, a comprehensive\nunderstanding of existing planning benchmarks appears to be lacking. Without\nit, comparing planning algorithms' performance across domains or selecting\nsuitable algorithms for new scenarios remains challenging. In this paper, we\nexamine a range of planning benchmarks to identify commonly used testbeds for\nalgorithm development and highlight potential gaps. These benchmarks are\ncategorized into embodied environments, web navigation, scheduling, games and\npuzzles, and everyday task automation. Our study recommends the most\nappropriate benchmarks for various algorithms and offers insights to guide\nfuture benchmark development.\n","authors":["Haoming Li","Zhaoliang Chen","Jonathan Zhang","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2504.14773v1.pdf","comment":"10 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2504.15281v1","updated":"2025-04-21T17:59:55Z","published":"2025-04-21T17:59:55Z","title":"StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on\n  3D Gaussians","summary":"  3D Gaussian Splatting (3DGS) excels in photorealistic scene reconstruction\nbut struggles with stylized scenarios (e.g., cartoons, games) due to fragmented\ntextures, semantic misalignment, and limited adaptability to abstract\naesthetics. We propose StyleMe3D, a holistic framework for 3D GS style transfer\nthat integrates multi-modal style conditioning, multi-level semantic alignment,\nand perceptual quality enhancement. Our key insights include: (1) optimizing\nonly RGB attributes preserves geometric integrity during stylization; (2)\ndisentangling low-, medium-, and high-level semantics is critical for coherent\nstyle transfer; (3) scalability across isolated objects and complex scenes is\nessential for practical deployment. StyleMe3D introduces four novel components:\nDynamic Style Score Distillation (DSSD), leveraging Stable Diffusion's latent\nspace for semantic alignment; Contrastive Style Descriptor (CSD) for localized,\ncontent-aware texture transfer; Simultaneously Optimized Scale (SOS) to\ndecouple style details and structural coherence; and 3D Gaussian Quality\nAssessment (3DG-QA), a differentiable aesthetic prior trained on human-rated\ndata to suppress artifacts and enhance visual harmony. Evaluated on NeRF\nsynthetic dataset (objects) and tandt db (scenes) datasets, StyleMe3D\noutperforms state-of-the-art methods in preserving geometric details (e.g.,\ncarvings on sculptures) and ensuring stylistic consistency across scenes (e.g.,\ncoherent lighting in landscapes), while maintaining real-time rendering. This\nwork bridges photorealistic 3D GS and artistic stylization, unlocking\napplications in gaming, virtual worlds, and digital art.\n","authors":["Cailin Zhuang","Yaoqi Hu","Xuanyang Zhang","Wei Cheng","Jiacheng Bao","Shengqi Liu","Yiying Yang","Xianfang Zeng","Gang Yu","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2504.15281v1.pdf","comment":"16 pages; Project page: https://styleme3d.github.io/"},{"id":"http://arxiv.org/abs/2504.15279v1","updated":"2025-04-21T17:59:53Z","published":"2025-04-21T17:59:53Z","title":"VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal\n  Large Language Models","summary":"  Visual reasoning is a core component of human intelligence and a critical\ncapability for advanced multimodal models. Yet current reasoning evaluations of\nmultimodal large language models (MLLMs) often rely on text descriptions and\nallow language-based reasoning shortcuts, failing to measure genuine\nvision-centric reasoning. To address this, we introduce VisuLogic: a benchmark\nof 1,000 human-verified problems across six categories (e.g., quantitative\nshifts, spatial relations, attribute comparisons). These various types of\nquestions can be evaluated to assess the visual reasoning capabilities of MLLMs\nfrom multiple perspectives. We evaluate leading MLLMs on this benchmark and\nanalyze their results to identify common failure modes. Most models score below\n30% accuracy-only slightly above the 25% random baseline and far below the\n51.4% achieved by humans-revealing significant gaps in visual reasoning.\nFurthermore, we provide a supplementary training dataset and a\nreinforcement-learning baseline to support further progress.\n","authors":["Weiye Xu","Jiahao Wang","Weiyun Wang","Zhe Chen","Wengang Zhou","Aijun Yang","Lewei Lu","Houqiang Li","Xiaohua Wang","Xizhou Zhu","Wenhai Wang","Jifeng Dai","Jinguo Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.15279v1.pdf","comment":"Code, data, and baselines are available at\n  https://visulogic-benchmark.github.io/VisuLogic"},{"id":"http://arxiv.org/abs/2504.15280v1","updated":"2025-04-21T17:59:53Z","published":"2025-04-21T17:59:53Z","title":"Seeing from Another Perspective: Evaluating Multi-View Understanding in\n  MLLMs","summary":"  Multi-view understanding, the ability to reconcile visual information across\ndiverse viewpoints for effective navigation, manipulation, and 3D scene\ncomprehension, is a fundamental challenge in Multi-Modal Large Language Models\n(MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive\nadvances in high-level reasoning and planning, they frequently fall short when\nconfronted with multi-view geometric consistency and cross-view correspondence.\nTo comprehensively evaluate the challenges of MLLMs in multi-view scene\nreasoning, we propose All-Angles Bench, a benchmark of over 2,100 human\ncarefully annotated multi-view question-answer pairs across 90 diverse\nreal-world scenes. Our six tasks (counting, attribute identification, relative\ndistance, relative direction, object manipulation, and camera pose estimation)\nspecifically test model's geometric correspondence and the capacity to align\ninformation consistently across views. Our extensive experiments, benchmark on\n27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and\nGPT-4o against human evaluators reveals a substantial performance gap,\nindicating that current MLLMs remain far from human-level proficiency. Through\nin-depth analysis, we show that MLLMs are particularly underperforming under\ntwo aspects: (1) cross-view correspondence for partially occluded views and (2)\nestablishing the coarse camera poses. These findings highlight the necessity of\ndomain-specific refinements or modules that embed stronger multi-view\nawareness. We believe that our All-Angles Bench offers valuable insights and\ncontribute to bridging the gap between MLLMs and human-level multi-view\nunderstanding. The project and benchmark are publicly available at\nhttps://danielchyeh.github.io/All-Angles-Bench/.\n","authors":["Chun-Hsiao Yeh","Chenyu Wang","Shengbang Tong","Ta-Ying Cheng","Rouyu Wang","Tianzhe Chu","Yuexiang Zhai","Yubei Chen","Shenghua Gao","Yi Ma"],"pdf_url":"https://arxiv.org/pdf/2504.15280v1.pdf","comment":"Project page: https://danielchyeh.github.io/All-Angles-Bench/"},{"id":"http://arxiv.org/abs/2504.15278v1","updated":"2025-04-21T17:59:49Z","published":"2025-04-21T17:59:49Z","title":"DRAWER: Digital Reconstruction and Articulation With Environment Realism","summary":"  Creating virtual digital replicas from real-world data unlocks significant\npotential across domains like gaming and robotics. In this paper, we present\nDRAWER, a novel framework that converts a video of a static indoor scene into a\nphotorealistic and interactive digital environment. Our approach centers on two\nmain contributions: (i) a reconstruction module based on a dual scene\nrepresentation that reconstructs the scene with fine-grained geometric details,\nand (ii) an articulation module that identifies articulation types and hinge\npositions, reconstructs simulatable shapes and appearances and integrates them\ninto the scene. The resulting virtual environment is photorealistic,\ninteractive, and runs in real time, with compatibility for game engines and\nrobotic simulation platforms. We demonstrate the potential of DRAWER by using\nit to automatically create an interactive game in Unreal Engine and to enable\nreal-to-sim-to-real transfer for robotics applications.\n","authors":["Hongchi Xia","Entong Su","Marius Memmel","Arhan Jain","Raymond Yu","Numfor Mbiziwo-Tiapo","Ali Farhadi","Abhishek Gupta","Shenlong Wang","Wei-Chiu Ma"],"pdf_url":"https://arxiv.org/pdf/2504.15278v1.pdf","comment":"Project page: https://drawer-art.github.io/"},{"id":"http://arxiv.org/abs/2503.19902v2","updated":"2025-04-21T17:59:26Z","published":"2025-03-25T17:58:29Z","title":"ICE: Intrinsic Concept Extraction from a Single Image via Diffusion\n  Models","summary":"  The inherent ambiguity in defining visual concepts poses significant\nchallenges for modern generative models, such as the diffusion-based\nText-to-Image (T2I) models, in accurately learning concepts from a single\nimage. Existing methods lack a systematic way to reliably extract the\ninterpretable underlying intrinsic concepts. To address this challenge, we\npresent ICE, short for Intrinsic Concept Extraction, a novel framework that\nexclusively utilises a T2I model to automatically and systematically extract\nintrinsic concepts from a single image. ICE consists of two pivotal stages. In\nthe first stage, ICE devises an automatic concept localization module to\npinpoint relevant text-based concepts and their corresponding masks within the\nimage. This critical stage streamlines concept initialization and provides\nprecise guidance for subsequent analysis. The second stage delves deeper into\neach identified mask, decomposing the object-level concepts into intrinsic\nconcepts and general concepts. This decomposition allows for a more granular\nand interpretable breakdown of visual elements. Our framework demonstrates\nsuperior performance on intrinsic concept extraction from a single image in an\nunsupervised manner. Project page: https://visual-ai.github.io/ice\n","authors":["Fernando Julio Cendra","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2503.19902v2.pdf","comment":"CVPR 2025, Project page: https://visual-ai.github.io/ice"},{"id":"http://arxiv.org/abs/2504.15271v1","updated":"2025-04-21T17:57:28Z","published":"2025-04-21T17:57:28Z","title":"Eagle 2.5: Boosting Long-Context Post-Training for Frontier\n  Vision-Language Models","summary":"  We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)\nfor long-context multimodal learning. Our work addresses the challenges in long\nvideo comprehension and high-resolution image understanding, introducing a\ngeneralist framework for both tasks. The proposed training framework\nincorporates Automatic Degrade Sampling and Image Area Preservation, two\ntechniques that preserve contextual integrity and visual details. The framework\nalso includes numerous efficiency optimizations in the pipeline for\nlong-context data training. Finally, we propose Eagle-Video-110K, a novel\ndataset that integrates both story-level and clip-level annotations,\nfacilitating long-video understanding. Eagle 2.5 demonstrates substantial\nimprovements on long-context multimodal benchmarks, providing a robust solution\nto the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B\nachieves 72.4% on Video-MME with 512 input frames, matching the results of\ntop-tier commercial model such as GPT-4o and large-scale open-source models\nlike Qwen2.5-VL-72B and InternVL2.5-78B.\n","authors":["Guo Chen","Zhiqi Li","Shihao Wang","Jindong Jiang","Yicheng Liu","Lidong Lu","De-An Huang","Wonmin Byeon","Matthieu Le","Tuomas Rintamaki","Tyler Poon","Max Ehrlich","Tuomas Rintamaki","Tyler Poon","Tong Lu","Limin Wang","Bryan Catanzaro","Jan Kautz","Andrew Tao","Zhiding Yu","Guilin Liu"],"pdf_url":"https://arxiv.org/pdf/2504.15271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15270v1","updated":"2025-04-21T17:57:21Z","published":"2025-04-21T17:57:21Z","title":"An LMM for Efficient Video Understanding via Reinforced Compression of\n  Video Cubes","summary":"  Large Multimodal Models (LMMs) uniformly perceive video frames, creating\ncomputational inefficiency for videos with inherently varying temporal\ninformation density. This paper present \\textbf{Quicksviewer}, an LMM with new\nperceiving paradigm that partitions a video of nonuniform density into varying\ncubes using Gumbel Softmax, followed by a unified resampling for each cube to\nachieve efficient video understanding. This simple and intuitive approach\ndynamically compress video online based on its temporal density, significantly\nreducing spatiotemporal redundancy (overall 45$\\times$ compression rate), while\nenabling efficient training with large receptive field. We train the model from\na language backbone through three progressive stages, each incorporating\nlengthy videos on average of 420s/1fps thanks to the perceiving efficiency.\nWith only 0.8M total video-text samples for training, our model outperforms the\ndirect baseline employing a fixed partitioning strategy by a maximum of 8.72 in\naccuracy, demonstrating the effectiveness in performance. On Video-MME,\nQuicksviewer achieves SOTA under modest sequence lengths using just up to 5\\%\nof tokens per frame required by baselines. With this paradigm, scaling up the\nnumber of input frames reveals a clear power law of the model capabilities. It\nis also empirically verified that the segments generated by the cubing network\ncan help for analyzing continuous events in videos.\n","authors":["Ji Qi","Yuan Yao","Yushi Bai","Bin Xu","Juanzi Li","Zhiyuan Liu","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2504.15270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15267v1","updated":"2025-04-21T17:49:06Z","published":"2025-04-21T17:49:06Z","title":"Diffusion Bridge Models for 3D Medical Image Translation","summary":"  Diffusion tensor imaging (DTI) provides crucial insights into the\nmicrostructure of the human brain, but it can be time-consuming to acquire\ncompared to more readily available T1-weighted (T1w) magnetic resonance imaging\n(MRI). To address this challenge, we propose a diffusion bridge model for 3D\nbrain image translation between T1w MRI and DTI modalities. Our model learns to\ngenerate high-quality DTI fractional anisotropy (FA) images from T1w images and\nvice versa, enabling cross-modality data augmentation and reducing the need for\nextensive DTI acquisition. We evaluate our approach using perceptual\nsimilarity, pixel-level agreement, and distributional consistency metrics,\ndemonstrating strong performance in capturing anatomical structures and\npreserving information on white matter integrity. The practical utility of the\nsynthetic data is validated through sex classification and Alzheimer's disease\nclassification tasks, where the generated images achieve comparable performance\nto real data. Our diffusion bridge model offers a promising solution for\nimproving neuroimaging datasets and supporting clinical decision-making, with\nthe potential to significantly impact neuroimaging research and clinical\npractice.\n","authors":["Shaorong Zhang","Tamoghna Chattopadhyay","Sophia I. Thomopoulos","Jose-Luis Ambite","Paul M. Thompson","Greg Ver Steeg"],"pdf_url":"https://arxiv.org/pdf/2504.15267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15262v1","updated":"2025-04-21T17:43:21Z","published":"2025-04-21T17:43:21Z","title":"Revealing the 3D Cosmic Web through Gravitationally Constrained Neural\n  Fields","summary":"  Weak gravitational lensing is the slight distortion of galaxy shapes caused\nprimarily by the gravitational effects of dark matter in the universe. In our\nwork, we seek to invert the weak lensing signal from 2D telescope images to\nreconstruct a 3D map of the universe's dark matter field. While inversion\ntypically yields a 2D projection of the dark matter field, accurate 3D maps of\nthe dark matter distribution are essential for localizing structures of\ninterest and testing theories of our universe. However, 3D inversion poses\nsignificant challenges. First, unlike standard 3D reconstruction that relies on\nmultiple viewpoints, in this case, images are only observed from a single\nviewpoint. This challenge can be partially addressed by observing how galaxy\nemitters throughout the volume are lensed. However, this leads to the second\nchallenge: the shapes and exact locations of unlensed galaxies are unknown, and\ncan only be estimated with a very large degree of uncertainty. This introduces\nan overwhelming amount of noise which nearly drowns out the lensing signal\ncompletely. Previous approaches tackle this by imposing strong assumptions\nabout the structures in the volume. We instead propose a methodology using a\ngravitationally-constrained neural field to flexibly model the continuous\nmatter distribution. We take an analysis-by-synthesis approach, optimizing the\nweights of the neural network through a fully differentiable physical forward\nmodel to reproduce the lensing signal present in image measurements. We\nshowcase our method on simulations, including realistic simulated measurements\nof dark matter distributions that mimic data from upcoming telescope surveys.\nOur results show that our method can not only outperform previous methods, but\nimportantly is also able to recover potentially surprising dark matter\nstructures.\n","authors":["Brandon Zhao","Aviad Levis","Liam Connor","Pratul P. Srinivasan","Katherine L. Bouman"],"pdf_url":"https://arxiv.org/pdf/2504.15262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15259v1","updated":"2025-04-21T17:38:50Z","published":"2025-04-21T17:38:50Z","title":"Bringing Diversity from Diffusion Models to Semantic-Guided Face Asset\n  Generation","summary":"  Digital modeling and reconstruction of human faces serve various\napplications. However, its availability is often hindered by the requirements\nof data capturing devices, manual labor, and suitable actors. This situation\nrestricts the diversity, expressiveness, and control over the resulting models.\nThis work aims to demonstrate that a semantically controllable generative\nnetwork can provide enhanced control over the digital face modeling process. To\nenhance diversity beyond the limited human faces scanned in a controlled\nsetting, we introduce a novel data generation pipeline that creates a\nhigh-quality 3D face database using a pre-trained diffusion model. Our proposed\nnormalization module converts synthesized data from the diffusion model into\nhigh-quality scanned data. Using the 44,000 face models we obtained, we further\ndeveloped an efficient GAN-based generator. This generator accepts semantic\nattributes as input, and generates geometry and albedo. It also allows\ncontinuous post-editing of attributes in the latent space. Our asset refinement\ncomponent subsequently creates physically-based facial assets. We introduce a\ncomprehensive system designed for creating and editing high-quality face\nassets. Our proposed model has undergone extensive experiment, comparison and\nevaluation. We also integrate everything into a web-based interactive tool. We\naim to make this tool publicly available with the release of the paper.\n","authors":["Yunxuan Cai","Sitao Xiang","Zongjian Li","Haiwei Chen","Yajie Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.15259v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15252v1","updated":"2025-04-21T17:33:02Z","published":"2025-04-21T17:33:02Z","title":"SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam","summary":"  Understanding and monitoring aquatic biodiversity is critical for ecological\nhealth and conservation efforts. This paper proposes SuoiAI, an end-to-end\npipeline for building a dataset of aquatic invertebrates in Vietnam and\nemploying machine learning (ML) techniques for species classification. We\noutline the methods for data collection, annotation, and model training,\nfocusing on reducing annotation effort through semi-supervised learning and\nleveraging state-of-the-art object detection and classification models. Our\napproach aims to overcome challenges such as data scarcity, fine-grained\nclassification, and deployment in diverse environmental conditions.\n","authors":["Tue Vo","Lakshay Sharma","Tuan Dinh","Khuong Dinh","Trang Nguyen","Trung Phan","Minh Do","Duong Vu"],"pdf_url":"https://arxiv.org/pdf/2504.15252v1.pdf","comment":"Published as a workshop paper at \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025"},{"id":"http://arxiv.org/abs/2504.15232v1","updated":"2025-04-21T17:08:36Z","published":"2025-04-21T17:08:36Z","title":"Shape-Guided Clothing Warping for Virtual Try-On","summary":"  Image-based virtual try-on aims to seamlessly fit in-shop clothing to a\nperson image while maintaining pose consistency. Existing methods commonly\nemploy the thin plate spline (TPS) transformation or appearance flow to deform\nin-shop clothing for aligning with the person's body. Despite their promising\nperformance, these methods often lack precise control over fine details,\nleading to inconsistencies in shape between clothing and the person's body as\nwell as distortions in exposed limb regions. To tackle these challenges, we\npropose a novel shape-guided clothing warping method for virtual try-on, dubbed\nSCW-VTON, which incorporates global shape constraints and additional limb\ntextures to enhance the realism and consistency of the warped clothing and\ntry-on results. To integrate global shape constraints for clothing warping, we\ndevise a dual-path clothing warping module comprising a shape path and a flow\npath. The former path captures the clothing shape aligned with the person's\nbody, while the latter path leverages the mapping between the pre- and\npost-deformation of the clothing shape to guide the estimation of appearance\nflow. Furthermore, to alleviate distortions in limb regions of try-on results,\nwe integrate detailed limb guidance by developing a limb reconstruction network\nbased on masked image modeling. Through the utilization of SCW-VTON, we are\nable to generate try-on results with enhanced clothing shape consistency and\nprecise control over details. Extensive experiments demonstrate the superiority\nof our approach over state-of-the-art methods both qualitatively and\nquantitatively. The code is available at https://github.com/xyhanHIT/SCW-VTON.\n","authors":["Xiaoyu Han","Shunyuan Zheng","Zonglin Li","Chenyang Wang","Xin Sun","Quanling Meng"],"pdf_url":"https://arxiv.org/pdf/2504.15232v1.pdf","comment":"Accepted by ACM MM 2024. The code is available at\n  https://github.com/xyhanHIT/SCW-VTON"},{"id":"http://arxiv.org/abs/2504.13690v2","updated":"2025-04-21T17:07:18Z","published":"2025-04-18T13:46:32Z","title":"Analysing the Robustness of Vision-Language-Models to Common Corruptions","summary":"  Vision-language models (VLMs) have demonstrated impressive capabilities in\nunderstanding and reasoning about visual and textual content. However, their\nrobustness to common image corruptions remains under-explored. In this work, we\npresent the first comprehensive analysis of VLM robustness across 19 corruption\ntypes from the ImageNet-C benchmark, spanning four categories: noise, blur,\nweather, and digital distortions. We introduce two new benchmarks, TextVQA-C\nand GQA-C, to systematically evaluate how corruptions affect scene text\nunderstanding and object-based reasoning, respectively. Our analysis reveals\nthat transformer-based VLMs exhibit distinct vulnerability patterns across\ntasks: text recognition deteriorates most severely under blur and snow\ncorruptions, while object reasoning shows higher sensitivity to corruptions\nsuch as frost and impulse noise. We connect these observations to the\nfrequency-domain characteristics of different corruptions, revealing how\ntransformers' inherent bias toward low-frequency processing explains their\ndifferential robustness patterns. Our findings provide valuable insights for\ndeveloping more corruption-robust vision-language models for real-world\napplications.\n","authors":["Muhammad Usama","Syeda Aishah Asim","Syed Bilal Ali","Syed Talal Wasim","Umair Bin Mansoor"],"pdf_url":"https://arxiv.org/pdf/2504.13690v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.10592,\n  arXiv:2301.12597 by other authors"},{"id":"http://arxiv.org/abs/2312.14216v2","updated":"2025-04-21T16:26:57Z","published":"2023-12-21T12:11:00Z","title":"DreamDistribution: Learning Prompt Distribution for Diverse\n  In-distribution Generation","summary":"  The popularization of Text-to-Image (T2I) diffusion models enables the\ngeneration of high-quality images from text descriptions. However, generating\ndiverse customized images with reference visual attributes remains challenging.\nThis work focuses on personalizing T2I diffusion models at a more abstract\nconcept or category level, adapting commonalities from a set of reference\nimages while creating new instances with sufficient variations. We introduce a\nsolution that allows a pretrained T2I diffusion model to learn a set of soft\nprompts, enabling the generation of novel images by sampling prompts from the\nlearned distribution. These prompts offer text-guided editing capabilities and\nadditional flexibility in controlling variation and mixing between multiple\ndistributions. We also show the adaptability of the learned prompt distribution\nto other tasks, such as text-to-3D. Finally we demonstrate effectiveness of our\napproach through quantitative analysis including automatic evaluation and human\nassessment. Project website: https://briannlongzhao.github.io/DreamDistribution\n","authors":["Brian Nlong Zhao","Yuhang Xiao","Jiashu Xu","Xinyang Jiang","Yifan Yang","Dongsheng Li","Laurent Itti","Vibhav Vineet","Yunhao Ge"],"pdf_url":"https://arxiv.org/pdf/2312.14216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15199v1","updated":"2025-04-21T16:16:19Z","published":"2025-04-21T16:16:19Z","title":"Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's\n  LLM-CLIP Framework for Image Captioning","summary":"  MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models.\n","authors":["Yassir Benhammou","Alessandro Tiberio","Gabriel Trautmann","Suman Kalyan"],"pdf_url":"https://arxiv.org/pdf/2504.15199v1.pdf","comment":"9 pages, 2 tables, 1 figure"},{"id":"http://arxiv.org/abs/2504.15193v1","updated":"2025-04-21T16:02:26Z","published":"2025-04-21T16:02:26Z","title":"Automated Measurement of Eczema Severity with Self-Supervised Learning","summary":"  Automated diagnosis of eczema using images acquired from digital camera can\nenable individuals to self-monitor their recovery. The process entails first\nsegmenting out the eczema region from the image and then measuring the severity\nof eczema in the segmented region. The state-of-the-art methods for automated\neczema diagnosis rely on deep neural networks such as convolutional neural\nnetwork (CNN) and have shown impressive performance in accurately measuring the\nseverity of eczema. However, these methods require massive volume of annotated\ndata to train which can be hard to obtain. In this paper, we propose a\nself-supervised learning framework for automated eczema diagnosis under limited\ntraining data regime. Our framework consists of two stages: i) Segmentation,\nwhere we use an in-context learning based algorithm called SegGPT for few-shot\nsegmentation of eczema region from the image; ii) Feature extraction and\nclassification, where we extract DINO features from the segmented regions and\nfeed it to a multi-layered perceptron (MLP) for 4-class classification of\neczema severity. When evaluated on a dataset of annotated \"in-the-wild\" eczema\nimages, we show that our method outperforms (Weighted F1: 0.67 $\\pm$ 0.01) the\nstate-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted\nF1: 0.44 $\\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\\pm$ 0.22). Our\nresults show that self-supervised learning can be a viable solution for\nautomated skin diagnosis where labeled data is scarce.\n","authors":["Neelesh Kumar","Oya Aran"],"pdf_url":"https://arxiv.org/pdf/2504.15193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15192v1","updated":"2025-04-21T16:01:51Z","published":"2025-04-21T16:01:51Z","title":"Breast density in MRI: an AI-based quantification and relationship to\n  assessment in mammography","summary":"  Mammographic breast density is a well-established risk factor for breast\ncancer. Recently there has been interest in breast MRI as an adjunct to\nmammography, as this modality provides an orthogonal and highly quantitative\nassessment of breast tissue. However, its 3D nature poses analytic challenges\nrelated to delineating and aggregating complex structures across slices. Here,\nwe applied an in-house machine-learning algorithm to assess breast density on\nnormal breasts in three MRI datasets. Breast density was consistent across\ndifferent datasets (0.104 - 0.114). Analysis across different age groups also\ndemonstrated strong consistency across datasets and confirmed a trend of\ndecreasing density with age as reported in previous studies. MR breast density\nwas correlated with mammographic breast density, although some notable\ndifferences suggest that certain breast density components are captured only on\nMRI. Future work will determine how to integrate MR breast density with current\ntools to improve future breast cancer risk prediction.\n","authors":["Yaqian Chen","Lin Li","Hanxue Gu","Haoyu Dong","Derek L. Nguyen","Allan D. Kirk","Maciej A. Mazurowski","E. Shelley Hwang"],"pdf_url":"https://arxiv.org/pdf/2504.15192v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.15182v1","updated":"2025-04-21T15:44:06Z","published":"2025-04-21T15:44:06Z","title":"Tiger200K: Manually Curated High Visual Quality Video Dataset from UGC\n  Platform","summary":"  The recent surge in open-source text-to-video generation models has\nsignificantly energized the research community, yet their dependence on\nproprietary training datasets remains a key constraint. While existing open\ndatasets like Koala-36M employ algorithmic filtering of web-scraped videos from\nearly platforms, they still lack the quality required for fine-tuning advanced\nvideo generation models. We present Tiger200K, a manually curated high visual\nquality video dataset sourced from User-Generated Content (UGC) platforms. By\nprioritizing visual fidelity and aesthetic quality, Tiger200K underscores the\ncritical role of human expertise in data curation, and providing high-quality,\ntemporally consistent video-text pairs for fine-tuning and optimizing video\ngeneration architectures through a simple but effective pipeline including shot\nboundary detection, OCR, border detecting, motion filter and fine bilingual\ncaption. The dataset will undergo ongoing expansion and be released as an\nopen-source initiative to advance research and applications in video generative\nmodels. Project page: https://tinytigerpan.github.io/tiger200k/\n","authors":["Xianpan Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.15182v1.pdf","comment":"Project page: https://tinytigerpan.github.io/tiger200k/"},{"id":"http://arxiv.org/abs/2504.15179v1","updated":"2025-04-21T15:40:14Z","published":"2025-04-21T15:40:14Z","title":"FaceCraft4D: Animated 3D Facial Avatar Generation from a Single Image","summary":"  We present a novel framework for generating high-quality, animatable 4D\navatar from a single image. While recent advances have shown promising results\nin 4D avatar creation, existing methods either require extensive multiview data\nor struggle with shape accuracy and identity consistency. To address these\nlimitations, we propose a comprehensive system that leverages shape, image, and\nvideo priors to create full-view, animatable avatars. Our approach first\nobtains initial coarse shape through 3D-GAN inversion. Then, it enhances\nmultiview textures using depth-guided warping signals for cross-view\nconsistency with the help of the image diffusion model. To handle expression\nanimation, we incorporate a video prior with synchronized driving signals\nacross viewpoints. We further introduce a Consistent-Inconsistent training to\neffectively handle data inconsistencies during 4D reconstruction. Experimental\nresults demonstrate that our method achieves superior quality compared to the\nprior art, while maintaining consistency across different viewpoints and\nexpressions.\n","authors":["Fei Yin","Mallikarjun B R","Chun-Han Yao","Rafał Mantiuk","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2504.15179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11201v2","updated":"2025-04-21T15:37:50Z","published":"2024-10-15T02:37:39Z","title":"Tree of Attributes Prompt Learning for Vision-Language Models","summary":"  Prompt learning has proven effective in adapting vision language models for\ndownstream tasks. However, existing methods usually append learnable prompt\ntokens solely with the category names to obtain textual features, which fails\nto fully leverage the rich context indicated in the category name. To address\nthis issue, we propose the Tree of Attributes Prompt learning (TAP), which\nfirst instructs LLMs to generate a tree of attributes with a \"concept -\nattribute - description\" structure for each category, and then learn the\nhierarchy with vision and text prompt tokens. Unlike existing methods that\nmerely augment category names with a set of unstructured descriptions, our\napproach essentially distills structured knowledge graphs associated with class\nnames from LLMs. Furthermore, our approach introduces text and vision prompts\ndesigned to explicitly learn the corresponding visual attributes, effectively\nserving as domain experts. Additionally, the general and diverse descriptions\ngenerated based on the class names may be wrong or absent in the specific given\nimages. To address this misalignment, we further introduce a vision-conditional\npooling module to extract instance-specific text features. Extensive\nexperimental results demonstrate that our approach outperforms state-of-the-art\nmethods on the zero-shot base-to-novel generalization, cross-dataset transfer,\nas well as few-shot classification across 11 diverse datasets. Code is\navailable at https://github.com/HHenryD/TAP.\n","authors":["Tong Ding","Wanhua Li","Zhongqi Miao","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.11201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15176v1","updated":"2025-04-21T15:35:48Z","published":"2025-04-21T15:35:48Z","title":"DSPO: Direct Semantic Preference Optimization for Real-World Image\n  Super-Resolution","summary":"  Recent advances in diffusion models have improved Real-World Image\nSuper-Resolution (Real-ISR), but existing methods lack human feedback\nintegration, risking misalignment with human preference and may leading to\nartifacts, hallucinations and harmful content generation. To this end, we are\nthe first to introduce human preference alignment into Real-ISR, a technique\nthat has been successfully applied in Large Language Models and Text-to-Image\ntasks to effectively enhance the alignment of generated outputs with human\npreferences. Specifically, we introduce Direct Preference Optimization (DPO)\ninto Real-ISR to achieve alignment, where DPO serves as a general alignment\ntechnique that directly learns from the human preference dataset. Nevertheless,\nunlike high-level tasks, the pixel-level reconstruction objectives of Real-ISR\nare difficult to reconcile with the image-level preferences of DPO, which can\nlead to the DPO being overly sensitive to local anomalies, leading to reduced\ngeneration quality. To resolve this dichotomy, we propose Direct Semantic\nPreference Optimization (DSPO) to align instance-level human preferences by\nincorporating semantic guidance, which is through two strategies: (a) semantic\ninstance alignment strategy, implementing instance-level alignment to ensure\nfine-grained perceptual consistency, and (b) user description feedback\nstrategy, mitigating hallucinations through semantic textual feedback on\ninstance-level images. As a plug-and-play solution, DSPO proves highly\neffective in both one-step and multi-step SR frameworks.\n","authors":["Miaomiao Cai","Simiao Li","Wei Li","Xudong Huang","Hanting Chen","Jie Hu","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2504.15176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15170v1","updated":"2025-04-21T15:23:59Z","published":"2025-04-21T15:23:59Z","title":"HSANET: A Hybrid Self-Cross Attention Network For Remote Sensing Change\n  Detection","summary":"  The remote sensing image change detection task is an essential method for\nlarge-scale monitoring. We propose HSANet, a network that uses hierarchical\nconvolution to extract multi-scale features. It incorporates hybrid\nself-attention and cross-attention mechanisms to learn and fuse global and\ncross-scale information. This enables HSANet to capture global context at\ndifferent scales and integrate cross-scale features, refining edge details and\nimproving detection performance. We will also open-source our model code:\nhttps://github.com/ChengxiHAN/HSANet.\n","authors":["Chengxi Han","Xiaoyu Su","Zhiqiang Wei","Meiqi Hu","Yichu Xu"],"pdf_url":"https://arxiv.org/pdf/2504.15170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15165v1","updated":"2025-04-21T15:16:13Z","published":"2025-04-21T15:16:13Z","title":"An Efficient Aerial Image Detection with Variable Receptive Fields","summary":"  Aerial object detection using unmanned aerial vehicles (UAVs) faces critical\nchallenges including sub-10px targets, dense occlusions, and stringent\ncomputational constraints. Existing detectors struggle to balance accuracy and\nefficiency due to rigid receptive fields and redundant architectures. To\naddress these limitations, we propose Variable Receptive Field DETR (VRF-DETR),\na transformer-based detector incorporating three key components: 1) Multi-Scale\nContext Fusion (MSCF) module that dynamically recalibrates features through\nadaptive spatial attention and gated multi-scale fusion, 2) Gated Convolution\n(GConv) layer enabling parameter-efficient local-context modeling via depthwise\nseparable operations and dynamic gating, and 3) Gated Multi-scale Fusion (GMCF)\nBottleneck that hierarchically disentangles occluded objects through cascaded\nglobal-local interactions. Experiments on VisDrone2019 demonstrate VRF-DETR\nachieves 51.4\\% mAP\\textsubscript{50} and 31.8\\% mAP\\textsubscript{50:95} with\nonly 13.5M parameters. This work establishes a new efficiency-accuracy Pareto\nfrontier for UAV-based detection tasks.\n","authors":["Liu Wenbin"],"pdf_url":"https://arxiv.org/pdf/2504.15165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15159v1","updated":"2025-04-21T15:05:22Z","published":"2025-04-21T15:05:22Z","title":"Acquire and then Adapt: Squeezing out Text-to-Image Model for Image\n  Restoration","summary":"  Recently, pre-trained text-to-image (T2I) models have been extensively\nadopted for real-world image restoration because of their powerful generative\nprior. However, controlling these large models for image restoration usually\nrequires a large number of high-quality images and immense computational\nresources for training, which is costly and not privacy-friendly. In this\npaper, we find that the well-trained large T2I model (i.e., Flux) is able to\nproduce a variety of high-quality images aligned with real-world distributions,\noffering an unlimited supply of training samples to mitigate the above issue.\nSpecifically, we proposed a training data construction pipeline for image\nrestoration, namely FluxGen, which includes unconditional image generation,\nimage selection, and degraded image simulation. A novel light-weighted adapter\n(FluxIR) with squeeze-and-excitation layers is also carefully designed to\ncontrol the large Diffusion Transformer (DiT)-based T2I model so that\nreasonable details can be restored. Experiments demonstrate that our proposed\nmethod enables the Flux model to adapt effectively to real-world image\nrestoration tasks, achieving superior scores and visual quality on both\nsynthetic and real-world degradation datasets - at only about 8.5\\% of the\ntraining cost compared to current approaches.\n","authors":["Junyuan Deng","Xinyi Wu","Yongxing Yang","Congchao Zhu","Song Wang","Zhenyao Wu"],"pdf_url":"https://arxiv.org/pdf/2504.15159v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2504.15155v1","updated":"2025-04-21T14:57:48Z","published":"2025-04-21T14:57:48Z","title":"Dynamic 3D KAN Convolution with Adaptive Grid Optimization for\n  Hyperspectral Image Classification","summary":"  Deep neural networks face several challenges in hyperspectral image\nclassification, including high-dimensional data, sparse distribution of ground\nobjects, and spectral redundancy, which often lead to classification\noverfitting and limited generalization capability. To more efficiently adapt to\nground object distributions while extracting image features without introducing\nexcessive parameters and skipping redundant information, this paper proposes\nKANet based on an improved 3D-DenseNet model, consisting of 3D KAN Conv and an\nadaptive grid update mechanism. By introducing learnable univariate B-spline\nfunctions on network edges, specifically by flattening three-dimensional\nneighborhoods into vectors and applying B-spline-parameterized nonlinear\nactivation functions to replace the fixed linear weights of traditional 3D\nconvolutional kernels, we precisely capture complex spectral-spatial nonlinear\nrelationships in hyperspectral data. Simultaneously, through a dynamic grid\nadjustment mechanism, we adaptively update the grid point positions of\nB-splines based on the statistical characteristics of input data, optimizing\nthe resolution of spline functions to match the non-uniform distribution of\nspectral features, significantly improving the model's accuracy in\nhigh-dimensional data modeling and parameter efficiency, effectively\nalleviating the curse of dimensionality. This characteristic demonstrates\nsuperior neural scaling laws compared to traditional convolutional neural\nnetworks and reduces overfitting risks in small-sample and high-noise\nscenarios. KANet enhances model representation capability through a 3D dynamic\nexpert convolution system without increasing network depth or width. The\nproposed method demonstrates superior performance on IN, UP, and KSC datasets,\noutperforming mainstream hyperspectral image classification approaches.\n","authors":["Guandong Li","Mengxia Ye"],"pdf_url":"https://arxiv.org/pdf/2504.15155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15152v1","updated":"2025-04-21T14:55:57Z","published":"2025-04-21T14:55:57Z","title":"Landmark-Free Preoperative-to-Intraoperative Registration in\n  Laparoscopic Liver Resection","summary":"  Liver registration by overlaying preoperative 3D models onto intraoperative\n2D frames can assist surgeons in perceiving the spatial anatomy of the liver\nclearly for a higher surgical success rate. Existing registration methods rely\nheavily on anatomical landmark-based workflows, which encounter two major\nlimitations: 1) ambiguous landmark definitions fail to provide efficient\nmarkers for registration; 2) insufficient integration of intraoperative liver\nvisual information in shape deformation modeling. To address these challenges,\nin this paper, we propose a landmark-free preoperative-to-intraoperative\nregistration framework utilizing effective self-supervised learning, termed\n\\ourmodel. This framework transforms the conventional 3D-2D workflow into a\n3D-3D registration pipeline, which is then decoupled into rigid and non-rigid\nregistration subtasks. \\ourmodel~first introduces a feature-disentangled\ntransformer to learn robust correspondences for recovering rigid\ntransformations. Further, a structure-regularized deformation network is\ndesigned to adjust the preoperative model to align with the intraoperative\nliver surface. This network captures structural correlations through geometry\nsimilarity modeling in a low-rank transformer network. To facilitate the\nvalidation of the registration performance, we also construct an in-vivo\nregistration dataset containing liver resection videos of 21 patients, called\n\\emph{P2I-LReg}, which contains 346 keyframes that provide a global view of the\nliver together with liver mask annotations and calibrated camera intrinsic\nparameters. Extensive experiments and user studies on both synthetic and\nin-vivo datasets demonstrate the superiority and potential clinical\napplicability of our method.\n","authors":["Jun Zhou","Bingchen Gao","Kai Wang","Jialun Pei","Pheng-Ann Heng","Jing Qin"],"pdf_url":"https://arxiv.org/pdf/2504.15152v1.pdf","comment":"TMI under review"},{"id":"http://arxiv.org/abs/2504.15145v1","updated":"2025-04-21T14:49:15Z","published":"2025-04-21T14:49:15Z","title":"\"I Know It When I See It\": Mood Spaces for Connecting and Expressing\n  Visual Concepts","summary":"  Expressing complex concepts is easy when they can be labeled or quantified,\nbut many ideas are hard to define yet instantly recognizable. We propose a Mood\nBoard, where users convey abstract concepts with examples that hint at the\nintended direction of attribute changes. We compute an underlying Mood Space\nthat 1) factors out irrelevant features and 2) finds the connections between\nimages, thus bringing relevant concepts closer. We invent a fibration\ncomputation to compress/decompress pre-trained features into/from a compact\nspace, 50-100x smaller. The main innovation is learning to mimic the pairwise\naffinity relationship of the image tokens across exemplars. To focus on the\ncoarse-to-fine hierarchical structures in the Mood Space, we compute the top\neigenvector structure from the affinity matrix and define a loss in the\neigenvector space. The resulting Mood Space is locally linear and compact,\nallowing image-level operations, such as object averaging, visual analogy, and\npose transfer, to be performed as a simple vector operation in Mood Space. Our\nlearning is efficient in computation without any fine-tuning, needs only a few\n(2-20) exemplars, and takes less than a minute to learn.\n","authors":["Huzheng Yang","Katherine Xu","Michael D. Grossberg","Yutong Bai","Jianbo Shi"],"pdf_url":"https://arxiv.org/pdf/2504.15145v1.pdf","comment":"Project page: https://huzeyann.github.io/mspace/"},{"id":"http://arxiv.org/abs/2410.10733v7","updated":"2025-04-21T14:48:23Z","published":"2024-10-14T17:15:07Z","title":"Deep Compression Autoencoder for Efficient High-Resolution Diffusion\n  Models","summary":"  We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder\nmodels for accelerating high-resolution diffusion models. Existing autoencoder\nmodels have demonstrated impressive results at a moderate spatial compression\nratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for\nhigh spatial compression ratios (e.g., 64x). We address this challenge by\nintroducing two key techniques: (1) Residual Autoencoding, where we design our\nmodels to learn residuals based on the space-to-channel transformed features to\nalleviate the optimization difficulty of high spatial-compression autoencoders;\n(2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases\ntraining strategy for mitigating the generalization penalty of high\nspatial-compression autoencoders. With these designs, we improve the\nautoencoder's spatial compression ratio up to 128 while maintaining the\nreconstruction quality. Applying our DC-AE to latent diffusion models, we\nachieve significant speedup without accuracy drop. For example, on ImageNet\n512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup\non H100 GPU for UViT-H while achieving a better FID, compared with the widely\nused SD-VAE-f8 autoencoder. Our code is available at\nhttps://github.com/mit-han-lab/efficientvit.\n","authors":["Junyu Chen","Han Cai","Junsong Chen","Enze Xie","Shang Yang","Haotian Tang","Muyang Li","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2410.10733v7.pdf","comment":"ICLR 2025. The first two authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2504.15134v1","updated":"2025-04-21T14:37:37Z","published":"2025-04-21T14:37:37Z","title":"Instance-Adaptive Keypoint Learning with Local-to-Global Geometric\n  Aggregation for Category-Level Object Pose Estimation","summary":"  Category-level object pose estimation aims to predict the 6D pose and size of\npreviously unseen instances from predefined categories, requiring strong\ngeneralization across diverse object instances. Although many previous methods\nattempt to mitigate intra-class variations, they often struggle with instances\nexhibiting complex geometries or significant deviations from canonical shapes.\nTo address this challenge, we propose INKL-Pose, a novel category-level object\npose estimation framework that enables INstance-adaptive Keypoint Learning with\nlocal-to-global geometric aggregation. Specifically, our approach first\npredicts semantically consistent and geometric informative keypoints through an\nInstance-Adaptive Keypoint Generator, then refines them with: (1) a Local\nKeypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global\nKeypoint Feature Aggregator using bidirectional Mamba for structural\nconsistency. To enable bidirectional modeling in Mamba, we introduce a Feature\nSequence Flipping strategy that preserves spatial coherence while constructing\nbackward feature sequences. Additionally, we design a surface loss and a\nseparation loss to enforce uniform coverage and spatial diversity in keypoint\ndistribution. The generated keypoints are finally mapped to a canonical space\nfor regressing the object's 6D pose and size. Extensive experiments on\nCAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves\nstate-of-the-art performance and significantly outperforms existing methods.\n","authors":["Xiao Zhang","Lu Zou","Tao Lu","Yuan Yao","Zhangjin Huang","Guoping Wang"],"pdf_url":"https://arxiv.org/pdf/2504.15134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15133v1","updated":"2025-04-21T14:33:55Z","published":"2025-04-21T14:33:55Z","title":"EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models","summary":"  In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.\n","authors":["Ziwen Xu","Shuxun Wang","Kewei Xu","Haoming Xu","Mengru Wang","Xinle Deng","Yunzhi Yao","Guozhou Zheng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.15133v1.pdf","comment":"Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2503.10596v2","updated":"2025-04-21T14:25:51Z","published":"2025-03-13T17:43:10Z","title":"GroundingSuite: Measuring Complex Multi-Granular Pixel Grounding","summary":"  Pixel grounding, encompassing tasks such as Referring Expression Segmentation\n(RES), has garnered considerable attention due to its immense potential for\nbridging the gap between vision and language modalities. However, advancements\nin this domain are currently constrained by limitations inherent in existing\ndatasets, including limited object categories, insufficient textual diversity,\nand a scarcity of high-quality annotations. To mitigate these limitations, we\nintroduce GroundingSuite, which comprises: (1) an automated data annotation\nframework leveraging multiple Vision-Language Model (VLM) agents; (2) a\nlarge-scale training dataset encompassing 9.56 million diverse referring\nexpressions and their corresponding segmentations; and (3) a meticulously\ncurated evaluation benchmark consisting of 3,800 images. The GroundingSuite\ntraining dataset facilitates substantial performance improvements, enabling\nmodels trained on it to achieve state-of-the-art results. Specifically, a cIoU\nof 68.9 on gRefCOCO and a gIoU of 55.3 on RefCOCOm. Moreover, the\nGroundingSuite annotation framework demonstrates superior efficiency compared\nto the current leading data annotation method, i.e., $4.5 \\times$ faster than\nthe GLaMM.\n","authors":["Rui Hu","Lianghui Zhu","Yuxuan Zhang","Tianheng Cheng","Lei Liu","Heng Liu","Longjin Ran","Xiaoxin Chen","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.10596v2.pdf","comment":"Work in progress. Code: https://github.com/hustvl/GroundingSuite.\n  Update: add more results & polish the report"},{"id":"http://arxiv.org/abs/2504.15129v1","updated":"2025-04-21T14:25:23Z","published":"2025-04-21T14:25:23Z","title":"A General Infrastructure and Workflow for Quadrotor Deep Reinforcement\n  Learning and Reality Deployment","summary":"  Deploying robot learning methods to a quadrotor in unstructured outdoor\nenvironments is an exciting task. Quadrotors operating in real-world\nenvironments by learning-based methods encounter several challenges: a large\namount of simulator generated data required for training, strict demands for\nreal-time processing onboard, and the sim-to-real gap caused by dynamic and\nnoisy conditions. Current works have made a great breakthrough in applying\nlearning-based methods to end-to-end control of quadrotors, but rarely mention\nthe infrastructure system training from scratch and deploying to reality, which\nmakes it difficult to reproduce methods and applications. To bridge this gap,\nwe propose a platform that enables the seamless transfer of end-to-end deep\nreinforcement learning (DRL) policies. We integrate the training environment,\nflight dynamics control, DRL algorithms, the MAVROS middleware stack, and\nhardware into a comprehensive workflow and architecture that enables\nquadrotors' policies to be trained from scratch to real-world deployment in\nseveral minutes. Our platform provides rich types of environments including\nhovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and\nplanning in unknown environments, as a physical experiment benchmark. Through\nextensive empirical validation, we demonstrate the efficiency of proposed\nsim-to-real platform, and robust outdoor flight performance under real-world\nperturbations. Details can be found from our website\nhttps://emnavi.tech/AirGym/.\n","authors":["Kangyao Huang","Hao Wang","Yu Luo","Jingyu Chen","Jintao Chen","Xiangkui Zhang","Xiangyang Ji","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2504.15129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15122v1","updated":"2025-04-21T14:19:19Z","published":"2025-04-21T14:19:19Z","title":"MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry\n  Monocular Video","summary":"  We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS)\nframework capable of reconstructing sharp and high-quality novel\nspatio-temporal views from blurry monocular videos in an end-to-end manner.\nExisting dynamic novel view synthesis (NVS) methods are highly sensitive to\nmotion blur in casually captured videos, resulting in significant degradation\nof rendering quality. While recent approaches address motion-blurred inputs for\nNVS, they primarily focus on static scene reconstruction and lack dedicated\nmotion modeling for dynamic objects. To overcome these limitations, our MoBGS\nintroduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for\neffective latent camera trajectory estimation, improving global camera motion\ndeblurring. In addition, we propose a physically-inspired Latent Camera-induced\nExposure Estimation (LCEE) method to ensure consistent deblurring of both\nglobal camera and local object motion. Our MoBGS framework ensures the temporal\nconsistency of unseen latent timestamps and robust motion decomposition of\nstatic and dynamic regions. Extensive experiments on the Stereo Blur dataset\nand real-world blurry videos show that our MoBGS significantly outperforms the\nvery recent advanced methods (DyBluRF and Deblur4DGS), achieving\nstate-of-the-art performance for dynamic NVS under motion blur.\n","authors":["Minh-Quan Viet Bui","Jongmin Park","Juan Luis Gonzalez Bello","Jaeho Moon","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2504.15122v1.pdf","comment":"The first two authors contributed equally to this work (equal\n  contribution). The last two authors advised equally to this work"},{"id":"http://arxiv.org/abs/2504.15121v1","updated":"2025-04-21T14:19:00Z","published":"2025-04-21T14:19:00Z","title":"Robust and Real-time Surface Normal Estimation from Stereo Disparities\n  using Affine Transformations","summary":"  This work introduces a novel method for surface normal estimation from\nrectified stereo image pairs, leveraging affine transformations derived from\ndisparity values to achieve fast and accurate results. We demonstrate how the\nrectification of stereo image pairs simplifies the process of surface normal\nestimation by reducing computational complexity. To address noise reduction, we\ndevelop a custom algorithm inspired by convolutional operations, tailored to\nprocess disparity data efficiently. We also introduce adaptive heuristic\ntechniques for efficiently detecting connected surface components within the\nimages, further improving the robustness of the method. By integrating these\nmethods, we construct a surface normal estimator that is both fast and\naccurate, producing a dense, oriented point cloud as the final output. Our\nmethod is validated using both simulated environments and real-world stereo\nimages from the Middlebury and Cityscapes datasets, demonstrating significant\nimprovements in real-time performance and accuracy when implemented on a GPU.\nUpon acceptance, the shader source code will be made publicly available to\nfacilitate further research and reproducibility.\n","authors":["Csongor Csanad Kariko","Muhammad Rafi Faisal","Levente Hajder"],"pdf_url":"https://arxiv.org/pdf/2504.15121v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15118v1","updated":"2025-04-21T14:16:46Z","published":"2025-04-21T14:16:46Z","title":"Improving Sound Source Localization with Joint Slot Attention on Image\n  and Audio","summary":"  Sound source localization (SSL) is the task of locating the source of sound\nwithin an image. Due to the lack of localization labels, the de facto standard\nin SSL has been to represent an image and audio as a single embedding vector\neach, and use them to learn SSL via contrastive learning. To this end, previous\nwork samples one of local image features as the image embedding and aggregates\nall local audio features to obtain the audio embedding, which is far from\noptimal due to the presence of noise and background irrelevant to the actual\ntarget in the input. We present a novel SSL method that addresses this chronic\nissue by joint slot attention on image and audio. To be specific, two slots\ncompetitively attend image and audio features to decompose them into target and\noff-target representations, and only target representations of image and audio\nare used for contrastive learning. Also, we introduce cross-modal attention\nmatching to further align local features of image and audio. Our method\nachieved the best in almost all settings on three public benchmarks for SSL,\nand substantially outperformed all the prior work in cross-modal retrieval.\n","authors":["Inho Kim","Youngkil Song","Jicheol Park","Won Hwa Kim","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2504.15118v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2504.15108v1","updated":"2025-04-21T13:59:44Z","published":"2025-04-21T13:59:44Z","title":"Unwarping Screen Content Images via Structure-texture Enhancement\n  Network and Transformation Self-estimation","summary":"  While existing implicit neural network-based image unwarping methods perform\nwell on natural images, they struggle to handle screen content images (SCIs),\nwhich often contain large geometric distortions, text, symbols, and sharp\nedges. To address this, we propose a structure-texture enhancement network\n(STEN) with transformation self-estimation for SCI warping. STEN integrates a\nB-spline implicit neural representation module and a transformation error\nestimation and self-correction algorithm. It comprises two branches: the\nstructure estimation branch (SEB), which enhances local aggregation and global\ndependency modeling, and the texture estimation branch (TEB), which improves\ntexture detail synthesis using B-spline implicit neural representation.\nAdditionally, the transformation self-estimation module autonomously estimates\nthe transformation error and corrects the coordinate transformation matrix,\neffectively handling real-world image distortions. Extensive experiments on\npublic SCI datasets demonstrate that our approach significantly outperforms\nstate-of-the-art methods. Comparisons on well-known natural image datasets also\nshow the potential of our approach for natural image distortion.\n","authors":["Zhenzhen Xiao","Heng Liu","Bingwen Hu"],"pdf_url":"https://arxiv.org/pdf/2504.15108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15105v1","updated":"2025-04-21T13:54:33Z","published":"2025-04-21T13:54:33Z","title":"A triple-branch network for latent fingerprint enhancement guided by\n  orientation fields and minutiae","summary":"  Latent fingerprint enhancement is a critical step in the process of latent\nfingerprint identification. Existing deep learning-based enhancement methods\nstill fall short of practical application requirements, particularly in\nrestoring low-quality fingerprint regions. Recognizing that different regions\nof latent fingerprints require distinct enhancement strategies, we propose a\nTriple Branch Spatial Fusion Network (TBSFNet), which simultaneously enhances\ndifferent regions of the image using tailored strategies. Furthermore, to\nimprove the generalization capability of the network, we integrate orientation\nfield and minutiae-related modules into TBSFNet and introduce a Multi-Level\nFeature Guidance Network (MLFGNet). Experimental results on the MOLF and MUST\ndatasets demonstrate that MLFGNet outperforms existing enhancement algorithms.\n","authors":["Yurun Wang","Zerong Qi","Shujun Fu","Mingzheng Hu"],"pdf_url":"https://arxiv.org/pdf/2504.15105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.23765v3","updated":"2025-04-21T13:43:53Z","published":"2025-03-31T06:30:35Z","title":"STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World\n  Understanding?","summary":"  The use of Multimodal Large Language Models (MLLMs) as an end-to-end solution\nfor Embodied AI and Autonomous Driving has become a prevailing trend. While\nMLLMs have been extensively studied for visual semantic understanding tasks,\ntheir ability to perform precise and quantitative spatial-temporal\nunderstanding in real-world applications remains largely unexamined, leading to\nuncertain prospects. To evaluate models' Spatial-Temporal Intelligence, we\nintroduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporal\nunderstanding through challenging tasks such as estimating and predicting the\nappearance, pose, displacement, and motion of objects. Our benchmark\nencompasses a wide range of robot and vehicle operations across desktop,\nindoor, and outdoor scenarios. The extensive experiments reveals that the\nstate-of-the-art MLLMs still struggle in real-world spatial-temporal\nunderstanding, especially in tasks requiring precise distance estimation and\nmotion analysis.\n","authors":["Yun Li","Yiming Zhang","Tao Lin","XiangRui Liu","Wenxiao Cai","Zheng Liu","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.23765v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15095v1","updated":"2025-04-21T13:30:51Z","published":"2025-04-21T13:30:51Z","title":"VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced\n  Long-Range Depth Estimation","summary":"  Monocular depth estimation (MDE) aims to predict per-pixel depth values from\na single RGB image. Recent advancements have positioned diffusion models as\neffective MDE tools by framing the challenge as a conditional image generation\ntask. Despite their progress, these methods often struggle with accurately\nreconstructing distant depths, due largely to the imbalanced distribution of\ndepth values and an over-reliance on spatial-domain features. To overcome these\nlimitations, we introduce VistaDepth, a novel framework that integrates\nadaptive frequency-domain feature enhancements with an adaptive\nweight-balancing mechanism into the diffusion process. Central to our approach\nis the Latent Frequency Modulation (LFM) module, which dynamically refines\nspectral responses in the latent feature space, thereby improving the\npreservation of structural details and reducing noisy artifacts. Furthermore,\nwe implement an adaptive weighting strategy that modulates the diffusion loss\nin real-time, enhancing the model's sensitivity towards distant depth\nreconstruction. These innovations collectively result in superior depth\nperception performance across both distance and detail. Experimental\nevaluations confirm that VistaDepth achieves state-of-the-art performance among\ndiffusion-based MDE techniques, particularly excelling in the accurate\nreconstruction of distant regions.\n","authors":["Mingxia Zhan","Li Zhang","XiaoMeng Chu","Beibei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.15095v1.pdf","comment":"8 pages, 6 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.15085v1","updated":"2025-04-21T13:18:54Z","published":"2025-04-21T13:18:54Z","title":"Hierarchical Attention Fusion of Visual and Textual Representations for\n  Cross-Domain Sequential Recommendation","summary":"  Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by\nleveraging historical interactions across multiple domains, focusing on\nmodeling cross-domain preferences through intra- and inter-sequence item\nrelationships. Inspired by human cognitive processes, we propose Hierarchical\nAttention Fusion of Visual and Textual Representations (HAF-VT), a novel\napproach integrating visual and textual data to enhance cognitive modeling.\nUsing the frozen CLIP model, we generate image and text embeddings, enriching\nitem representations with multimodal data. A hierarchical attention mechanism\njointly learns single-domain and cross-domain preferences, mimicking human\ninformation integration. Evaluated on four e-commerce datasets, HAF-VT\noutperforms existing methods in capturing cross-domain user interests, bridging\ncognitive principles with computational models and highlighting the role of\nmultimodal data in sequential decision-making.\n","authors":["Wangyu Wu","Zhenhong Chen","Siqi Song","Xianglin Qiua","Xiaowei Huang","Fei Ma","Jimin Xiao"],"pdf_url":"https://arxiv.org/pdf/2504.15085v1.pdf","comment":"Accepted at CogSCI 2025"},{"id":"http://arxiv.org/abs/2405.06945v3","updated":"2025-04-21T12:33:10Z","published":"2024-05-11T07:56:19Z","title":"Direct Learning of Mesh and Appearance via 3D Gaussian Splatting","summary":"  Accurately reconstructing a 3D scene including explicit geometry information\nis both attractive and challenging. Geometry reconstruction can benefit from\nincorporating differentiable appearance models, such as Neural Radiance Fields\nand 3D Gaussian Splatting (3DGS). However, existing methods encounter\nefficiency issues due to indirect geometry learning and the paradigm of\nseparately modeling geometry and surface appearance. In this work, we propose a\nlearnable scene model that incorporates 3DGS with an explicit geometry\nrepresentation, namely a mesh. Our model learns the mesh and appearance in an\nend-to-end manner, where we bind 3D Gaussians to the mesh faces and perform\ndifferentiable rendering of 3DGS to obtain photometric supervision. The model\ncreates an effective information pathway to supervise the learning of both 3DGS\nand mesh. Experimental results demonstrate that the learned scene model not\nonly improves efficiency and rendering quality but also enables manipulation\nvia the explicit mesh. In addition, our model has a unique advantage in\nadapting to scene updates, thanks to the end-to-end learning of both mesh and\nappearance.\n","authors":["Ancheng Lin","Yusheng Xiang","Paul Kennedy","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2405.06945v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15054v1","updated":"2025-04-21T12:30:01Z","published":"2025-04-21T12:30:01Z","title":"Structure-guided Diffusion Transformer for Low-Light Image Enhancement","summary":"  While the diffusion transformer (DiT) has become a focal point of interest in\nrecent years, its application in low-light image enhancement remains a blank\narea for exploration. Current methods recover the details from low-light images\nwhile inevitably amplifying the noise in images, resulting in poor visual\nquality. In this paper, we firstly introduce DiT into the low-light enhancement\ntask and design a novel Structure-guided Diffusion Transformer based Low-light\nimage enhancement (SDTL) framework. We compress the feature through wavelet\ntransform to improve the inference efficiency of the model and capture the\nmulti-directional frequency band. Then we propose a Structure Enhancement\nModule (SEM) that uses structural prior to enhance the texture and leverages an\nadaptive fusion strategy to achieve more accurate enhancement effect. In\nAddition, we propose a Structure-guided Attention Block (SAB) to pay more\nattention to texture-riched tokens and avoid interference from noisy areas in\nnoise prediction. Extensive qualitative and quantitative experiments\ndemonstrate that our method achieves SOTA performance on several popular\ndatasets, validating the effectiveness of SDTL in improving image quality and\nthe potential of DiT in low-light enhancement tasks.\n","authors":["Xiangchen Yin","Zhenda Yu","Longtao Jiang","Xin Gao","Xiao Sun","Zhi Liu","Xun Yang"],"pdf_url":"https://arxiv.org/pdf/2504.15054v1.pdf","comment":"Accepted by IEEE Transactions on Multimedia (TMM)"},{"id":"http://arxiv.org/abs/2504.15051v1","updated":"2025-04-21T12:20:46Z","published":"2025-04-21T12:20:46Z","title":"VeLU: Variance-enhanced Learning Unit for Deep Neural Networks","summary":"  Activation functions are fundamental in deep neural networks and directly\nimpact gradient flow, optimization stability, and generalization. Although ReLU\nremains standard because of its simplicity, it suffers from vanishing gradients\nand lacks adaptability. Alternatives like Swish and GELU introduce smooth\ntransitions, but fail to dynamically adjust to input statistics. We propose\nVeLU, a Variance-enhanced Learning Unit as an activation function that\ndynamically scales based on input variance by integrating ArcTan-Sin\ntransformations and Wasserstein-2 regularization, effectively mitigating\ncovariate shifts and stabilizing optimization. Extensive experiments on\nViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm\nVeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.\nThe codes of VeLU are publicly available on GitHub.\n","authors":["Ashkan Shakarami","Yousef Yeganeh","Azade Farshad","Lorenzo Nicolè","Stefano Ghidoni","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2504.15051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15049v1","updated":"2025-04-21T12:12:43Z","published":"2025-04-21T12:12:43Z","title":"ScanEdit: Hierarchically-Guided Functional 3D Scan Editing","summary":"  With the fast pace of 3D capture technology and resulting abundance of 3D\ndata, effective 3D scene editing becomes essential for a variety of graphics\napplications. In this work we present ScanEdit, an instruction-driven method\nfor functional editing of complex, real-world 3D scans. To model large and\ninterdependent sets of ob- jectswe propose a hierarchically-guided approach.\nGiven a 3D scan decomposed into its object instances, we first construct a\nhierarchical scene graph representation to enable effective, tractable editing.\nWe then leverage reason- ing capabilities of Large Language Models (LLMs) and\ntranslate high-level language instructions into actionable commands applied\nhierarchically to the scene graph. Fi- nally, ScanEdit integrates LLM-based\nguidance with ex- plicit physical constraints and generates realistic scenes\nwhere object arrangements obey both physics and common sense. In our extensive\nexperimental evaluation ScanEdit outperforms state of the art and demonstrates\nexcellent re- sults for a variety of real-world scenes and input instruc-\ntions.\n","authors":["Mohamed el amine Boudjoghra","Ivan Laptev","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2504.15049v1.pdf","comment":"Project webpage: https://aminebdj.github.io/scanedit/ Video:\n  https://www.youtube.com/watch?v=Dfmu2g6pVlg"},{"id":"http://arxiv.org/abs/2410.02073v2","updated":"2025-04-21T12:09:08Z","published":"2024-10-02T22:42:20Z","title":"Depth Pro: Sharp Monocular Metric Depth in Less Than a Second","summary":"  We present a foundation model for zero-shot metric monocular depth\nestimation. Our model, Depth Pro, synthesizes high-resolution depth maps with\nunparalleled sharpness and high-frequency details. The predictions are metric,\nwith absolute scale, without relying on the availability of metadata such as\ncamera intrinsics. And the model is fast, producing a 2.25-megapixel depth map\nin 0.3 seconds on a standard GPU. These characteristics are enabled by a number\nof technical contributions, including an efficient multi-scale vision\ntransformer for dense prediction, a training protocol that combines real and\nsynthetic datasets to achieve high metric accuracy alongside fine boundary\ntracing, dedicated evaluation metrics for boundary accuracy in estimated depth\nmaps, and state-of-the-art focal length estimation from a single image.\nExtensive experiments analyze specific design choices and demonstrate that\nDepth Pro outperforms prior work along multiple dimensions. We release code and\nweights at https://github.com/apple/ml-depth-pro\n","authors":["Aleksei Bochkovskii","Amaël Delaunoy","Hugo Germain","Marcel Santos","Yichao Zhou","Stephan R. Richter","Vladlen Koltun"],"pdf_url":"https://arxiv.org/pdf/2410.02073v2.pdf","comment":"Published at ICLR 2025. Code and weights available at\n  https://github.com/apple/ml-depth-pro"},{"id":"http://arxiv.org/abs/2504.15041v1","updated":"2025-04-21T11:53:43Z","published":"2025-04-21T11:53:43Z","title":"Distribution-aware Forgetting Compensation for Exemplar-Free Lifelong\n  Person Re-identification","summary":"  Lifelong Person Re-identification (LReID) suffers from a key challenge in\npreserving old knowledge while adapting to new information. The existing\nsolutions include rehearsal-based and rehearsal-free methods to address this\nchallenge. Rehearsal-based approaches rely on knowledge distillation,\ncontinuously accumulating forgetting during the distillation process.\nRehearsal-free methods insufficiently learn the distribution of each domain,\nleading to forgetfulness over time. To solve these issues, we propose a novel\nDistribution-aware Forgetting Compensation (DAFC) model that explores\ncross-domain shared representation learning and domain-specific distribution\nintegration without using old exemplars or knowledge distillation. We propose a\nText-driven Prompt Aggregation (TPA) that utilizes text features to enrich\nprompt elements and guide the prompt model to learn fine-grained\nrepresentations for each instance. This can enhance the differentiation of\nidentity information and establish the foundation for domain distribution\nawareness. Then, Distribution-based Awareness and Integration (DAI) is designed\nto capture each domain-specific distribution by a dedicated expert network and\nadaptively consolidate them into a shared region in high-dimensional space. In\nthis manner, DAI can consolidate and enhance cross-domain shared representation\nlearning while alleviating catastrophic forgetting. Furthermore, we develop a\nKnowledge Consolidation Mechanism (KCM) that comprises instance-level\ndiscrimination and cross-domain consistency alignment strategies to facilitate\nmodel adaptive learning of new knowledge from the current domain and promote\nknowledge consolidation learning between acquired domain-specific\ndistributions, respectively. Experimental results show that our DAFC outperform\nstate-of-the-art methods by at least 9.8\\%/6.6\\% and 6.4\\%/6.2\\% of average\nmAP/R@1 on two training orders.\n","authors":["Shiben Liu","Huijie Fan","Qiang Wang","Baojie Fan","Yandong Tang","Liangqiong Qu"],"pdf_url":"https://arxiv.org/pdf/2504.15041v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.04161v2","updated":"2025-04-21T11:47:05Z","published":"2024-10-05T13:46:56Z","title":"Overcoming False Illusions in Real-World Face Restoration with\n  Multi-Modal Guided Diffusion Model","summary":"  We introduce a novel Multi-modal Guided Real-World Face Restoration (MGFR)\ntechnique designed to improve the quality of facial image restoration from\nlow-quality inputs. Leveraging a blend of attribute text prompts, high-quality\nreference images, and identity information, MGFR can mitigate the generation of\nfalse facial attributes and identities often associated with generative face\nrestoration methods. By incorporating a dual-control adapter and a two-stage\ntraining strategy, our method effectively utilizes multi-modal prior\ninformation for targeted restoration tasks. We also present the Reface-HQ\ndataset, comprising over 21,000 high-resolution facial images across 4800\nidentities, to address the need for reference face training images. Our\napproach achieves superior visual quality in restoring facial details under\nsevere degradation and allows for controlled restoration processes, enhancing\nthe accuracy of identity preservation and attribute correction. Including\nnegative quality samples and attribute prompts in the training further refines\nthe model's ability to generate detailed and perceptually accurate images.\n","authors":["Keda Tao","Jinjin Gu","Yulun Zhang","Xiucheng Wang","Nan Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.04161v2.pdf","comment":"23 Pages, 28 Figures, ICLR 2025"},{"id":"http://arxiv.org/abs/2504.04756v2","updated":"2025-04-21T11:42:20Z","published":"2025-04-07T06:08:59Z","title":"Continuous Locomotive Crowd Behavior Generation","summary":"  Modeling and reproducing crowd behaviors are important in various domains\nincluding psychology, robotics, transport engineering and virtual environments.\nConventional methods have focused on synthesizing momentary scenes, which have\ndifficulty in replicating the continuous nature of real-world crowds. In this\npaper, we introduce a novel method for automatically generating continuous,\nrealistic crowd trajectories with heterogeneous behaviors and interactions\namong individuals. We first design a crowd emitter model. To do this, we obtain\nspatial layouts from single input images, including a segmentation map,\nappearance map, population density map and population probability, prior to\ncrowd generation. The emitter then continually places individuals on the\ntimeline by assigning independent behavior characteristics such as agents'\ntype, pace, and start/end positions using diffusion models. Next, our crowd\nsimulator produces their long-term locomotions. To simulate diverse actions, it\ncan augment their behaviors based on a Markov chain. As a result, our overall\nframework populates the scenes with heterogeneous crowd behaviors by\nalternating between the proposed emitter and simulator. Note that all the\ncomponents in the proposed framework are user-controllable. Lastly, we propose\na benchmark protocol to evaluate the realism and quality of the generated\ncrowds in terms of the scene-level population dynamics and the individual-level\ntrajectory accuracy. We demonstrate that our approach effectively models\ndiverse crowd behavior patterns and generalizes well across different\ngeographical environments. Code is publicly available at\nhttps://github.com/InhwanBae/CrowdES .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2504.04756v2.pdf","comment":"Accepted at CVPR 2025. Project page:\n  https://ihbae.com/publication/crowdes/"},{"id":"http://arxiv.org/abs/2504.15032v1","updated":"2025-04-21T11:41:22Z","published":"2025-04-21T11:41:22Z","title":"DyST-XL: Dynamic Layout Planning and Content Control for Compositional\n  Text-to-Video Generation","summary":"  Compositional text-to-video generation, which requires synthesizing dynamic\nscenes with multiple interacting entities and precise spatial-temporal\nrelationships, remains a critical challenge for diffusion-based models.\nExisting methods struggle with layout discontinuity, entity identity drift, and\nimplausible interaction dynamics due to unconstrained cross-attention\nmechanisms and inadequate physics-aware reasoning. To address these\nlimitations, we propose DyST-XL, a \\textbf{training-free} framework that\nenhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through\nframe-aware control. DyST-XL integrates three key innovations: (1) A Dynamic\nLayout Planner that leverages large language models (LLMs) to parse input\nprompts into entity-attribute graphs and generates physics-aware keyframe\nlayouts, with intermediate frames interpolated via trajectory optimization; (2)\nA Dual-Prompt Controlled Attention Mechanism that enforces localized text-video\nalignment through frame-aware attention masking, achieving the precise control\nover individual entities; and (3) An Entity-Consistency Constraint strategy\nthat propagates first-frame feature embeddings to subsequent frames during\ndenoising, preserving object identity without manual annotation. Experiments\ndemonstrate that DyST-XL excels in compositional text-to-video generation,\nsignificantly improving performance on complex prompts and bridging a crucial\ngap in training-free video synthesis.\n","authors":["Weijie He","Mushui Liu","Yunlong Yu","Zhao Wang","Chao Wu"],"pdf_url":"https://arxiv.org/pdf/2504.15032v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.15028v1","updated":"2025-04-21T11:29:06Z","published":"2025-04-21T11:29:06Z","title":"A Controllable Appearance Representation for Flexible Transfer and\n  Editing","summary":"  We present a method that computes an interpretable representation of material\nappearance within a highly compact, disentangled latent space. This\nrepresentation is learned in a self-supervised fashion using an adapted\nFactorVAE. We train our model with a carefully designed unlabeled dataset,\navoiding possible biases induced by human-generated labels. Our model\ndemonstrates strong disentanglement and interpretability by effectively\nencoding material appearance and illumination, despite the absence of explicit\nsupervision. Then, we use our representation as guidance for training a\nlightweight IP-Adapter to condition a diffusion pipeline that transfers the\nappearance of one or more images onto a target geometry, and allows the user to\nfurther edit the resulting appearance. Our approach offers fine-grained control\nover the generated results: thanks to the well-structured compact latent space,\nusers can intuitively manipulate attributes such as hue or glossiness in image\nspace to achieve the desired final appearance.\n","authors":["Santiago Jimenez-Navarro","Julia Guerrero-Viu","Belen Masia"],"pdf_url":"https://arxiv.org/pdf/2504.15028v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15026v1","updated":"2025-04-21T11:18:16Z","published":"2025-04-21T11:18:16Z","title":"Gaussian Shading++: Rethinking the Realistic Deployment Challenge of\n  Performance-Lossless Image Watermark for Diffusion Models","summary":"  Ethical concerns surrounding copyright protection and inappropriate content\ngeneration pose challenges for the practical implementation of diffusion\nmodels. One effective solution involves watermarking the generated images.\nExisting methods primarily focus on ensuring that watermark embedding does not\ndegrade the model performance. However, they often overlook critical challenges\nin real-world deployment scenarios, such as the complexity of watermark key\nmanagement, user-defined generation parameters, and the difficulty of\nverification by arbitrary third parties. To address this issue, we propose\nGaussian Shading++, a diffusion model watermarking method tailored for\nreal-world deployment. We propose a double-channel design that leverages\npseudorandom error-correcting codes to encode the random seed required for\nwatermark pseudorandomization, achieving performance-lossless watermarking\nunder a fixed watermark key and overcoming key management challenges.\nAdditionally, we model the distortions introduced during generation and\ninversion as an additive white Gaussian noise channel and employ a novel soft\ndecision decoding strategy during extraction, ensuring strong robustness even\nwhen generation parameters vary. To enable third-party verification, we\nincorporate public key signatures, which provide a certain level of resistance\nagainst forgery attacks even when model inversion capabilities are fully\ndisclosed. Extensive experiments demonstrate that Gaussian Shading++ not only\nmaintains performance losslessness but also outperforms existing methods in\nterms of robustness, making it a more practical solution for real-world\ndeployment.\n","authors":["Zijin Yang","Xin Zhang","Kejiang Chen","Kai Zeng","Qiyi Yao","Han Fang","Weiming Zhang","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2504.15026v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.13620v3","updated":"2025-04-21T11:03:43Z","published":"2025-01-23T12:42:42Z","title":"A Cognitive Paradigm Approach to Probe the Perception-Reasoning\n  Interface in VLMs","summary":"  A fundamental challenge in artificial intelligence involves understanding the\ncognitive processes underlying visual reasoning in sophisticated models like\nVision-Language Models (VLMs). How do these models integrate visual perception\nwith abstract thought, especially when reasoning across multiple images?\nDrawing inspiration from cognitive science, this paper introduces a structured\nevaluation framework using Bongard Problems (BPs) - a classic test of visual\nabstraction to dissect the perception-reasoning interface in VLMs. We propose\nthree distinct evaluation paradigms, mirroring human problem-solving\nstrategies: Direct Visual Rule Learning (DVRL; holistic processing), Deductive\nRule Learning (DRL; rule extraction and application), and Componential Analysis\n(CA; analytical decomposition via textual descriptions). These paradigms allow\nus to systematically vary the cognitive load and probe specific processing\nstages. Notably, the CA paradigm enables the evaluation of multi-image\nreasoning even in VLMs architecturally limited to single images and facilitates\nthe isolation of reasoning capabilities from perceptual limitations by\ncontrolling the descriptive input. Ablation studies further confirm that\nreasoning abilities improve significantly when perceptual challenges are\nmitigated. Our framework provides a valuable diagnostic tool, highlighting the\nneed to enhance visual processing fidelity for achieving more robust and\nhuman-like visual intelligence in AI.\n","authors":["Mohit Vaishnav","Tanel Tammet"],"pdf_url":"https://arxiv.org/pdf/2501.13620v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20197v3","updated":"2025-04-21T10:57:32Z","published":"2024-10-26T15:04:04Z","title":"Transferable Adversarial Attacks on SAM and Its Downstream Models","summary":"  The utilization of large foundational models has a dilemma: while fine-tuning\ndownstream tasks from them holds promise for making use of the well-generalized\nknowledge in practical applications, their open accessibility also poses\nthreats of adverse usage. This paper, for the first time, explores the\nfeasibility of adversarial attacking various downstream models fine-tuned from\nthe segment anything model (SAM), by solely utilizing the information from the\nopen-sourced SAM. In contrast to prevailing transfer-based adversarial attacks,\nwe demonstrate the existence of adversarial dangers even without accessing the\ndownstream task and dataset to train a similar surrogate model. To enhance the\neffectiveness of the adversarial attack towards models fine-tuned on unknown\ndatasets, we propose a universal meta-initialization (UMI) algorithm to extract\nthe intrinsic vulnerability inherent in the foundation model, which is then\nutilized as the prior knowledge to guide the generation of adversarial\nperturbations. Moreover, by formulating the gradient difference in the\nattacking process between the open-sourced SAM and its fine-tuned downstream\nmodels, we theoretically demonstrate that a deviation occurs in the adversarial\nupdate direction by directly maximizing the distance of encoded feature\nembeddings in the open-sourced SAM. Consequently, we propose a gradient robust\nloss that simulates the associated uncertainty with gradient-based noise\naugmentation to enhance the robustness of generated adversarial examples (AEs)\ntowards this deviation, thus improving the transferability. Extensive\nexperiments demonstrate the effectiveness of the proposed universal\nmeta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs\nand their downstream models. Code is available at\nhttps://github.com/xiasong0501/GRAT.\n","authors":["Song Xia","Wenhan Yang","Yi Yu","Xun Lin","Henghui Ding","Ling-Yu Duan","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.20197v3.pdf","comment":"update fig 1"},{"id":"http://arxiv.org/abs/2501.12369v2","updated":"2025-04-21T10:43:56Z","published":"2025-01-21T18:49:06Z","title":"DARB-Splatting: Generalizing Splatting with Decaying Anisotropic Radial\n  Basis Functions","summary":"  Splatting-based 3D reconstruction methods have gained popularity with the\nadvent of 3D Gaussian Splatting, efficiently synthesizing high-quality novel\nviews. These methods commonly resort to using exponential family functions,\nsuch as the Gaussian function, as reconstruction kernels due to their\nanisotropic nature, ease of projection, and differentiability in rasterization.\nHowever, the field remains restricted to variations within the exponential\nfamily, leaving generalized reconstruction kernels largely underexplored,\npartly due to the lack of easy integrability in 3D to 2D projections. In this\nlight, we show that a class of decaying anisotropic radial basis functions\n(DARBFs), which are non-negative functions of the Mahalanobis distance,\nsupports splatting by approximating the Gaussian function's closed-form\nintegration advantage. With this fresh perspective, we demonstrate up to 34%\nfaster convergence during training and a 45% reduction in memory consumption\nacross various DARB reconstruction kernels, while maintaining comparable PSNR,\nSSIM, and LPIPS results. We will make the code available.\n","authors":["Vishagar Arunan","Saeedha Nazar","Hashiru Pramuditha","Vinasirajan Viruthshaan","Sameera Ramasinghe","Simon Lucey","Ranga Rodrigo"],"pdf_url":"https://arxiv.org/pdf/2501.12369v2.pdf","comment":"Link to the project page:\n  https://randomnerds.github.io/darbs.github.io/"},{"id":"http://arxiv.org/abs/2504.13074v3","updated":"2025-04-21T10:34:50Z","published":"2025-04-17T16:37:27Z","title":"SkyReels-V2: Infinite-length Film Generative Model","summary":"  Recent advances in video generation have been driven by diffusion models and\nautoregressive frameworks, yet critical challenges persist in harmonizing\nprompt adherence, visual quality, motion dynamics, and duration: compromises in\nmotion dynamics to enhance temporal visual quality, constrained video duration\n(5-10 seconds) to prioritize resolution, and inadequate shot-aware generation\nstemming from general-purpose MLLMs' inability to interpret cinematic grammar,\nsuch as shot composition, actor expressions, and camera motions. These\nintertwined limitations hinder realistic long-form synthesis and professional\nfilm-style generation. To address these limitations, we propose SkyReels-V2, an\nInfinite-length Film Generative Model, that synergizes Multi-modal Large\nLanguage Model (MLLM), Multi-stage Pretraining, Reinforcement Learning, and\nDiffusion Forcing Framework. Firstly, we design a comprehensive structural\nrepresentation of video that combines the general descriptions by the\nMulti-modal LLM and the detailed shot language by sub-expert models. Aided with\nhuman annotation, we then train a unified Video Captioner, named\nSkyCaptioner-V1, to efficiently label the video data. Secondly, we establish\nprogressive-resolution pretraining for the fundamental video generation,\nfollowed by a four-stage post-training enhancement: Initial concept-balanced\nSupervised Fine-Tuning (SFT) improves baseline quality; Motion-specific\nReinforcement Learning (RL) training with human-annotated and synthetic\ndistortion data addresses dynamic artifacts; Our diffusion forcing framework\nwith non-decreasing noise schedules enables long-video synthesis in an\nefficient search space; Final high-quality SFT refines visual fidelity. All the\ncode and models are available at https://github.com/SkyworkAI/SkyReels-V2.\n","authors":["Guibin Chen","Dixuan Lin","Jiangping Yang","Chunze Lin","Junchen Zhu","Mingyuan Fan","Hao Zhang","Sheng Chen","Zheng Chen","Chengcheng Ma","Weiming Xiong","Wei Wang","Nuo Pang","Kang Kang","Zhiheng Xu","Yuzhe Jin","Yupeng Liang","Yubing Song","Peng Zhao","Boyuan Xu","Di Qiu","Debang Li","Zhengcong Fei","Yang Li","Yahui Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.13074v3.pdf","comment":"31 pages,10 figures"},{"id":"http://arxiv.org/abs/2504.15009v1","updated":"2025-04-21T10:19:12Z","published":"2025-04-21T10:19:12Z","title":"Insert Anything: Image Insertion via In-Context Editing in DiT","summary":"  This work presents Insert Anything, a unified framework for reference-based\nimage insertion that seamlessly integrates objects from reference images into\ntarget scenes under flexible, user-specified control guidance. Instead of\ntraining separate models for individual tasks, our approach is trained once on\nour new AnyInsertion dataset--comprising 120K prompt-image pairs covering\ndiverse tasks such as person, object, and garment insertion--and effortlessly\ngeneralizes to a wide range of insertion scenarios. Such a challenging setting\nrequires capturing both identity features and fine-grained details, while\nallowing versatile local adaptations in style, color, and texture. To this end,\nwe propose to leverage the multimodal attention of the Diffusion Transformer\n(DiT) to support both mask- and text-guided editing. Furthermore, we introduce\nan in-context editing mechanism that treats the reference image as contextual\ninformation, employing two prompting strategies to harmonize the inserted\nelements with the target scene while faithfully preserving their distinctive\nfeatures. Extensive experiments on AnyInsertion, DreamBooth, and VTON-HD\nbenchmarks demonstrate that our method consistently outperforms existing\nalternatives, underscoring its great potential in real-world applications such\nas creative content generation, virtual try-on, and scene composition.\n","authors":["Wensong Song","Hong Jiang","Zongxing Yang","Ruijie Quan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2504.15009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15007v1","updated":"2025-04-21T10:13:59Z","published":"2025-04-21T10:13:59Z","title":"Shifts in Doctors' Eye Movements Between Real and AI-Generated Medical\n  Images","summary":"  Eye-tracking analysis plays a vital role in medical imaging, providing key\ninsights into how radiologists visually interpret and diagnose clinical cases.\nIn this work, we first analyze radiologists' attention and agreement by\nmeasuring the distribution of various eye-movement patterns, including saccades\ndirection, amplitude, and their joint distribution. These metrics help uncover\npatterns in attention allocation and diagnostic strategies. Furthermore, we\ninvestigate whether and how doctors' gaze behavior shifts when viewing\nauthentic (Real) versus deep-learning-generated (Fake) images. To achieve this,\nwe examine fixation bias maps, focusing on first, last, short, and longest\nfixations independently, along with detailed saccades patterns, to quantify\ndifferences in gaze distribution and visual saliency between authentic and\nsynthetic images.\n","authors":["David C Wong","Bin Wang","Gorkem Durak","Marouane Tliba","Mohamed Amine Kerkouri","Aladine Chetouani","Ahmet Enis Cetin","Cagdas Topel","Nicolo Gennaro","Camila Vendrami","Tugce Agirlar Trabzonlu","Amir Ali Rahsepar","Laetitia Perronne","Matthew Antalek","Onural Ozturk","Gokcan Okur","Andrew C. Gordon","Ayis Pyrros","Frank H Miller","Amir A Borhani","Hatice Savas","Eric M. Hart"],"pdf_url":"https://arxiv.org/pdf/2504.15007v1.pdf","comment":"This paper was accepted at ETRA 2025 Japan"},{"id":"http://arxiv.org/abs/2504.15003v1","updated":"2025-04-21T10:04:26Z","published":"2025-04-21T10:04:26Z","title":"NTIRE 2025 Challenge on Short-form UGC Video Quality Assessment and\n  Enhancement: KwaiSR Dataset and Study","summary":"  In this work, we build the first benchmark dataset for short-form UGC Image\nSuper-resolution in the wild, termed KwaiSR, intending to advance the research\non developing image super-resolution algorithms for short-form UGC platforms.\nThis dataset is collected from the Kwai Platform, which is composed of two\nparts, i.e., synthetic and wild parts. Among them, the synthetic dataset,\nincluding 1,900 image pairs, is produced by simulating the degradation\nfollowing the distribution of real-world low-quality short-form UGC images,\naiming to provide the ground truth for training and objective comparison in the\nvalidation/testing. The wild dataset contains low-quality images collected\ndirectly from the Kwai Platform, which are filtered using the quality\nassessment method KVQ from the Kwai Platform. As a result, the KwaiSR dataset\ncontains 1800 synthetic image pairs and 1900 wild images, which are divided\ninto training, validation, and testing parts with a ratio of 8:1:1. Based on\nthe KwaiSR dataset, we organize the NTIRE 2025 challenge on a second short-form\nUGC Video quality assessment and enhancement, which attracts lots of\nresearchers to develop the algorithm for it. The results of this competition\nhave revealed that our KwaiSR dataset is pretty challenging for existing Image\nSR methods, which is expected to lead to a new direction in the image\nsuper-resolution field. The dataset can be found from\nhttps://lixinustc.github.io/NTIRE2025-KVQE-KwaSR-KVQ.github.io/.\n","authors":["Xin Li","Xijun Wang","Bingchen Li","Kun Yuan","Yizhen Shao","Suhang Yao","Ming Sun","Chao Zhou","Radu Timofte","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2504.15003v1.pdf","comment":"KwaiSR dataset, a new dataset for image super-resolution, used for\n  CVPR NTIRE 2025 Challenge; CVPR 2025 workshop paper"},{"id":"http://arxiv.org/abs/2411.03312v2","updated":"2025-04-21T09:34:59Z","published":"2024-11-05T18:54:21Z","title":"Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters","summary":"  Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks, driven by incorporating image\nrepresentations into the token inputs of Large Language Models (LLMs). However,\ntheir real-world deployment is often constrained by high latency during\ninference due to the substantial compute required by the LLM to process the\nlarge number of input tokens, predominantly arising from the image. To reduce\ninference costs, one can either downsize the LLM or reduce the number of input\ntokens needed to represent the image, the latter of which has been the focus of\nmany recent efforts around token compression. However, it is unclear what the\noptimal trade-off is given a fixed inference budget. We first characterize this\noptimal trade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs is achieved by using the largest LLM that\nfits within the inference budget while minimizing visual token count - often to\na single token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take the first steps toward designing token compression algorithms\ntailored for high-compression settings, utilizing prompt-based compression of\ntokens. Our work underscores the performance and efficiency benefits of\noperating in low visual token regimes and the importance of developing tailored\ntoken reduction algorithms for such conditions. Code is available at\nhttps://github.com/locuslab/llava-token-compression.\n","authors":["Kevin Y. Li","Sachin Goyal","Joao D. Semedo","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2411.03312v2.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.14988v1","updated":"2025-04-21T09:30:41Z","published":"2025-04-21T09:30:41Z","title":"Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A\n  Comprehensive Evaluation","summary":"  Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated\nremarkable multimodal perception capabilities, garnering significant attention.\nWhile numerous evaluation studies have emerged, assessing LVLMs both\nholistically and on specialized tasks, fine-grained image tasks-fundamental to\ncomputer vision-remain largely unexplored. To fill this gap, we introduce a\ncomprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 3.49\nmillion questions and 3.32 million images. Our evaluation systematically\nexamines LVLMs from both human-oriented and machine-oriented perspectives,\nfocusing on their semantic recognition and fine-grained feature representation\ncapabilities. Through extensive experiments on eight representative LVLMs/VLMs,\nwe uncover key findings regarding the influence of training paradigms, modality\nalignment, perturbation susceptibility, and fine-grained category reasoning on\ntask performance. This work provides critical insights into the limitations of\ncurrent LVLMs and offers guidance for future data construction and model design\nin the development of more advanced LVLMs. Our code is open-source and\navailable at https://github.com/SEU-VIPGroup/FG-BMK.\n","authors":["Hong-Tao Yu","Xiu-Shen Wei","Yuxin Peng","Serge Belongie"],"pdf_url":"https://arxiv.org/pdf/2504.14988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14977v1","updated":"2025-04-21T09:09:21Z","published":"2025-04-21T09:09:21Z","title":"RealisDance-DiT: Simple yet Strong Baseline towards Controllable\n  Character Animation in the Wild","summary":"  Controllable character animation remains a challenging problem, particularly\nin handling rare poses, stylized characters, character-object interactions,\ncomplex illumination, and dynamic scenes. To tackle these issues, prior work\nhas largely focused on injecting pose and appearance guidance via elaborate\nbypass networks, but often struggles to generalize to open-world scenarios. In\nthis paper, we propose a new perspective that, as long as the foundation model\nis powerful enough, straightforward model modifications with flexible\nfine-tuning strategies can largely address the above challenges, taking a step\ntowards controllable character animation in the wild. Specifically, we\nintroduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our\nsufficient analysis reveals that the widely adopted Reference Net design is\nsuboptimal for large-scale DiT models. Instead, we demonstrate that minimal\nmodifications to the foundation model architecture yield a surprisingly strong\nbaseline. We further propose the low-noise warmup and \"large batches and small\niterations\" strategies to accelerate model convergence during fine-tuning while\nmaximally preserving the priors of the foundation model. In addition, we\nintroduce a new test dataset that captures diverse real-world challenges,\ncomplementing existing benchmarks such as TikTok dataset and UBC fashion video\ndataset, to comprehensively evaluate the proposed method. Extensive experiments\nshow that RealisDance-DiT outperforms existing methods by a large margin.\n","authors":["Jingkai Zhou","Yifan Wu","Shikai Li","Min Wei","Chao Fan","Weihua Chen","Wei Jiang","Fan Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14977v1.pdf","comment":"Project Page:\n  https://thefoxofsky.github.io/project_pages_new/RealisDance-DiT/index"},{"id":"http://arxiv.org/abs/2504.14975v1","updated":"2025-04-21T09:05:52Z","published":"2025-04-21T09:05:52Z","title":"Cyc3D: Fine-grained Controllable 3D Generation via Cycle Consistency\n  Regularization","summary":"  Despite the remarkable progress of 3D generation, achieving controllability,\ni.e., ensuring consistency between generated 3D content and input conditions\nlike edge and depth, remains a significant challenge. Existing methods often\nstruggle to maintain accurate alignment, leading to noticeable discrepancies.\nTo address this issue, we propose \\name{}, a new framework that enhances\ncontrollable 3D generation by explicitly encouraging cyclic consistency between\nthe second-order 3D content, generated based on extracted signals from the\nfirst-order generation, and its original input controls. Specifically, we\nemploy an efficient feed-forward backbone that can generate a 3D object from an\ninput condition and a text prompt. Given an initial viewpoint and a control\nsignal, a novel view is rendered from the generated 3D content, from which the\nextracted condition is used to regenerate the 3D content. This re-generated\noutput is then rendered back to the initial viewpoint, followed by another\nround of control signal extraction, forming a cyclic process with two\nconsistency constraints. \\emph{View consistency} ensures coherence between the\ntwo generated 3D objects, measured by semantic similarity to accommodate\ngenerative diversity. \\emph{Condition consistency} aligns the final extracted\nsignal with the original input control, preserving structural or geometric\ndetails throughout the process. Extensive experiments on popular benchmarks\ndemonstrate that \\name{} significantly improves controllability, especially for\nfine-grained details, outperforming existing methods across various conditions\n(e.g., +14.17\\% PSNR for edge, +6.26\\% PSNR for sketch).\n","authors":["Hongbin Xu","Chaohui Yu","Feng Xiao","Jiazheng Xing","Hai Ci","Weitao Chen","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2504.14975v1.pdf","comment":"Preprint version. The code will be open after finishing the reviewing\n  process"},{"id":"http://arxiv.org/abs/2504.14967v1","updated":"2025-04-21T08:50:12Z","published":"2025-04-21T08:50:12Z","title":"3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact\n  Tensorial Representations","summary":"  Recent studies have combined 3D Gaussian and 3D Morphable Models (3DMM) to\nconstruct high-quality 3D head avatars. In this line of research, existing\nmethods either fail to capture the dynamic textures or incur significant\noverhead in terms of runtime speed or storage space. To this end, we propose a\nnovel method that addresses all the aforementioned demands. In specific, we\nintroduce an expressive and compact representation that encodes texture-related\nattributes of the 3D Gaussians in the tensorial format. We store appearance of\nneutral expression in static tri-planes, and represents dynamic texture details\nfor different expressions using lightweight 1D feature lines, which are then\ndecoded into opacity offset relative to the neutral face. We further propose\nadaptive truncated opacity penalty and class-balanced sampling to improve\ngeneralization across different expressions. Experiments show this design\nenables accurate face dynamic details capturing while maintains real-time\nrendering and significantly reduces storage costs, thus broadening the\napplicability to more scenarios.\n","authors":["Yating Wang","Xuan Wang","Ran Yi","Yanbo Fan","Jichen Hu","Jingcheng Zhu","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2504.14967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04861v3","updated":"2025-04-21T08:37:24Z","published":"2023-12-08T06:31:19Z","title":"Exploring Radar Data Representations in Autonomous Driving: A\n  Comprehensive Review","summary":"  With the rapid advancements of sensor technology and deep learning,\nautonomous driving systems are providing safe and efficient access to\nintelligent vehicles as well as intelligent transportation. Among these\nequipped sensors, the radar sensor plays a crucial role in providing robust\nperception information in diverse environmental conditions. This review focuses\non exploring different radar data representations utilized in autonomous\ndriving systems. Firstly, we introduce the capabilities and limitations of the\nradar sensor by examining the working principles of radar perception and signal\nprocessing of radar measurements. Then, we delve into the generation process of\nfive radar representations, including the ADC signal, radar tensor, point\ncloud, grid map, and micro-Doppler signature. For each radar representation, we\nexamine the related datasets, methods, advantages and limitations. Furthermore,\nwe discuss the challenges faced in these data representations and propose\npotential research directions. Above all, this comprehensive review offers an\nin-depth insight into how these representations enhance autonomous system\ncapabilities, providing guidance for radar perception researchers. To\nfacilitate retrieval and comparison of different data representations, datasets\nand methods, we provide an interactive website at\nhttps://radar-camera-fusion.github.io/radar.\n","authors":["Shanliang Yao","Runwei Guan","Zitian Peng","Chenhang Xu","Yilu Shi","Weiping Ding","Eng Gee Lim","Yong Yue","Hyungjoon Seo","Ka Lok Man","Jieming Ma","Xiaohui Zhu","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2312.04861v3.pdf","comment":"Accepted by TITS"},{"id":"http://arxiv.org/abs/2504.13713v2","updated":"2025-04-21T08:33:42Z","published":"2025-04-18T14:28:34Z","title":"SLAM&Render: A Benchmark for the Intersection Between Neural Rendering,\n  Gaussian Splatting and SLAM","summary":"  Models and methods originally developed for novel view synthesis and scene\nrendering, such as Neural Radiance Fields (NeRF) and Gaussian Splatting, are\nincreasingly being adopted as representations in Simultaneous Localization and\nMapping (SLAM). However, existing datasets fail to include the specific\nchallenges of both fields, such as multimodality and sequentiality in SLAM or\ngeneralization across viewpoints and illumination conditions in neural\nrendering. To bridge this gap, we introduce SLAM&Render, a novel dataset\ndesigned to benchmark methods in the intersection between SLAM and novel view\nrendering. It consists of 40 sequences with synchronized RGB, depth, IMU, robot\nkinematic data, and ground-truth pose streams. By releasing robot kinematic\ndata, the dataset also enables the assessment of novel SLAM strategies when\napplied to robot manipulators. The dataset sequences span five different setups\nfeaturing consumer and industrial objects under four different lighting\nconditions, with separate training and test trajectories per scene, as well as\nobject rearrangements. Our experimental results, obtained with several\nbaselines from the literature, validate SLAM&Render as a relevant benchmark for\nthis emerging research area.\n","authors":["Samuel Cerezo","Gaetano Meli","Tomás Berriel Martins","Kirill Safronov","Javier Civera"],"pdf_url":"https://arxiv.org/pdf/2504.13713v2.pdf","comment":"8 pages, 8 figures, RA-L submission"},{"id":"http://arxiv.org/abs/2504.10842v2","updated":"2025-04-21T08:23:17Z","published":"2025-04-15T03:59:36Z","title":"A comprehensive review of remote sensing in wetland classification and\n  mapping","summary":"  Wetlands constitute critical ecosystems that support both biodiversity and\nhuman well-being; however, they have experienced a significant decline since\nthe 20th century. Back in the 1970s, researchers began to employ remote sensing\ntechnologies for wetland classification and mapping to elucidate the extent and\nvariations of wetlands. Although some review articles summarized the\ndevelopment of this field, there is a lack of a thorough and in-depth\nunderstanding of wetland classification and mapping: (1) the scientific\nimportance of wetlands, (2) major data, methods used in wetland classification\nand mapping, (3) driving factors of wetland changes, (4) current research\nparadigm and limitations, (5) challenges and opportunities in wetland\nclassification and mapping under the context of technological innovation and\nglobal environmental change. In this review, we aim to provide a comprehensive\nperspective and new insights into wetland classification and mapping for\nreaders to answer these questions. First, we conduct a meta-analysis of over\n1,200 papers, encompassing wetland types, methods, sensor types, and study\nsites, examining prevailing trends in wetland classification and mapping. Next,\nwe review and synthesize the wetland features and existing data and methods in\nwetland classification and mapping. We also summarize typical wetland mapping\nproducts and explore the intrinsic driving factors of wetland changes across\nmultiple spatial and temporal scales. Finally, we discuss current limitations\nand propose future directions in response to global environmental change and\ntechnological innovation. This review consolidates our understanding of wetland\nremote sensing and offers scientific recommendations that foster transformative\nprogress in wetland science.\n","authors":["Shuai Yuan","Xiangan Liang","Tianwu Lin","Shuang Chen","Rui Liu","Jie Wang","Hongsheng Zhang","Peng Gong"],"pdf_url":"https://arxiv.org/pdf/2504.10842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14952v1","updated":"2025-04-21T08:22:58Z","published":"2025-04-21T08:22:58Z","title":"PIV-FlowDiffuser:Transfer-learning-based denoising diffusion models for\n  PIV","summary":"  Deep learning algorithms have significantly reduced the computational time\nand improved the spatial resolution of particle image velocimetry~(PIV).\nHowever, the models trained on synthetic datasets might have a degraded\nperformance on practical particle images due to domain gaps. As a result,\nspecial residual patterns are often observed for the vector fields of deep\nlearning-based estimators. To reduce the special noise step-by-step, we employ\na denoising diffusion model~(FlowDiffuser) for PIV analysis. And the\ndata-hungry iterative denoising diffusion model is trained via a transfer\nlearning strategy, resulting in our PIV-FlowDiffuser method. Specifically, (1)\npre-training a FlowDiffuser model with multiple optical flow datasets of the\ncomputer vision community, such as Sintel, KITTI, etc; (2) fine-tuning the\npre-trained model on synthetic PIV datasets. Note that the PIV images are\nupsampled by a factor of two to resolve the small-scale turbulent flow\nstructures. The visualized results indicate that our PIV-FlowDiffuser\neffectively suppresses the noise patterns. Therefore, the denoising diffusion\nmodel reduces the average end-point error~($AEE$) by 59.4% over RAFT256-PIV\nbaseline on the classic Cai's dataset. Besides, PIV-FlowDiffuser exhibits\nenhanced generalization performance on unseen particle images due to transfer\nlearning. Overall, this study highlights the transfer-learning-based denoising\ndiffusion models for PIV. And a detailed implementation is recommended for\ninterested readers in the repository\nhttps://github.com/Zhu-Qianyu/PIV-FlowDiffuser.\n","authors":["Qianyu Zhu","Junjie Wang","Jeremiah Hu","Jia Ai","Yong Lee"],"pdf_url":"https://arxiv.org/pdf/2504.14952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18961v9","updated":"2025-04-21T08:17:03Z","published":"2023-10-29T10:03:49Z","title":"AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly\n  Detection","summary":"  Zero-shot anomaly detection (ZSAD) requires detection models trained using\nauxiliary data to detect anomalies without any training sample in a target\ndataset. It is a crucial task when training data is not accessible due to\nvarious concerns, eg, data privacy, yet it is challenging since the models need\nto generalize to anomalies across different domains where the appearance of\nforeground objects, abnormal regions, and background features, such as\ndefects/tumors on different products/organs, can vary significantly. Recently\nlarge pre-trained vision-language models (VLMs), such as CLIP, have\ndemonstrated strong zero-shot recognition ability in various vision tasks,\nincluding anomaly detection. However, their ZSAD performance is weak since the\nVLMs focus more on modeling the class semantics of the foreground objects\nrather than the abnormality/normality in the images. In this paper we introduce\na novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across\ndifferent domains. The key insight of AnomalyCLIP is to learn object-agnostic\ntext prompts that capture generic normality and abnormality in an image\nregardless of its foreground objects. This allows our model to focus on the\nabnormal image regions rather than the object semantics, enabling generalized\nnormality and abnormality recognition on diverse types of objects. Large-scale\nexperiments on 17 real-world anomaly detection datasets show that AnomalyCLIP\nachieves superior zero-shot performance of detecting and segmenting anomalies\nin datasets of highly diverse class semantics from various defect inspection\nand medical imaging domains. Code will be made available at\nhttps://github.com/zqhang/AnomalyCLIP.\n","authors":["Qihang Zhou","Guansong Pang","Yu Tian","Shibo He","Jiming Chen"],"pdf_url":"https://arxiv.org/pdf/2310.18961v9.pdf","comment":"Accepted by ICLR 2024"},{"id":"http://arxiv.org/abs/2504.12626v2","updated":"2025-04-21T08:13:35Z","published":"2025-04-17T04:02:31Z","title":"Packing Input Frame Context in Next-Frame Prediction Models for Video\n  Generation","summary":"  We present a neural network structure, FramePack, to train next-frame (or\nnext-frame-section) prediction models for video generation. The FramePack\ncompresses input frames to make the transformer context length a fixed number\nregardless of the video length. As a result, we are able to process a large\nnumber of frames using video diffusion with computation bottleneck similar to\nimage diffusion. This also makes the training video batch sizes significantly\nhigher (batch sizes become comparable to image diffusion training). We also\npropose an anti-drifting sampling method that generates frames in inverted\ntemporal order with early-established endpoints to avoid exposure bias (error\naccumulation over iterations). Finally, we show that existing video diffusion\nmodels can be finetuned with FramePack, and their visual quality may be\nimproved because the next-frame prediction supports more balanced diffusion\nschedulers with less extreme flow shift timesteps.\n","authors":["Lvmin Zhang","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2504.12626v2.pdf","comment":"https://github.com/lllyasviel/FramePack"},{"id":"http://arxiv.org/abs/2504.14933v1","updated":"2025-04-21T07:53:58Z","published":"2025-04-21T07:53:58Z","title":"TWIG: Two-Step Image Generation using Segmentation Masks in Diffusion\n  Models","summary":"  In today's age of social media and marketing, copyright issues can be a major\nroadblock to the free sharing of images. Generative AI models have made it\npossible to create high-quality images, but concerns about copyright\ninfringement are a hindrance to their abundant use. As these models use data\nfrom training images to generate new ones, it is often a daunting task to\nensure they do not violate intellectual property rights. Some AI models have\neven been noted to directly copy copyrighted images, a problem often referred\nto as source copying. Traditional copyright protection measures such as\nwatermarks and metadata have also proven to be futile in this regard. To\naddress this issue, we propose a novel two-step image generation model inspired\nby the conditional diffusion model. The first step involves creating an image\nsegmentation mask for some prompt-based generated images. This mask embodies\nthe shape of the image. Thereafter, the diffusion model is asked to generate\nthe image anew while avoiding the shape in question. This approach shows a\ndecrease in structural similarity from the training image, i.e. we are able to\navoid the source copying problem using this approach without expensive\nretraining of the model or user-centered prompt generation techniques. This\nmakes our approach the most computationally inexpensive approach to avoiding\nboth copyright infringement and source copying for diffusion model-based image\ngeneration.\n","authors":["Mazharul Islam Rakib","Showrin Rahman","Joyanta Jyoti Mondal","Xi Xiao","David Lewis","Alessandra Mileo","Meem Arafat Manab"],"pdf_url":"https://arxiv.org/pdf/2504.14933v1.pdf","comment":"16 pages, 9 figures, preprint"},{"id":"http://arxiv.org/abs/2504.11014v3","updated":"2025-04-21T07:47:10Z","published":"2025-04-15T09:37:54Z","title":"GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*","summary":"  The emerging trend in computer vision emphasizes developing universal models\ncapable of simultaneously addressing multiple diverse tasks. Such universality\ntypically requires joint training across multi-domain datasets to ensure\neffective generalization. However, monocular 3D object detection presents\nunique challenges in multi-domain training due to the scarcity of datasets\nannotated with accurate 3D ground-truth labels, especially beyond typical\nroad-based autonomous driving contexts. To address this challenge, we introduce\na novel weakly supervised framework leveraging pseudo-labels. Current\npretrained models often struggle to accurately detect pedestrians in non-road\nenvironments due to inherent dataset biases. Unlike generalized image-based 2D\nobject detection models, achieving similar generalization in monocular 3D\ndetection remains largely unexplored. In this paper, we propose GATE3D, a novel\nframework designed specifically for generalized monocular 3D object detection\nvia weak supervision. GATE3D effectively bridges domain gaps by employing\nconsistency losses between 2D and 3D predictions. Remarkably, our model\nachieves competitive performance on the KITTI benchmark as well as on an\nindoor-office dataset collected by us to evaluate the generalization\ncapabilities of our framework. Our results demonstrate that GATE3D\nsignificantly accelerates learning from limited annotated data through\neffective pre-training strategies, highlighting substantial potential for\nbroader impacts in robotics, augmented reality, and virtual reality\napplications. Project page: https://ies0411.github.io/GATE3D/\n","authors":["Eunsoo Im","Changhyun Jee","Jung Kwon Lee"],"pdf_url":"https://arxiv.org/pdf/2504.11014v3.pdf","comment":"Accepted (Poster) to the 3rd CV4MR Workshop at CVPR 2025:\n  https://openreview.net/forum?id=00RQ8Cv3ia"},{"id":"http://arxiv.org/abs/2502.17850v2","updated":"2025-04-21T07:47:00Z","published":"2025-02-25T04:54:24Z","title":"A Novel Retinal Image Contrast Enhancement -- Fuzzy-Based Method","summary":"  The vascular structure in retinal images plays a crucial role in ophthalmic\ndiagnostics, and its accuracies are directly influenced by the quality of the\nretinal image. Contrast enhancement is one of the crucial steps in any\nsegmentation algorithm - the more so since the retinal images are related to\nmedical diagnosis. Contrast enhancement is a vital step that not only\nintensifies the darkness of the blood vessels but also prevents minor\ncapillaries from being disregarded during the process. This paper proposes a\nnovel model that utilizes the linear blending of Fuzzy Contrast Enhancement\n(FCE) and Contrast Limited Adaptive Histogram Equalization (CLAHE) to enhance\nthe retinal image for retinal vascular structure segmentation. The scheme is\ntested using the Digital Retinal Images for Vessel Extraction (DRIVE) dataset.\nThe assertion was then evaluated through performance comparison among other\nmethodologies which are Gray-scaling, Histogram Equalization (HE), FCE, and\nCLAHE. It was evident in this paper that the combination of FCE and CLAHE\nmethods showed major improvement. Both FCE and CLAHE methods dominating with\n88% as better enhancement methods proved that preprocessing through fuzzy logic\nis effective.\n","authors":["Adnan Shaout","Jiho Han"],"pdf_url":"https://arxiv.org/pdf/2502.17850v2.pdf","comment":"This version corrects a typographical error in the title of the\n  previous submission with no changes to the content of the paper"},{"id":"http://arxiv.org/abs/2504.14921v1","updated":"2025-04-21T07:40:35Z","published":"2025-04-21T07:40:35Z","title":"Fast Adversarial Training with Weak-to-Strong Spatial-Temporal\n  Consistency in the Frequency Domain on Videos","summary":"  Adversarial Training (AT) has been shown to significantly enhance adversarial\nrobustness via a min-max optimization approach. However, its effectiveness in\nvideo recognition tasks is hampered by two main challenges. First, fast\nadversarial training for video models remains largely unexplored, which\nseverely impedes its practical applications. Specifically, most video\nadversarial training methods are computationally costly, with long training\ntimes and high expenses. Second, existing methods struggle with the trade-off\nbetween clean accuracy and adversarial robustness. To address these challenges,\nwe introduce Video Fast Adversarial Training with Weak-to-Strong consistency\n(VFAT-WS), the first fast adversarial training method for video data.\nSpecifically, VFAT-WS incorporates the following key designs: First, it\nintegrates a straightforward yet effective temporal frequency augmentation\n(TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a\nsingle-step PGD attack to boost training efficiency and robustness. Second, it\ndevises a weak-to-strong spatial-temporal consistency regularization, which\nseamlessly integrates the simpler TF-AUG and the more complex STF-AUG.\nLeveraging the consistency regularization, it steers the learning process from\nsimple to complex augmentations. Both of them work together to achieve a better\ntrade-off between clean accuracy and robustness. Extensive experiments on\nUCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that\nVFAT-WS achieves great improvements in adversarial robustness and corruption\nrobustness, while accelerating training by nearly 490%.\n","authors":["Songping Wang","Hanqing Liu","Yueming Lyu","Xiantao Hu","Ziwen He","Wei Wang","Caifeng Shan","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14920v1","updated":"2025-04-21T07:39:29Z","published":"2025-04-21T07:39:29Z","title":"DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in\n  Fine-Grained Visual Understanding","summary":"  Humans can effortlessly locate desired objects in cluttered environments,\nrelying on a cognitive mechanism known as visual search to efficiently filter\nout irrelevant information and focus on task-related regions. Inspired by this\nprocess, we propose Dyfo (Dynamic Focus), a training-free dynamic focusing\nvisual search method that enhances fine-grained visual understanding in large\nmultimodal models (LMMs). Unlike existing approaches which require additional\nmodules or data collection, Dyfo leverages a bidirectional interaction between\nLMMs and visual experts, using a Monte Carlo Tree Search (MCTS) algorithm to\nsimulate human-like focus adjustments. This enables LMMs to focus on key visual\nregions while filtering out irrelevant content, without introducing additional\ntraining caused by vocabulary expansion or the integration of specialized\nlocalization modules. Experimental results demonstrate that Dyfo significantly\nimproves fine-grained visual understanding and reduces hallucination issues in\nLMMs, achieving superior performance across both fixed and dynamic resolution\nmodels. The code is available at https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025\n","authors":["Geng Li","Jinglin Xu","Yunzhen Zhao","Yuxin Peng"],"pdf_url":"https://arxiv.org/pdf/2504.14920v1.pdf","comment":"Accepted by CVPR 2025 (Hightlight). Project page with code:\n  https://github.com/PKU-ICST-MIPL/DyFo_CVPR2025"},{"id":"http://arxiv.org/abs/2504.14919v1","updated":"2025-04-21T07:38:25Z","published":"2025-04-21T07:38:25Z","title":"GenCLIP: Generalizing CLIP Prompts for Zero-shot Anomaly Detection","summary":"  Zero-shot anomaly detection (ZSAD) aims to identify anomalies in unseen\ncategories by leveraging CLIP's zero-shot capabilities to match text prompts\nwith visual features. A key challenge in ZSAD is learning general prompts\nstably and utilizing them effectively, while maintaining both generalizability\nand category specificity. Although general prompts have been explored in prior\nworks, achieving their stable optimization and effective deployment remains a\nsignificant challenge. In this work, we propose GenCLIP, a novel framework that\nlearns and leverages general prompts more effectively through multi-layer\nprompting and dual-branch inference. Multi-layer prompting integrates\ncategory-specific visual cues from different CLIP layers, enriching general\nprompts with more comprehensive and robust feature representations. By\ncombining general prompts with multi-layer visual features, our method further\nenhances its generalization capability. To balance specificity and\ngeneralization, we introduce a dual-branch inference strategy, where a\nvision-enhanced branch captures fine-grained category-specific features, while\na query-only branch prioritizes generalization. The complementary outputs from\nboth branches improve the stability and reliability of anomaly detection across\nunseen categories. Additionally, we propose an adaptive text prompt filtering\nmechanism, which removes irrelevant or atypical class names not encountered\nduring CLIP's training, ensuring that only meaningful textual inputs contribute\nto the final vision-language alignment.\n","authors":["Donghyeong Kim","Chaewon Park","Suhwan Cho","Hyeonjeong Lim","Minseok Kang","Jungho Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2504.14919v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14913v1","updated":"2025-04-21T07:32:28Z","published":"2025-04-21T07:32:28Z","title":"Guidelines for External Disturbance Factors in the Use of OCR in\n  Real-World Environments","summary":"  The performance of OCR has improved with the evolution of AI technology. As\nOCR continues to broaden its range of applications, the increased likelihood of\ninterference introduced by various usage environments can prevent it from\nachieving its inherent performance. This results in reduced recognition\naccuracy under certain conditions, and makes the quality control of recognition\ndevices more challenging. Therefore, to ensure that users can properly utilize\nOCR, we compiled the real-world external disturbance factors that cause\nperformance degradation, along with the resulting image degradation phenomena,\ninto an external disturbance factor table and, by also indicating how to make\nuse of it, organized them into guidelines.\n","authors":["Kenji Iwata","Eiki Ishidera","Toshifumi Yamaai","Yutaka Satoh","Hiroshi Tanaka","Katsuhiko Takahashi","Akio Furuhata","Yoshihisa Tanabe","Hiroshi Matsumura"],"pdf_url":"https://arxiv.org/pdf/2504.14913v1.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2504.11922v2","updated":"2025-04-21T07:27:30Z","published":"2025-04-16T09:57:23Z","title":"Zooming In on Fakes: A Novel Dataset for Localized AI-Generated Image\n  Detection with Forgery Amplification Approach","summary":"  The rise of AI-generated image editing tools has made localized forgeries\nincreasingly realistic, posing challenges for visual content integrity.\nAlthough recent efforts have explored localized AIGC detection, existing\ndatasets predominantly focus on object-level forgeries while overlooking\nbroader scene edits in regions such as sky or ground. To address these\nlimitations, we introduce \\textbf{BR-Gen}, a large-scale dataset of 150,000\nlocally forged images with diverse scene-aware annotations, which are based on\nsemantic calibration to ensure high-quality samples. BR-Gen is constructed\nthrough a fully automated Perception-Creation-Evaluation pipeline to ensure\nsemantic coherence and visual realism. In addition, we further propose\n\\textbf{NFA-ViT}, a Noise-guided Forgery Amplification Vision Transformer that\nenhances the detection of localized forgeries by amplifying forgery-related\nfeatures across the entire image. NFA-ViT mines heterogeneous regions in\nimages, \\emph{i.e.}, potential edited areas, by noise fingerprints.\nSubsequently, attention mechanism is introduced to compel the interaction\nbetween normal and abnormal features, thereby propagating the generalization\ntraces throughout the entire image, allowing subtle forgeries to influence a\nbroader context and improving overall detection robustness. Extensive\nexperiments demonstrate that BR-Gen constructs entirely new scenarios that are\nnot covered by existing methods. Take a step further, NFA-ViT outperforms\nexisting methods on BR-Gen and generalizes well across current benchmarks. All\ndata and codes are available at https://github.com/clpbc/BR-Gen.\n","authors":["Lvpan Cai","Haowei Wang","Jiayi Ji","YanShu ZhouMen","Yiwei Ma","Xiaoshuai Sun","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2504.11922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11326v2","updated":"2025-04-21T07:23:45Z","published":"2025-04-15T16:02:47Z","title":"PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of\n  Complex Videos in the Wild","summary":"  This report provides a comprehensive overview of the 4th Pixel-level Video\nUnderstanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025.\nIt summarizes the challenge outcomes, participating methodologies, and future\nresearch directions. The challenge features two tracks: MOSE, which focuses on\ncomplex scene video object segmentation, and MeViS, which targets\nmotion-guided, language-based video segmentation. Both tracks introduce new,\nmore challenging datasets designed to better reflect real-world scenarios.\nThrough detailed evaluation and analysis, the challenge offers valuable\ninsights into the current state-of-the-art and emerging trends in complex video\nsegmentation. More information can be found on the workshop website:\nhttps://pvuw.github.io/.\n","authors":["Henghui Ding","Chang Liu","Nikhila Ravi","Shuting He","Yunchao Wei","Song Bai","Philip Torr","Kehuan Song","Xinglin Xie","Kexin Zhang","Licheng Jiao","Lingling Li","Shuyuan Yang","Xuqiang Cao","Linnan Zhao","Jiaxuan Zhao","Fang Liu","Mengjiao Wang","Junpei Zhang","Xu Liu","Yuting Yang","Mengru Ma","Hao Fang","Runmin Cong","Xiankai Lu","Zhiyang Chen","Wei Zhang","Tianming Liang","Haichao Jiang","Wei-Shi Zheng","Jian-Fang Hu","Haobo Yuan","Xiangtai Li","Tao Zhang","Lu Qi","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2504.11326v2.pdf","comment":"Workshop Page: https://pvuw.github.io/. arXiv admin note: text\n  overlap with arXiv:2504.00476, arXiv:2504.05178"},{"id":"http://arxiv.org/abs/2504.14906v1","updated":"2025-04-21T07:21:28Z","published":"2025-04-21T07:21:28Z","title":"OmniAudio: Generating Spatial Audio from 360-Degree Video","summary":"  Traditional video-to-audio generation techniques primarily focus on\nfield-of-view (FoV) video and non-spatial audio, often missing the spatial cues\nnecessary for accurately representing sound sources in 3D environments. To\naddress this limitation, we introduce a novel task, 360V2SA, to generate\nspatial audio from 360-degree videos, specifically producing First-order\nAmbisonics (FOA) audio - a standard format for representing 3D spatial audio\nthat captures sound directionality and enables realistic 3D audio reproduction.\nWe first create Sphere360, a novel dataset tailored for this task that is\ncurated from real-world data. We also design an efficient semi-automated\npipeline for collecting and cleaning paired video-audio data. To generate\nspatial audio from 360-degree video, we propose a novel framework OmniAudio,\nwhich leverages self-supervised pre-training using both spatial audio data (in\nFOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a\ndual-branch framework that utilizes both panoramic and FoV video inputs to\ncapture comprehensive local and global information from 360-degree videos.\nExperimental results demonstrate that OmniAudio achieves state-of-the-art\nperformance across both objective and subjective metrics on Sphere360. Code and\ndatasets will be released at https://github.com/liuhuadai/OmniAudio. The demo\npage is available at https://OmniAudio-360V2SA.github.io.\n","authors":["Huadai Liu","Tianyi Luo","Qikai Jiang","Kaicheng Luo","Peiwen Sun","Jialei Wan","Rongjie Huang","Qian Chen","Wen Wang","Xiangtai Li","Shiliang Zhang","Zhijie Yan","Zhou Zhao","Wei Xue"],"pdf_url":"https://arxiv.org/pdf/2504.14906v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2504.10329v2","updated":"2025-04-21T07:21:01Z","published":"2025-04-14T15:36:28Z","title":"InstructEngine: Instruction-driven Text-to-Image Alignment","summary":"  Reinforcement Learning from Human/AI Feedback (RLHF/RLAIF) has been\nextensively utilized for preference alignment of text-to-image models. Existing\nmethods face certain limitations in terms of both data and algorithm. For\ntraining data, most approaches rely on manual annotated preference data, either\nby directly fine-tuning the generators or by training reward models to provide\ntraining signals. However, the high annotation cost makes them difficult to\nscale up, the reward model consumes extra computation and cannot guarantee\naccuracy. From an algorithmic perspective, most methods neglect the value of\ntext and only take the image feedback as a comparative signal, which is\ninefficient and sparse. To alleviate these drawbacks, we propose the\nInstructEngine framework. Regarding annotation cost, we first construct a\ntaxonomy for text-to-image generation, then develop an automated data\nconstruction pipeline based on it. Leveraging advanced large multimodal models\nand human-defined rules, we generate 25K text-image preference pairs. Finally,\nwe introduce cross-validation alignment method, which refines data efficiency\nby organizing semantically analogous samples into mutually comparable pairs.\nEvaluations on DrawBench demonstrate that InstructEngine improves SD v1.5 and\nSDXL's performance by 10.53% and 5.30%, outperforming state-of-the-art\nbaselines, with ablation study confirming the benefits of InstructEngine's all\ncomponents. A win rate of over 50% in human reviews also proves that\nInstructEngine better aligns with human preferences.\n","authors":["Xingyu Lu","Yuhang Hu","YiFan Zhang","Kaiyu Jiang","Changyi Liu","Tianke Zhang","Jinpeng Wang","Chun Yuan","Bin Wen","Fan Yang","Tingting Gao","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.10329v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.02901v4","updated":"2025-04-21T07:20:39Z","published":"2023-10-04T15:39:57Z","title":"Efficient Vectorized Backpropagation Algorithms for Training Feedforward\n  Networks Composed of Quadratic Neurons","summary":"  Higher order artificial neurons whose outputs are computed by applying an\nactivation function to a higher order multinomial function of the inputs have\nbeen considered in the past, but did not gain acceptance due to the extra\nparameters and computational cost. However, higher order neurons have\nsignificantly greater learning capabilities since the decision boundaries of\nhigher order neurons can be complex surfaces instead of just hyperplanes. The\nboundary of a single quadratic neuron can be a general hyper-quadric surface\nallowing it to learn many nonlinearly separable datasets. Since quadratic forms\ncan be represented by symmetric matrices, only $\\frac{n(n+1)}{2}$ additional\nparameters are needed instead of $n^2$. A quadratic Logistic regression model\nis first presented. Solutions to the XOR problem with a single quadratic neuron\nare considered. The complete vectorized equations for both forward and backward\npropagation in feedforward networks composed of quadratic neurons are derived.\nA reduced parameter quadratic neural network model with just $ n $ additional\nparameters per neuron that provides a compromise between learning ability and\ncomputational cost is presented. Comparison on benchmark classification\ndatasets are used to demonstrate that a final layer of quadratic neurons\nenables networks to achieve higher accuracy with significantly fewer hidden\nlayer neurons. In particular this paper shows that any dataset composed of\n$\\mathcal{C}$ bounded clusters can be separated with only a single layer of\n$\\mathcal{C}$ quadratic neurons.\n","authors":["Mathew Mithra Noel","Venkataraman Muthiah-Nakarajan","Yug D Oswal"],"pdf_url":"https://arxiv.org/pdf/2310.02901v4.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2504.14899v1","updated":"2025-04-21T07:10:41Z","published":"2025-04-21T07:10:41Z","title":"Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls\n  for Video Generation","summary":"  Camera and human motion controls have been extensively studied for video\ngeneration, but existing approaches typically address them separately,\nsuffering from limited data with high-quality annotations for both aspects. To\novercome this, we present Uni3C, a unified 3D-enhanced framework for precise\ncontrol of both camera and human motion in video generation. Uni3C includes two\nkey contributions. First, we propose a plug-and-play control module trained\nwith a frozen video generative backbone, PCDController, which utilizes\nunprojected point clouds from monocular depth to achieve accurate camera\ncontrol. By leveraging the strong 3D priors of point clouds and the powerful\ncapacities of video foundational models, PCDController shows impressive\ngeneralization, performing well regardless of whether the inference backbone is\nfrozen or fine-tuned. This flexibility enables different modules of Uni3C to be\ntrained in specific domains, i.e., either camera control or human motion\ncontrol, reducing the dependency on jointly annotated data. Second, we propose\na jointly aligned 3D world guidance for the inference phase that seamlessly\nintegrates both scenic point clouds and SMPL-X characters to unify the control\nsignals for camera and human motion, respectively. Extensive experiments\nconfirm that PCDController enjoys strong robustness in driving camera motion\nfor fine-tuned backbones of video generation. Uni3C substantially outperforms\ncompetitors in both camera controllability and human motion quality.\nAdditionally, we collect tailored validation sets featuring challenging camera\nmovements and human actions to validate the effectiveness of our method.\n","authors":["Chenjie Cao","Jingkai Zhou","Shikai Li","Jingyun Liang","Chaohui Yu","Fan Wang","Xiangyang Xue","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2504.14899v1.pdf","comment":"Project page: https://github.com/ewrfcas/Uni3C"},{"id":"http://arxiv.org/abs/2504.06920v2","updated":"2025-04-21T06:51:54Z","published":"2025-04-09T14:25:35Z","title":"S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in\n  Remote Sensing Applications","summary":"  We introduce the S-EO dataset: a large-scale, high-resolution dataset,\ndesigned to advance geometry-aware shadow detection. Collected from diverse\npublic-domain sources, including challenge datasets and government providers\nsuch as USGS, our dataset comprises 702 georeferenced tiles across the USA,\neach covering 500x500 m. Each tile includes multi-date, multi-angle WorldView-3\npansharpened RGB images, panchromatic images, and a ground-truth DSM of the\narea obtained from LiDAR scans. For each image, we provide a shadow mask\nderived from geometry and sun position, a vegetation mask based on the NDVI\nindex, and a bundle-adjusted RPC model. With approximately 20,000 images, the\nS-EO dataset establishes a new public resource for shadow detection in remote\nsensing imagery and its applications to 3D reconstruction. To demonstrate the\ndataset's impact, we train and evaluate a shadow detector, showcasing its\nability to generalize, even to aerial images. Finally, we extend EO-NeRF - a\nstate-of-the-art NeRF approach for satellite imagery - to leverage our shadow\npredictions for improved 3D reconstructions.\n","authors":["Elías Masquil","Roger Marí","Thibaud Ehret","Enric Meinhardt-Llopis","Pablo Musé","Gabriele Facciolo"],"pdf_url":"https://arxiv.org/pdf/2504.06920v2.pdf","comment":"Accepted at Earthvision 2025 (CVPR Workshop)"},{"id":"http://arxiv.org/abs/2504.14888v1","updated":"2025-04-21T06:32:25Z","published":"2025-04-21T06:32:25Z","title":"WMKA-Net: A Weighted Multi-Kernel Attention NetworkMethod for Retinal\n  Vessel Segmentation","summary":"  We propose a novel retinal vessel segmentation network, the Weighted\nMulti-Kernel Attention Network (WMKA-Net), which aims to address the issues of\ninsufficient multiscale feature capture, loss of contextual information, and\nnoise sensitivity in retinal vessel segmentation. WMKA-Net significantly\nimproves the segmentation performance of small vessels and low-contrast regions\nby integrating several innovative components, including the MultiKernelFeature\nFusion Module (MKDC), the Progressive Feature Weighting Fusion Strategy (UDFF),\nand the Attention Mechanism Module (AttentionBlock). The MKDC module employs\nmultiscale parallel convolutional kernels to extract vessel characteristics,\nthereby enhancing the ability to capture complex vascular structures. The UDFF\nstrategy optimizes the transmission of feature information by weighted fusion\nof high- and low-level features. The AttentionBlock highlights key regions and\nsuppresses noise interference through the attention mechanism. Experimental\nresults demonstrate that WMKA-Net achieves excellent segmentation performance\nin multiple public datasets, particularly in segmentation of small vessels and\nprocessing of pathological regions. This work provides a robust and efficient\nnew method for segmentation of the retinal vessel.\n","authors":["Xinran Xu","Yuliang Ma","Sifu Cai"],"pdf_url":"https://arxiv.org/pdf/2504.14888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21822v2","updated":"2025-04-21T06:30:30Z","published":"2024-10-29T07:45:59Z","title":"PK-YOLO: Pretrained Knowledge Guided YOLO for Brain Tumor Detection in\n  Multiplanar MRI Slices","summary":"  Brain tumor detection in multiplane Magnetic Resonance Imaging (MRI) slices\nis a challenging task due to the various appearances and relationships in the\nstructure of the multiplane images. In this paper, we propose a new You Only\nLook Once (YOLO)-based detection model that incorporates Pretrained Knowledge\n(PK), called PK-YOLO, to improve the performance for brain tumor detection in\nmultiplane MRI slices. To our best knowledge, PK-YOLO is the first pretrained\nknowledge guided YOLO-based object detector. The main components of the new\nmethod are a pretrained pure lightweight convolutional neural network-based\nbackbone via sparse masked modeling, a YOLO architecture with the pretrained\nbackbone, and a regression loss function for improving small object detection.\nThe pretrained backbone allows for feature transferability of object queries on\nindividual plane MRI slices into the model encoders, and the learned domain\nknowledge base can improve in-domain detection. The improved loss function can\nfurther boost detection performance on small-size brain tumors in multiplanar\ntwo-dimensional MRI slices. Experimental results show that the proposed PK-YOLO\nachieves competitive performance on the multiplanar MRI brain tumor detection\ndatasets compared to state-of-the-art YOLO-like and DETR-like object detectors.\nThe code is available at https://github.com/mkang315/PK-YOLO.\n","authors":["Ming Kang","Fung Fung Ting","Raphaël C. -W. Phan","Chee-Ming Ting"],"pdf_url":"https://arxiv.org/pdf/2410.21822v2.pdf","comment":"References updated; for example, papers in NeurIPS 2024 proceedings\n  appeared on 6 Feb 2025 and AAAI 2025 one on 11 Apr 2025"},{"id":"http://arxiv.org/abs/2503.08224v2","updated":"2025-04-21T06:29:45Z","published":"2025-03-11T09:42:40Z","title":"HRAvatar: High-Quality and Relightable Gaussian Head Avatar","summary":"  Reconstructing animatable and high-quality 3D head avatars from monocular\nvideos, especially with realistic relighting, is a valuable task. However, the\nlimited information from single-view input, combined with the complex head\nposes and facial movements, makes this challenging. Previous methods achieve\nreal-time performance by combining 3D Gaussian Splatting with a parametric head\nmodel, but the resulting head quality suffers from inaccurate face tracking and\nlimited expressiveness of the deformation model. These methods also fail to\nproduce realistic effects under novel lighting conditions. To address these\nissues, we propose HRAvatar, a 3DGS-based method that reconstructs\nhigh-fidelity, relightable 3D head avatars. HRAvatar reduces tracking errors\nthrough end-to-end optimization and better captures individual facial\ndeformations using learnable blendshapes and learnable linear blend skinning.\nAdditionally, it decomposes head appearance into several physical properties\nand incorporates physically-based shading to account for environmental\nlighting. Extensive experiments demonstrate that HRAvatar not only reconstructs\nsuperior-quality heads but also achieves realistic visual effects under varying\nlighting conditions.\n","authors":["Dongbin Zhang","Yunfei Liu","Lijian Lin","Ye Zhu","Kangjie Chen","Minghan Qin","Yu Li","Haoqian Wang"],"pdf_url":"https://arxiv.org/pdf/2503.08224v2.pdf","comment":"Accepted to CVPR 2025,Project page:\n  https://eastbeanzhang.github.io/HRAvatar"},{"id":"http://arxiv.org/abs/2504.14884v1","updated":"2025-04-21T06:29:04Z","published":"2025-04-21T06:29:04Z","title":"Memory-Augmented Dual-Decoder Networks for Multi-Class Unsupervised\n  Anomaly Detection","summary":"  Recent advances in unsupervised anomaly detection (UAD) have shifted from\nsingle-class to multi-class scenarios. In such complex contexts, the increasing\npattern diversity has brought two challenges to reconstruction-based\napproaches: (1) over-generalization: anomalies that are subtle or share\ncompositional similarities with normal patterns may be reconstructed with high\nfidelity, making them difficult to distinguish from normal instances; and (2)\ninsufficient normality reconstruction: complex normal features, such as\nintricate textures or fine-grained structures, may not be faithfully\nreconstructed due to the model's limited representational capacity, resulting\nin false positives. Existing methods typically focus on addressing the former,\nwhich unintentionally exacerbate the latter, resulting in inadequate\nrepresentation of intricate normal patterns. To concurrently address these two\nchallenges, we propose a Memory-augmented Dual-Decoder Networks (MDD-Net). This\nnetwork includes two critical components: a Dual-Decoder Reverse Distillation\nNetwork (DRD-Net) and a Class-aware Memory Module (CMM). Specifically, the\nDRD-Net incorporates a restoration decoder designed to recover normal features\nfrom synthetic abnormal inputs and an identity decoder to reconstruct features\nthat maintain the anomalous semantics. By exploiting the discrepancy between\nfeatures produced by two decoders, our approach refines anomaly scores beyond\nthe conventional encoder-decoder comparison paradigm, effectively reducing\nfalse positives and enhancing localization accuracy. Furthermore, the CMM\nexplicitly encodes and preserves class-specific normal prototypes, actively\nsteering the network away from anomaly reconstruction. Comprehensive\nexperimental results across several benchmarks demonstrate the superior\nperformance of our MDD-Net framework over current SoTA approaches in\nmulti-class UAD tasks.\n","authors":["Jingyu Xing","Chenwei Tang","Tao Wang","Rong Xiao","Wei Ju","Ji-Zhe Zhou","Liangli Zhen","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2504.14884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12235v4","updated":"2025-04-21T06:24:22Z","published":"2025-01-21T15:58:16Z","title":"DLEN: Dual Branch of Transformer for Low-Light Image Enhancement in Dual\n  Domains","summary":"  Low-light image enhancement (LLE) aims to improve the visual quality of\nimages captured in poorly lit conditions, which often suffer from low\nbrightness, low contrast, noise, and color distortions. These issues hinder the\nperformance of computer vision tasks such as object detection, facial\nrecognition, and autonomous driving.Traditional enhancement techniques, such as\nmulti-scale fusion and histogram equalization, fail to preserve fine details\nand often struggle with maintaining the natural appearance of enhanced images\nunder complex lighting conditions. Although the Retinex theory provides a\nfoundation for image decomposition, it often amplifies noise, leading to\nsuboptimal image quality. In this paper, we propose the Dual Light Enhance\nNetwork (DLEN), a novel architecture that incorporates two distinct attention\nmechanisms, considering both spatial and frequency domains. Our model\nintroduces a learnable wavelet transform module in the illumination estimation\nphase, preserving high- and low-frequency components to enhance edge and\ntexture details. Additionally, we design a dual-branch structure that leverages\nthe power of the Transformer architecture to enhance both the illumination and\nstructural components of the image.Through extensive experiments, our model\noutperforms state-of-the-art methods on standard benchmarks.Code is available\nhere: https://github.com/LaLaLoXX/DLEN\n","authors":["Junyu Xia","Jiesong Bai","Yihang Dong"],"pdf_url":"https://arxiv.org/pdf/2501.12235v4.pdf","comment":"9 pages and 6 figures"},{"id":"http://arxiv.org/abs/2504.14882v1","updated":"2025-04-21T06:20:50Z","published":"2025-04-21T06:20:50Z","title":"Some Optimizers are More Equal: Understanding the Role of Optimizers in\n  Group Fairness","summary":"  We study whether and how the choice of optimization algorithm can impact\ngroup fairness in deep neural networks. Through stochastic differential\nequation analysis of optimization dynamics in an analytically tractable setup,\nwe demonstrate that the choice of optimization algorithm indeed influences\nfairness outcomes, particularly under severe imbalance. Furthermore, we show\nthat when comparing two categories of optimizers, adaptive methods and\nstochastic methods, RMSProp (from the adaptive category) has a higher\nlikelihood of converging to fairer minima than SGD (from the stochastic\ncategory). Building on this insight, we derive two new theoretical guarantees\nshowing that, under appropriate conditions, RMSProp exhibits fairer parameter\nupdates and improved fairness in a single optimization step compared to SGD. We\nthen validate these findings through extensive experiments on three publicly\navailable datasets, namely CelebA, FairFace, and MS-COCO, across different\ntasks as facial expression recognition, gender classification, and multi-label\nclassification, using various backbones. Considering multiple fairness\ndefinitions including equalized odds, equal opportunity, and demographic\nparity, adaptive optimizers like RMSProp and Adam consistently outperform SGD\nin terms of group fairness, while maintaining comparable predictive accuracy.\nOur results highlight the role of adaptive updates as a crucial yet overlooked\nmechanism for promoting fair outcomes.\n","authors":["Mojtaba Kolahdouzi","Hatice Gunes","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2504.14882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18870v3","updated":"2025-04-21T06:15:33Z","published":"2024-12-25T11:07:04Z","title":"TSceneJAL: Joint Active Learning of Traffic Scenes for 3D Object\n  Detection","summary":"  Most autonomous driving (AD) datasets incur substantial costs for collection\nand labeling, inevitably yielding a plethora of low-quality and redundant data\ninstances, thereby compromising performance and efficiency. Many applications\nin AD systems necessitate high-quality training datasets using both existing\ndatasets and newly collected data. In this paper, we propose a traffic scene\njoint active learning (TSceneJAL) framework that can efficiently sample the\nbalanced, diverse, and complex traffic scenes from both labeled and unlabeled\ndata. The novelty of this framework is threefold: 1) a scene sampling scheme\nbased on a category entropy, to identify scenes containing multiple object\nclasses, thus mitigating class imbalance for the active learner; 2) a\nsimilarity sampling scheme, estimated through the directed graph representation\nand a marginalize kernel algorithm, to pick sparse and diverse scenes; 3) an\nuncertainty sampling scheme, predicted by a mixture density network, to select\ninstances with the most unclear or complex regression outcomes for the learner.\nFinally, the integration of these three schemes in a joint selection strategy\nyields an optimal and valuable subdataset. Experiments on the KITTI, Lyft,\nnuScenes and SUScape datasets demonstrate that our approach outperforms\nexisting state-of-the-art methods on 3D object detection tasks with up to 12%\nimprovements.\n","authors":["Chenyang Lei","Weiyuan Peng","Guang Zhou","Meiying Zhang","Qi Hao","Chunlin Ji","Chengzhong Xu"],"pdf_url":"https://arxiv.org/pdf/2412.18870v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14877v1","updated":"2025-04-21T06:07:32Z","published":"2025-04-21T06:07:32Z","title":"Collaborative Enhancement Network for Low-quality Multi-spectral Vehicle\n  Re-identification","summary":"  The performance of multi-spectral vehicle Re-identification (ReID) is\nsignificantly degraded when some important discriminative cues in visible, near\ninfrared and thermal infrared spectra are lost. Existing methods generate or\nenhance missing details in low-quality spectra data using the high-quality one,\ngenerally called the primary spectrum, but how to justify the primary spectrum\nis a challenging problem. In addition, when the quality of the primary spectrum\nis low, the enhancement effect would be greatly degraded, thus limiting the\nperformance of multi-spectral vehicle ReID. To address these problems, we\npropose the Collaborative Enhancement Network (CoEN), which generates a\nhigh-quality proxy from all spectra data and leverages it to supervise the\nselection of primary spectrum and enhance all spectra features in a\ncollaborative manner, for robust multi-spectral vehicle ReID. First, to\nintegrate the rich cues from all spectra data, we design the Proxy Generator\n(PG) to progressively aggregate multi-spectral features. Second, we design the\nDynamic Quality Sort Module (DQSM), which sorts all spectra data by measuring\ntheir correlations with the proxy, to accurately select the primary spectra\nwith the highest correlation. Finally, we design the Collaborative Enhancement\nModule (CEM) to effectively compensate for missing contents of all spectra by\ncollaborating the primary spectra and the proxy, thereby mitigating the impact\nof low-quality primary spectra. Extensive experiments on three benchmark\ndatasets are conducted to validate the efficacy of the proposed approach\nagainst other multi-spectral vehicle ReID methods. The codes will be released\nat https://github.com/yongqisun/CoEN.\n","authors":["Aihua Zheng","Yongqi Sun","Zi Wang","Chenglong Li","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2504.14877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14875v1","updated":"2025-04-21T06:02:03Z","published":"2025-04-21T06:02:03Z","title":"ReSpec: Relevance and Specificity Grounded Online Filtering for Learning\n  on Video-Text Data Streams","summary":"  The rapid growth of video-text data presents challenges in storage and\ncomputation during training. Online learning, which processes streaming data in\nreal-time, offers a promising solution to these issues while also allowing\nswift adaptations in scenarios demanding real-time responsiveness. One strategy\nto enhance the efficiency and effectiveness of learning involves identifying\nand prioritizing data that enhances performance on target downstream tasks. We\npropose Relevance and Specificity-based online filtering framework (ReSpec)\nthat selects data based on four criteria: (i) modality alignment for clean\ndata, (ii) task relevance for target focused data, (iii) specificity for\ninformative and detailed data, and (iv) efficiency for low-latency processing.\nRelevance is determined by the probabilistic alignment of incoming data with\ndownstream tasks, while specificity employs the distance to a root embedding\nrepresenting the least specific data as an efficient proxy for informativeness.\nBy establishing reference points from target task data, ReSpec filters incoming\ndata in real-time, eliminating the need for extensive storage and compute.\nEvaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains\nstate-of-the-art performance on five zeroshot video retrieval tasks, using as\nlittle as 5% of the data while incurring minimal compute. The source code is\navailable at https://github.com/cdjkim/ReSpec.\n","authors":["Chris Dongjoo Kim","Jihwan Moon","Sangwoo Moon","Heeseung Yun","Sihaeng Lee","Aniruddha Kembhavi","Soonyoung Lee","Gunhee Kim","Sangho Lee","Christopher Clark"],"pdf_url":"https://arxiv.org/pdf/2504.14875v1.pdf","comment":"CVPR 2025 (main conference)"},{"id":"http://arxiv.org/abs/2504.14868v1","updated":"2025-04-21T05:37:07Z","published":"2025-04-21T05:37:07Z","title":"Twin Co-Adaptive Dialogue for Progressive Image Generation","summary":"  Modern text-to-image generation systems have enabled the creation of\nremarkably realistic and high-quality visuals, yet they often falter when\nhandling the inherent ambiguities in user prompts. In this work, we present\nTwin-Co, a framework that leverages synchronized, co-adaptive dialogue to\nprogressively refine image generation. Instead of a static generation process,\nTwin-Co employs a dynamic, iterative workflow where an intelligent dialogue\nagent continuously interacts with the user. Initially, a base image is\ngenerated from the user's prompt. Then, through a series of synchronized\ndialogue exchanges, the system adapts and optimizes the image according to\nevolving user feedback. The co-adaptive process allows the system to\nprogressively narrow down ambiguities and better align with user intent.\nExperiments demonstrate that Twin-Co not only enhances user experience by\nreducing trial-and-error iterations but also improves the quality of the\ngenerated images, streamlining the creative process across various\napplications.\n","authors":["Jianhui Wang","Yangfan He","Yan Zhong","Xinyuan Song","Jiayi Su","Yuheng Feng","Hongyang He","Wenyu Zhu","Xinhang Yuan","Kuan Lu","Menghao Huo","Miao Zhang","Keqin Li","Jiaqi Chen","Tianyu Shi","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15167v5","updated":"2025-04-21T05:35:25Z","published":"2025-01-25T10:32:00Z","title":"Enhancing Intent Understanding for Ambiguous prompt: A Human-Machine\n  Co-Adaption Strategy","summary":"  Today's image generation systems are capable of producing realistic and\nhigh-quality images. However, user prompts often contain ambiguities, making it\ndifficult for these systems to interpret users' actual intentions.\nConsequently, many users must modify their prompts several times to ensure the\ngenerated images meet their expectations. While some methods focus on enhancing\nprompts to make the generated images fit user needs, the model is still hard to\nunderstand users' real needs, especially for non-expert users. In this\nresearch, we aim to enhance the visual parameter-tuning process, making the\nmodel user-friendly for individuals without specialized knowledge and better\nunderstand user needs. We propose a human-machine co-adaption strategy using\nmutual information between the user's prompts and the pictures under\nmodification as the optimizing target to make the system better adapt to user\nneeds. We find that an improved model can reduce the necessity for multiple\nrounds of adjustments. We also collect multi-round dialogue datasets with\nprompts and images pairs and user intent. Various experiments demonstrate the\neffectiveness of the proposed method in our proposed dataset. Our annotation\ntools and several examples of our dataset are available at\nhttps://zenodo.org/records/14876029 for easier review. We will make open source\nour full dataset and code.\n","authors":["Yangfan He","Jianhui Wang","Yijin Wang","Kun Li","Yan Zhong","Xinyuan Song","Li Sun","Jingyuan Lu","Miao Zhang","Tianyu Shi","Xinhang Yuan","Kuan Lu","Menghao Huo","Keqin Li","Jiaqi Chen"],"pdf_url":"https://arxiv.org/pdf/2501.15167v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09736v4","updated":"2025-04-21T05:17:26Z","published":"2024-01-18T05:31:53Z","title":"DirDist: A Metric for Comparing 3D Shapes Using Directional Distance\n  Fields","summary":"  Qualifying the discrepancy between 3D geometric models, which could be\nrepresented with either point clouds or triangle meshes, is a pivotal issue\nwith board applications. Existing methods mainly focus on directly establishing\nthe correspondence between two models and then aggregating point-wise distance\nbetween corresponding points, resulting in them being either inefficient or\nineffective. In this paper, we propose DirDist, an efficient, effective,\nrobust, and differentiable distance metric for 3D geometry data. Specifically,\nwe construct DirDist based on the proposed implicit representation of 3D\nmodels, namely directional distance field (DDF), which defines the directional\ndistances of 3D points to a model to capture its local surface geometry. We\nthen transfer the discrepancy between two 3D geometric models as the\ndiscrepancy between their DDFs defined on an identical domain, naturally\nestablishing model correspondence. To demonstrate the advantage of our DirDist,\nwe explore various distance metric-driven 3D geometric modeling tasks,\nincluding template surface fitting, rigid registration, non-rigid registration,\nscene flow estimation and human pose optimization. Extensive experiments show\nthat our DirDist achieves significantly higher accuracy under all tasks. As a\ngeneric distance metric, DirDist has the potential to advance the field of 3D\ngeometric modeling. The source code is available at\nhttps://github.com/rsy6318/DirDist.\n","authors":["Siyu Ren","Junhui Hou","Xiaodong Chen","Hongkai Xiong","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2401.09736v4.pdf","comment":"Accepted by T-PAMI"},{"id":"http://arxiv.org/abs/2502.12791v2","updated":"2025-04-21T05:17:15Z","published":"2025-02-18T11:52:25Z","title":"Activation-wise Propagation: A Universal Strategy to Break Timestep\n  Constraints in Spiking Neural Networks for 3D Data Processing","summary":"  Due to their event-driven and parameter-efficient effect, spiking neural\nnetworks (SNNs) show potential in tasks requiring real-time multi-sensor\nperception, such as autonomous driving. The spiking mechanism facilitates\nsparse encoding, enabling spatial and temporal data to be represented in a\ndiscrete manner. However, SNNs still lag behind artificial neural networks\n(ANNs) in terms of performance and computational efficiency. One major\nchallenge in SNNs is the timestep-wise iterative update of neuronal states,\nwhich makes it difficult to achieve an optimal trade-off among accuracy,\nlatency, and training cost. Although some methods perform well with shorter\ntimesteps, few propose strategies to overcome such constraint effectively.\nMoreover, many recent SNN advancements rely on either optimizations tailored to\nspecific architectures or a collection of specialized neuron-level strategies.\nWhile these approaches can enhance performance, they often lead to increased\ncomputational expense and restrict their application to particular\narchitectures or modalities. This leaves room for further exploration of\nsimple, universal, and structure-agnostic strategies that could offer broader\napplicability and efficiency. In this paper, we introduce Activation-wise\nMembrane Potential Propagation (AMP2), a novel state update mechanism for\nspiking neurons. Inspired by skip connections in deep networks, AMP2\nincorporates the membrane potential of neurons into network, eliminating the\nneed for iterative updates. Our method achieves significant improvements across\nvarious 3D modalities, including 3D point clouds and event streams, boosting\nSpiking PointNet's accuracy on ModelNet40 from 87.36% to 89.74% and surpassing\nANN PointNet in recognition accuracy on the DVS128 Gesture dataset.\n","authors":["Jian Song","Xiangfei Yang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04606v2","updated":"2025-04-21T05:14:31Z","published":"2025-01-08T16:41:31Z","title":"Enhancing Low-Cost Video Editing with Lightweight Adaptors and\n  Temporal-Aware Inversion","summary":"  Recent advancements in text-to-image (T2I) generation using diffusion models\nhave enabled cost-effective video-editing applications by leveraging\npre-trained models, eliminating the need for resource-intensive training.\nHowever, the frame-independence of T2I generation often results in poor\ntemporal consistency. Existing methods address this issue through temporal\nlayer fine-tuning or inference-based temporal propagation, but these approaches\nsuffer from high training costs or limited temporal coherence. To address these\nchallenges, we propose a General and Efficient Adapter (GE-Adapter) that\nintegrates temporal-spatial and semantic consistency with Baliteral DDIM\ninversion. This framework introduces three key components: (1) Frame-based\nTemporal Consistency Blocks (FTC Blocks) to capture frame-specific features and\nenforce smooth inter-frame transitions via temporally-aware loss functions; (2)\nChannel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral\nfilters to enhance spatial coherence by reducing noise and artifacts; and (3)\nToken-based Semantic Consistency Module (TSC Module) to maintain semantic\nalignment using shared prompt tokens and frame-specific tokens. Our method\nsignificantly improves perceptual quality, text-image alignment, and temporal\ncoherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves\nenhanced fidelity and frame-to-frame coherence, offering a practical solution\nfor T2V editing.\n","authors":["Yangfan He","Sida Li","Kun Li","Xinyuan Song","Xinhang Yuan","Keqin Li","Kuan Lu","Menghao Huo","Jiaqi Chen","Miao Zhang","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2501.04606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14860v1","updated":"2025-04-21T05:00:07Z","published":"2025-04-21T05:00:07Z","title":"Bridge the Gap: From Weak to Full Supervision for Temporal Action\n  Localization with PseudoFormer","summary":"  Weakly-supervised Temporal Action Localization (WTAL) has achieved notable\nsuccess but still suffers from a lack of temporal annotations, leading to a\nperformance and framework gap compared with fully-supervised methods. While\nrecent approaches employ pseudo labels for training, three key challenges:\ngenerating high-quality pseudo labels, making full use of different priors, and\noptimizing training methods with noisy labels remain unresolved. Due to these\nperspectives, we propose PseudoFormer, a novel two-branch framework that\nbridges the gap between weakly and fully-supervised Temporal Action\nLocalization (TAL). We first introduce RickerFusion, which maps all predicted\naction proposals to a global shared space to generate pseudo labels with better\nquality. Subsequently, we leverage both snippet-level and proposal-level labels\nwith different priors from the weak branch to train the regression-based model\nin the full branch. Finally, the uncertainty mask and iterative refinement\nmechanism are applied for training with noisy pseudo labels. PseudoFormer\nachieves state-of-the-art WTAL results on the two commonly used benchmarks,\nTHUMOS14 and ActivityNet1.3. Besides, extensive ablation studies demonstrate\nthe contribution of each component of our method.\n","authors":["Ziyi Liu","Yangcen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.14860v1.pdf","comment":"CVPR 2025: IEEE Conference on Computer Vision and Pattern Recognition"},{"id":"http://arxiv.org/abs/2503.12206v2","updated":"2025-04-21T04:44:14Z","published":"2025-03-15T17:11:41Z","title":"TLAC: Two-stage LMM Augmented CLIP for Zero-Shot Classification","summary":"  Contrastive Language-Image Pretraining (CLIP) has shown impressive zero-shot\nperformance on image classification. However, state-of-the-art methods often\nrely on fine-tuning techniques like prompt learning and adapter-based tuning to\noptimize CLIP's performance. The necessity for fine-tuning significantly limits\nCLIP's adaptability to novel datasets and domains. This requirement mandates\nsubstantial time and computational resources for each new dataset. To overcome\nthis limitation, we introduce simple yet effective training-free approaches,\nSingle-stage LMM Augmented CLIP (SLAC) and Two-stage LMM Augmented CLIP (TLAC),\nthat leverages powerful Large Multimodal Models (LMMs), such as Gemini, for\nimage classification. The proposed methods leverages the capabilities of\npre-trained LMMs, allowing for seamless adaptation to diverse datasets and\ndomains without the need for additional training. Our approaches involve\nprompting the LMM to identify objects within an image. Subsequently, the CLIP\ntext encoder determines the image class by identifying the dataset class with\nthe highest semantic similarity to the LLM predicted object. Our models\nachieved superior accuracy on 9 of 11 base-to-novel datasets, including\nImageNet, SUN397, and Caltech101, while maintaining a strictly training-free\nparadigm. Our TLAC model achieved an overall accuracy of 83.44%, surpassing the\nprevious state-of-the-art few-shot methods by a margin of 6.75%. Compared to\nother training-free approaches, our TLAC method achieved 83.6% average accuracy\nacross 13 datasets, a 9.7% improvement over the previous methods. Our Code is\navailable at https://github.com/ans92/TLAC\n","authors":["Ans Munir","Faisal Z. Qureshi","Muhammad Haris Khan","Mohsen Ali"],"pdf_url":"https://arxiv.org/pdf/2503.12206v2.pdf","comment":"Added code link in the abstract"},{"id":"http://arxiv.org/abs/2410.19245v2","updated":"2025-04-21T04:40:38Z","published":"2024-10-25T01:52:15Z","title":"MaCTG: Multi-Agent Collaborative Thought Graph for Automatic Programming","summary":"  With the rapid advancement of Large Language Models (LLMs), LLM-based\napproaches have demonstrated strong problem-solving capabilities across various\ndomains. However, in automatic programming, a single LLM is typically limited\nto function-level code generation, while multi-agent systems composed of\nmultiple LLMs often suffer from inefficient task planning. This lack of\nstructured coordination can lead to cascading hallucinations, where accumulated\nerrors across agents result in suboptimal workflows and excessive computational\ncosts. To overcome these challenges, we introduce MaCTG (Multi-Agent\nCollaborative Thought Graph), a novel multi-agent framework that employs a\ndynamic graph structure to facilitate precise task allocation and controlled\ncollaboration among LLM agents. MaCTG autonomously assigns agent roles based on\nprogramming requirements, dynamically refines task distribution through\ncontext-aware adjustments, and systematically verifies and integrates\nproject-level code, effectively reducing hallucination errors and improving\noverall accuracy. MaCTG enhances cost-effectiveness by implementing a hybrid\nLLM deployment, where proprietary models handle complex reasoning, while\nopen-source models are used for routine coding and validation tasks. To\nevaluate MaCTG's effectiveness, we applied it to traditional image processing\nauto-programming tasks, achieving a state-of-the-art accuracy of 83.33%.\nAdditionally, by leveraging its hybrid LLM configuration, MaCTG significantly\nreduced operational costs by 89.09% compared to existing multi-agent\nframeworks, demonstrating its efficiency, scalability, and real-world\napplicability.\n","authors":["Zixiao Zhao","Jing Sun","Zhe Hou","Zhiyuan Wei","Cheng-Hao Cai","Miao Qiao","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2410.19245v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19204v2","updated":"2025-04-21T04:18:34Z","published":"2025-02-26T15:10:05Z","title":"Distill Any Depth: Distillation Creates a Stronger Monocular Depth\n  Estimator","summary":"  Recent advances in zero-shot monocular depth estimation(MDE) have\nsignificantly improved generalization by unifying depth distributions through\nnormalized depth representations and by leveraging large-scale unlabeled data\nvia pseudo-label distillation. However, existing methods that rely on global\ndepth normalization treat all depth values equally, which can amplify noise in\npseudo-labels and reduce distillation effectiveness. In this paper, we present\na systematic analysis of depth normalization strategies in the context of\npseudo-label distillation. Our study shows that, under recent distillation\nparadigms (e.g., shared-context distillation), normalization is not always\nnecessary, as omitting it can help mitigate the impact of noisy supervision.\nFurthermore, rather than focusing solely on how depth information is\nrepresented, we propose Cross-Context Distillation, which integrates both\nglobal and local depth cues to enhance pseudo-label quality. We also introduce\nan assistant-guided distillation strategy that incorporates complementary depth\npriors from a diffusion-based teacher model, enhancing supervision diversity\nand robustness. Extensive experiments on benchmark datasets demonstrate that\nour approach significantly outperforms state-of-the-art methods, both\nquantitatively and qualitatively.\n","authors":["Xiankang He","Dongyan Guo","Hongji Li","Ruibo Li","Ying Cui","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.19204v2.pdf","comment":"project page: https://distill-any-depth-official.github.io/"},{"id":"http://arxiv.org/abs/2410.14148v4","updated":"2025-04-21T04:04:53Z","published":"2024-10-18T03:34:32Z","title":"Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in\n  Vision-Language Alignment","summary":"  The recent advancements in large language models (LLMs) and pre-trained\nvision models have accelerated the development of vision-language large models\n(VLLMs), enhancing the interaction between visual and linguistic modalities.\nDespite their notable success across various domains, VLLMs face challenges in\nmodality alignment, which can lead to issues like hallucinations and unsafe\ncontent generation. Current alignment techniques often rely on coarse feedback\nand external datasets, limiting scalability and performance. In this paper, we\npropose FiSAO (Fine-Grained Self-Alignment Optimization), a novel\nself-alignment method that utilizes the model's own visual encoder as a\nfine-grained verifier to improve vision-language alignment without the need for\nadditional data. By leveraging token-level feedback from the vision encoder,\nFiSAO significantly improves vision-language alignment, even surpassing\ntraditional preference tuning methods that require additional data. Through\nboth theoretical analysis and experimental validation, we demonstrate that\nFiSAO effectively addresses the misalignment problem in VLLMs, marking the\nfirst instance of token-level rewards being applied to such models.\n","authors":["Chenhang Cui","An Zhang","Yiyang Zhou","Zhaorun Chen","Gelei Deng","Huaxiu Yao","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.14148v4.pdf","comment":"23 pages; Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.14848v1","updated":"2025-04-21T04:01:22Z","published":"2025-04-21T04:01:22Z","title":"Object-Level Verbalized Confidence Calibration in Vision-Language Models\n  via Semantic Perturbation","summary":"  Vision-language models (VLMs) excel in various multimodal tasks but\nfrequently suffer from poor calibration, resulting in misalignment between\ntheir verbalized confidence and response correctness. This miscalibration\nundermines user trust, especially when models confidently provide incorrect or\nfabricated information. In this work, we propose a novel Confidence Calibration\nthrough Semantic Perturbation (CSP) framework to improve the calibration of\nverbalized confidence for VLMs in response to object-centric queries. We first\nintroduce a perturbed dataset where Gaussian noise is applied to the key object\nregions to simulate visual uncertainty at different confidence levels,\nestablishing an explicit mapping between visual ambiguity and confidence\nlevels. We further enhance calibration through a two-stage training process\ncombining supervised fine-tuning on the perturbed dataset with subsequent\npreference optimization. Extensive experiments on popular benchmarks\ndemonstrate that our method significantly improves the alignment between\nverbalized confidence and response correctness while maintaining or enhancing\noverall task performance. These results highlight the potential of semantic\nperturbation as a practical tool for improving the reliability and\ninterpretability of VLMs.\n","authors":["Yunpu Zhao","Rui Zhang","Junbin Xiao","Ruibo Hou","Jiaming Guo","Zihao Zhang","Yifan Hao","Yunji Chen"],"pdf_url":"https://arxiv.org/pdf/2504.14848v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01035v4","updated":"2025-04-21T03:58:50Z","published":"2024-09-02T08:10:51Z","title":"Task-Specific Directions: Definition, Exploration, and Utilization in\n  Parameter Efficient Fine-Tuning","summary":"  Large language models demonstrate impressive performance on downstream tasks,\nyet they require extensive resource consumption when fully fine-tuning all\nparameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT)\nstrategies, such as LoRA, have been developed. In this paper, we delve into the\nconcept of task-specific directions (TSDs), which are critical for\ntransitioning large models from pretrained states to task-specific enhancements\nin PEFT. We propose a framework to clearly define these directions and explore\ntheir properties and practical utilization challenges. We then introduce a\nnovel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the\nfine-tuning process, thereby enhancing model performance on targeted tasks.\nAdditionally, based on our exploration of TSD, we focus on an important issue\nin PEFT: the initialization of LoRA. While some works have pointed out the\nsignificance of initialization for LoRA's performance and proposed various\nstrategies, these methods are often empirical and not task-specific. To address\nthis issue, we propose LoRA-Init. Starting from TSD, we identify the directions\nthat require the most adjustment during fine-tuning for downstream tasks. By\ninitializing the matrices in LoRA with these directions, LoRA-Init\nsignificantly enhances LoRA's performance. Moreover, we can combine LoRA-Dash\nand LoRA-Init to create the final version of LoRA based on TSDs, which we refer\nto as LoRA-TSD. Extensive experiments have conclusively demonstrated the\neffectiveness of these methods, and in-depth analyses further reveal the\nunderlying mechanisms behind their success.\n","authors":["Chongjie Si","Zhiyi Shi","Shifan Zhang","Xiaokang Yang","Hanspeter Pfister","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2409.01035v4.pdf","comment":"Codes in https://github.com/Chongjie-Si/Subspace-Tuning"},{"id":"http://arxiv.org/abs/2504.14847v1","updated":"2025-04-21T03:58:40Z","published":"2025-04-21T03:58:40Z","title":"Reliable Multi-Modal Object Re-Identification via Modality-Aware Graph\n  Reasoning","summary":"  Multi-modal data provides abundant and diverse object information, crucial\nfor effective modal interactions in Re-Identification (ReID) tasks. However,\nexisting approaches often overlook the quality variations in local features and\nfail to fully leverage the complementary information across modalities,\nparticularly in the case of low-quality features. In this paper, we propose to\naddress this issue by leveraging a novel graph reasoning model, termed the\nModality-aware Graph Reasoning Network (MGRNet). Specifically, we first\nconstruct modality-aware graphs to enhance the extraction of fine-grained local\ndetails by effectively capturing and modeling the relationships between\npatches. Subsequently, the selective graph nodes swap operation is employed to\nalleviate the adverse effects of low-quality local features by considering both\nlocal and global information, enhancing the representation of discriminative\ninformation. Finally, the swapped modality-aware graphs are fed into the\nlocal-aware graph reasoning module, which propagates multi-modal information to\nyield a reliable feature representation. Another advantage of the proposed\ngraph reasoning approach is its ability to reconstruct missing modal\ninformation by exploiting inherent structural relationships, thereby minimizing\ndisparities between different modalities. Experimental results on four\nbenchmarks (RGBNT201, Market1501-MM, RGBNT100, MSVR310) indicate that the\nproposed method achieves state-of-the-art performance in multi-modal object\nReID. The code for our method will be available upon acceptance.\n","authors":["Xixi Wan","Aihua Zheng","Zi Wang","Bo Jiang","Jin Tang","Jixin Ma"],"pdf_url":"https://arxiv.org/pdf/2504.14847v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.02287v2","updated":"2025-04-21T03:46:22Z","published":"2025-04-03T05:23:08Z","title":"MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action\n  Recognition and Transformer-based Sensor Fusion","summary":"  Multi-modal multi-view action recognition is a rapidly growing field in\ncomputer vision, offering significant potential for applications in\nsurveillance. However, current datasets often fail to address real-world\nchallenges such as wide-area distributed settings, asynchronous data streams,\nand the lack of frame-level annotations. Furthermore, existing methods face\ndifficulties in effectively modeling inter-view relationships and enhancing\nspatial feature learning. In this paper, we introduce the MultiSensor-Home\ndataset, a novel benchmark designed for comprehensive action recognition in\nhome environments, and also propose the Multi-modal Multi-view\nTransformer-based Sensor Fusion (MultiTSF) method. The proposed\nMultiSensor-Home dataset features untrimmed videos captured by distributed\nsensors, providing high-resolution RGB and audio data along with detailed\nmulti-view frame-level action labels. The proposed MultiTSF method leverages a\nTransformer-based fusion mechanism to dynamically model inter-view\nrelationships. Furthermore, the proposed method integrates a human detection\nmodule to enhance spatial feature learning, guiding the model to prioritize\nframes with human activity to enhance action the recognition accuracy.\nExperiments on the proposed MultiSensor-Home and the existing MM-Office\ndatasets demonstrate the superiority of MultiTSF over the state-of-the-art\nmethods. Quantitative and qualitative results highlight the effectiveness of\nthe proposed method in advancing real-world multi-modal multi-view action\nrecognition.\n","authors":["Trung Thanh Nguyen","Yasutomo Kawanishi","Vijay John","Takahiro Komamizu","Ichiro Ide"],"pdf_url":"https://arxiv.org/pdf/2504.02287v2.pdf","comment":"The 19th IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG 2025)"},{"id":"http://arxiv.org/abs/2504.13432v2","updated":"2025-04-21T03:40:58Z","published":"2025-04-18T03:07:25Z","title":"Circular Image Deturbulence using Quasi-conformal Geometry","summary":"  The presence of inhomogeneous media between optical sensors and objects leads\nto distorted imaging outputs, significantly complicating downstream\nimage-processing tasks. A key challenge in image restoration is the lack of\nhigh-quality, paired-label images required for training supervised models. In\nthis paper, we introduce the Circular Quasi-Conformal Deturbulence (CQCD)\nframework, an unsupervised approach for removing image distortions through a\ncircular architecture. This design ensures that the restored image remains both\ngeometrically accurate and visually faithful while preventing the accumulation\nof incorrect estimations. The circular restoration process involves both\nforward and inverse mapping. To ensure the bijectivity of the estimated\nnon-rigid deformations, computational quasi-conformal geometry theories are\nleveraged to regularize the mapping, enforcing its homeomorphic properties.\nThis guarantees a well-defined transformation that preserves structural\nintegrity and prevents unwanted artifacts. Furthermore, tight-frame blocks are\nintegrated to encode distortion-sensitive features for precise recovery. To\nvalidate the performance of our approach, we conduct evaluations on various\nsynthetic and real-world captured images. Experimental results demonstrate that\nCQCD not only outperforms existing state-of-the-art deturbulence methods in\nterms of image restoration quality but also provides highly accurate\ndeformation field estimations.\n","authors":["Chu Chen","Han Zhang","Lok Ming Lui"],"pdf_url":"https://arxiv.org/pdf/2504.13432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10148v2","updated":"2025-04-21T03:29:53Z","published":"2025-04-14T11:59:58Z","title":"Hierarchical and Step-Layer-Wise Tuning of Attention Specialty for\n  Multi-Instance Synthesis in Diffusion Transformers","summary":"  Text-to-image (T2I) generation models often struggle with multi-instance\nsynthesis (MIS), where they must accurately depict multiple distinct instances\nin a single image based on complex prompts detailing individual features.\nTraditional MIS control methods for UNet architectures like SD v1.5/SDXL fail\nto adapt to DiT-based models like FLUX and SD v3.5, which rely on integrated\nattention between image and text tokens rather than text-image cross-attention.\nTo enhance MIS in DiT, we first analyze the mixed attention mechanism in DiT.\nOur token-wise and layer-wise analysis of attention maps reveals a hierarchical\nresponse structure: instance tokens dominate early layers, background tokens in\nmiddle layers, and attribute tokens in later layers. Building on this\nobservation, we propose a training-free approach for enhancing MIS in DiT-based\nmodels with hierarchical and step-layer-wise attention specialty tuning (AST).\nAST amplifies key regions while suppressing irrelevant areas in distinct\nattention maps across layers and steps, guided by the hierarchical structure.\nThis optimizes multimodal interactions by hierarchically decoupling the complex\nprompts with instance-based sketches. We evaluate our approach using upgraded\nsketch-based layouts for the T2I-CompBench and customized complex scenes. Both\nquantitative and qualitative results confirm our method enhances complex layout\ngeneration, ensuring precise instance placement and attribute representation in\nMIS.\n","authors":["Chunyang Zhang","Zhenhong Sun","Zhicheng Zhang","Junyan Wang","Yu Zhang","Dong Gong","Huadong Mo","Daoyi Dong"],"pdf_url":"https://arxiv.org/pdf/2504.10148v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14826v1","updated":"2025-04-21T03:00:18Z","published":"2025-04-21T03:00:18Z","title":"Distribution-aware Dataset Distillation for Efficient Image Restoration","summary":"  With the exponential increase in image data, training an image restoration\nmodel is laborious. Dataset distillation is a potential solution to this\nproblem, yet current distillation techniques are a blank canvas in the field of\nimage restoration. To fill this gap, we propose the Distribution-aware Dataset\nDistillation method (TripleD), a new framework that extends the principles of\ndataset distillation to image restoration. Specifically, TripleD uses a\npre-trained vision Transformer to extract features from images for complexity\nevaluation, and the subset (the number of samples is much smaller than the\noriginal training set) is selected based on complexity. The selected subset is\nthen fed through a lightweight CNN that fine-tunes the image distribution to\nalign with the distribution of the original dataset at the feature level. To\nefficiently condense knowledge, the training is divided into two stages. Early\nstages focus on simpler, low-complexity samples to build foundational\nknowledge, while later stages select more complex and uncertain samples as the\nmodel matures. Our method achieves promising performance on multiple image\nrestoration tasks, including multi-task image restoration, all-in-one image\nrestoration, and ultra-high-definition image restoration tasks. Note that we\ncan train a state-of-the-art image restoration model on an\nultra-high-definition (4K resolution) dataset using only one consumer-grade GPU\nin less than 8 hours (500 savings in computing resources and immeasurable\ntraining time).\n","authors":["Zhuoran Zheng","Xin Su","Chen Wu","Xiuyi Jia"],"pdf_url":"https://arxiv.org/pdf/2504.14826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14825v1","updated":"2025-04-21T03:00:17Z","published":"2025-04-21T03:00:17Z","title":"ECViT: Efficient Convolutional Vision Transformer with Local-Attention\n  and Multi-scale Stages","summary":"  Vision Transformers (ViTs) have revolutionized computer vision by leveraging\nself-attention to model long-range dependencies. However, ViTs face challenges\nsuch as high computational costs due to the quadratic scaling of self-attention\nand the requirement of a large amount of training data. To address these\nlimitations, we propose the Efficient Convolutional Vision Transformer (ECViT),\na hybrid architecture that effectively combines the strengths of CNNs and\nTransformers. ECViT introduces inductive biases such as locality and\ntranslation invariance, inherent to Convolutional Neural Networks (CNNs) into\nthe Transformer framework by extracting patches from low-level features and\nenhancing the encoder with convolutional operations. Additionally, it\nincorporates local-attention and a pyramid structure to enable efficient\nmulti-scale feature extraction and representation. Experimental results\ndemonstrate that ECViT achieves an optimal balance between performance and\nefficiency, outperforming state-of-the-art models on various image\nclassification tasks while maintaining low computational and storage\nrequirements. ECViT offers an ideal solution for applications that prioritize\nhigh efficiency without compromising performance.\n","authors":["Zhoujie Qian"],"pdf_url":"https://arxiv.org/pdf/2504.14825v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14815v1","updated":"2025-04-21T02:44:59Z","published":"2025-04-21T02:44:59Z","title":"What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale","summary":"  Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing.\n","authors":["Xiaoyong Yuan","Xiaolong Ma","Linke Guo","Lan Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.14815v1.pdf","comment":"17 pages, 15 figures"},{"id":"http://arxiv.org/abs/2504.00387v2","updated":"2025-04-21T02:40:21Z","published":"2025-04-01T03:17:24Z","title":"Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single\n  Panoramic Image for Your Immerse Exploration","summary":"  The reconstruction of immersive and realistic 3D scenes holds significant\npractical importance in various fields of computer vision and computer\ngraphics. Typically, immersive and realistic scenes should be free from\nobstructions by dynamic objects, maintain global texture consistency, and allow\nfor unrestricted exploration. The current mainstream methods for image-driven\nscene construction involves iteratively refining the initial image using a\nmoving virtual camera to generate the scene. However, previous methods struggle\nwith visual discontinuities due to global texture inconsistencies under varying\ncamera poses, and they frequently exhibit scene voids caused by\nforeground-background occlusions. To this end, we propose a novel layered 3D\nscene reconstruction framework from panoramic image, named Scene4U.\nSpecifically, Scene4U integrates an open-vocabulary segmentation model with a\nlarge language model to decompose a real panorama into multiple layers. Then,\nwe employs a layered repair module based on diffusion model to restore occluded\nregions using visual cues and depth information, generating a hierarchical\nrepresentation of the scene. The multi-layer panorama is then initialized as a\n3D Gaussian Splatting representation, followed by layered optimization, which\nultimately produces an immersive 3D scene with semantic and structural\nconsistency that supports free exploration. Scene4U outperforms\nstate-of-the-art method, improving by 24.24% in LPIPS and 24.40% in BRISQUE,\nwhile also achieving the fastest training speed. Additionally, to demonstrate\nthe robustness of Scene4U and allow users to experience immersive scenes from\nvarious landmarks, we build WorldVista3D dataset for 3D scene reconstruction,\nwhich contains panoramic images of globally renowned sites. The implementation\ncode and dataset will be released at https://github.com/LongHZ140516/Scene4U .\n","authors":["Zilong Huang","Jun He","Junyan Ye","Lihan Jiang","Weijia Li","Yiping Chen","Ting Han"],"pdf_url":"https://arxiv.org/pdf/2504.00387v2.pdf","comment":"CVPR 2025, 11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.09732v2","updated":"2025-04-21T02:36:09Z","published":"2024-10-13T05:26:36Z","title":"LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large\n  Multimodal Models","summary":"  With the rapid development of AI-generated content, the future internet may\nbe inundated with synthetic data, making the discrimination of authentic and\ncredible multimodal data increasingly challenging. Synthetic data detection has\nthus garnered widespread attention, and the performance of large multimodal\nmodels (LMMs) in this task has attracted significant interest. LMMs can provide\nnatural language explanations for their authenticity judgments, enhancing the\nexplainability of synthetic content detection. Simultaneously, the task of\ndistinguishing between real and synthetic data effectively tests the\nperception, knowledge, and reasoning capabilities of LMMs. In response, we\nintroduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to\ndetect synthetic data across multiple modalities. LOKI encompasses video,\nimage, 3D, text, and audio modalities, comprising 18K carefully curated\nquestions across 26 subcategories with clear difficulty levels. The benchmark\nincludes coarse-grained judgment and multiple-choice questions, as well as\nfine-grained anomaly selection and explanation tasks, allowing for a\ncomprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6\nclosed-source models on LOKI, highlighting their potential as synthetic data\ndetectors and also revealing some limitations in the development of LMM\ncapabilities. More information about LOKI can be found at\nhttps://opendatalab.github.io/LOKI/\n","authors":["Junyan Ye","Baichuan Zhou","Zilong Huang","Junan Zhang","Tianyi Bai","Hengrui Kang","Jun He","Honglin Lin","Zihao Wang","Tong Wu","Zhizheng Wu","Yiping Chen","Dahua Lin","Conghui He","Weijia Li"],"pdf_url":"https://arxiv.org/pdf/2410.09732v2.pdf","comment":"ICLR 2025 SPOTLIGHT, 83 pages, 63 figures"},{"id":"http://arxiv.org/abs/2504.14807v1","updated":"2025-04-21T02:15:37Z","published":"2025-04-21T02:15:37Z","title":"Real-Time Sleepiness Detection for Driver State Monitoring System","summary":"  A driver face monitoring system can detect driver fatigue, which is a\nsignificant factor in many accidents, using computer vision techniques. In this\npaper, we present a real-time technique for driver eye state detection. First,\nthe face is detected, and the eyes are located within the face region for\ntracking. A normalized cross-correlation-based online dynamic template matching\ntechnique, combined with Kalman filter tracking, is proposed to track the\ndetected eye positions in subsequent image frames. A support vector machine\nwith histogram of oriented gradients (HOG) features is used to classify the\nstate of the eyes as open or closed. If the eyes remain closed for a specified\nperiod, the driver is considered to be asleep, and an alarm is triggered.\n","authors":["Deepak Ghimire","Sunghwan Jeong","Sunhong Yoon","Sanghyun Park","Juhwan Choi"],"pdf_url":"https://arxiv.org/pdf/2504.14807v1.pdf","comment":"8 pages, published in GST 2015"},{"id":"http://arxiv.org/abs/2504.13406v2","updated":"2025-04-21T02:00:43Z","published":"2025-04-18T02:03:14Z","title":"LangCoop: Collaborative Driving with Language","summary":"  Multi-agent collaboration holds great promise for enhancing the safety,\nreliability, and mobility of autonomous driving systems by enabling information\nsharing among multiple connected agents. However, existing multi-agent\ncommunication approaches are hindered by limitations of existing communication\nmedia, including high bandwidth demands, agent heterogeneity, and information\nloss. To address these challenges, we introduce LangCoop, a new paradigm for\ncollaborative autonomous driving that leverages natural language as a compact\nyet expressive medium for inter-agent communication. LangCoop features two key\ninnovations: Mixture Model Modular Chain-of-thought (M$^3$CoT) for structured\nzero-shot vision-language reasoning and Natural Language Information Packaging\n(LangPack) for efficiently packaging information into concise, language-based\nmessages. Through extensive experiments conducted in the CARLA simulations, we\ndemonstrate that LangCoop achieves a remarkable 96\\% reduction in communication\nbandwidth (< 2KB per message) compared to image-based communication, while\nmaintaining competitive driving performance in the closed-loop evaluation. Our\nproject page and code are at https://xiangbogaobarry.github.io/LangCoop/.\n","authors":["Xiangbo Gao","Yuheng Wu","Rujia Wang","Chenxi Liu","Yang Zhou","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2504.13406v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14800v1","updated":"2025-04-21T01:58:29Z","published":"2025-04-21T01:58:29Z","title":"A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis,\n  and Solutions","summary":"  The small sample imbalance (S&I) problem is a major challenge in machine\nlearning and data analysis. It is characterized by a small number of samples\nand an imbalanced class distribution, which leads to poor model performance. In\naddition, indistinct inter-class feature distributions further complicate\nclassification tasks. Existing methods often rely on algorithmic heuristics\nwithout sufficiently analyzing the underlying data characteristics. We argue\nthat a detailed analysis from the data perspective is essential before\ndeveloping an appropriate solution. Therefore, this paper proposes a systematic\nanalytical framework for the S\\&I problem. We first summarize imbalance metrics\nand complexity analysis methods, highlighting the need for interpretable\nbenchmarks to characterize S&I problems. Second, we review recent solutions for\nconventional, complexity-based, and extreme S&I problems, revealing\nmethodological differences in handling various data distributions. Our summary\nfinds that resampling remains a widely adopted solution. However, we conduct\nexperiments on binary and multiclass datasets, revealing that classifier\nperformance differences significantly exceed the improvements achieved through\nresampling. Finally, this paper highlights open questions and discusses future\ntrends.\n","authors":["Shuxian Zhao","Jie Gui","Minjing Dong","Baosheng Yu","Zhipeng Gui","Lu Dong","Yuan Yan Tang","James Tin-Yau Kwok"],"pdf_url":"https://arxiv.org/pdf/2504.14800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14798v1","updated":"2025-04-21T01:56:15Z","published":"2025-04-21T01:56:15Z","title":"Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned\n  Models","summary":"  Machine Unlearning (MUL) is crucial for privacy protection and content\nregulation, yet recent studies reveal that traces of forgotten information\npersist in unlearned models, enabling adversaries to resurface removed\nknowledge. Existing verification methods only confirm whether unlearning was\nexecuted, failing to detect such residual information leaks. To address this,\nwe introduce the concept of Robust Unlearning, ensuring models are\nindistinguishable from retraining and resistant to adversarial recovery. To\nempirically evaluate whether unlearning techniques meet this security standard,\nwe propose the Unlearning Mapping Attack (UMA), a post-unlearning verification\nframework that actively probes models for forgotten traces using adversarial\nqueries. Extensive experiments on discriminative and generative tasks show that\nexisting unlearning techniques remain vulnerable, even when passing existing\nverification metrics. By establishing UMA as a practical verification tool,\nthis study sets a new standard for assessing and enhancing machine unlearning\nsecurity.\n","authors":["Hao Xuan","Xingyu Li"],"pdf_url":"https://arxiv.org/pdf/2504.14798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12761v2","updated":"2025-04-21T01:50:29Z","published":"2025-01-22T09:54:43Z","title":"Modality Unified Attack for Omni-Modality Person Re-Identification","summary":"  Deep learning based person re-identification (re-id) models have been widely\nemployed in surveillance systems. Recent studies have demonstrated that\nblack-box single-modality and cross-modality re-id models are vulnerable to\nadversarial examples (AEs), leaving the robustness of multi-modality re-id\nmodels unexplored. Due to the lack of knowledge about the specific type of\nmodel deployed in the target black-box surveillance system, we aim to generate\nmodality unified AEs for omni-modality (single-, cross- and multi-modality)\nre-id models. Specifically, we propose a novel Modality Unified Attack method\nto train modality-specific adversarial generators to generate AEs that\neffectively attack different omni-modality models. A multi-modality model is\nadopted as the surrogate model, wherein the features of each modality are\nperturbed by metric disruption loss before fusion. To collapse the common\nfeatures of omni-modality models, Cross Modality Simulated Disruption approach\nis introduced to mimic the cross-modality feature embeddings by intentionally\nfeeding images to non-corresponding modality-specific subnetworks of the\nsurrogate model. Moreover, Multi Modality Collaborative Disruption strategy is\ndevised to facilitate the attacker to comprehensively corrupt the informative\ncontent of person images by leveraging a multi modality feature collaborative\nmetric disruption loss. Extensive experiments show that our MUA method can\neffectively attack the omni-modality re-id models, achieving 55.9%, 24.4%,\n49.0% and 62.7% mean mAP Drop Rate, respectively.\n","authors":["Yuan Bian","Min Liu","Yunqi Yi","Xueping Wang","Yunfeng Ma","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2501.12761v2.pdf","comment":"9 pages,3 figures"},{"id":"http://arxiv.org/abs/2504.14795v1","updated":"2025-04-21T01:50:10Z","published":"2025-04-21T01:50:10Z","title":"Segmentation with Noisy Labels via Spatially Correlated Distributions","summary":"  In semantic segmentation, the accuracy of models heavily depends on the\nhigh-quality annotations. However, in many practical scenarios such as medical\nimaging and remote sensing, obtaining true annotations is not straightforward\nand usually requires significant human labor. Relying on human labor often\nintroduces annotation errors, including mislabeling, omissions, and\ninconsistency between annotators. In the case of remote sensing, differences in\nprocurement time can lead to misaligned ground truth annotations. These label\nerrors are not independently distributed, and instead usually appear in\nspatially connected regions where adjacent pixels are more likely to share the\nsame errors. To address these issues, we propose an approximate Bayesian\nestimation based on a probabilistic model that assumes training data includes\nlabel errors, incorporating the tendency for these errors to occur with spatial\ncorrelations between adjacent pixels. Bayesian inference requires computing the\nposterior distribution of label errors, which becomes intractable when spatial\ncorrelations are present. We represent the correlation of label errors between\nadjacent pixels through a Gaussian distribution whose covariance is structured\nby a Kac-Murdock-Szeg\\\"{o} (KMS) matrix, solving the computational challenges.\nThrough experiments on multiple segmentation tasks, we confirm that leveraging\nthe spatial correlation of label errors significantly improves performance.\nNotably, in specific tasks such as lung segmentation, the proposed method\nachieves performance comparable to training with clean labels under moderate\nnoise levels. Code is available at\nhttps://github.com/pfnet-research/Bayesian_SpatialCorr.\n","authors":["Ryu Tadokoro","Tsukasa Takagi","Shin-ichi Maeda"],"pdf_url":"https://arxiv.org/pdf/2504.14795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12919v2","updated":"2025-04-21T01:06:56Z","published":"2024-11-19T23:17:09Z","title":"Robust multi-coil MRI reconstruction via self-supervised denoising","summary":"  To examine the effect of incorporating self-supervised denoising as a\npre-processing step for training deep learning (DL) based reconstruction\nmethods on data corrupted by Gaussian noise. K-space data employed for training\nare typically multi-coil and inherently noisy. Although DL-based reconstruction\nmethods trained on fully sampled data can enable high reconstruction quality,\nobtaining large, noise-free datasets is impractical. We leverage Generalized\nStein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based\nreconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based\nDeep Learning (MoDL). We evaluate the impact of denoising on the performance of\nthese DL-based methods in solving accelerated multi-coil magnetic resonance\nimaging (MRI) reconstruction. The experiments were carried out on T2-weighted\nbrain and fat-suppressed proton-density knee scans. We observed that\nself-supervised denoising enhances the quality and efficiency of MRI\nreconstructions across various scenarios. Specifically, employing denoised\nimages rather than noisy counterparts when training DL networks results in\nlower normalized root mean squared error (NRMSE), higher structural similarity\nindex measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR\nlevels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB,\n14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising\nis an essential pre-processing technique capable of improving the efficacy of\nDL-based MRI reconstruction methods under diverse conditions. By refining the\nquality of input data, denoising enables training more effective DL networks,\npotentially bypassing the need for noise-free reference MRI scans.\n","authors":["Asad Aali","Marius Arvinte","Sidharth Kumar","Yamin I. Arefeen","Jonathan I. Tamir"],"pdf_url":"https://arxiv.org/pdf/2411.12919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12515v2","updated":"2025-04-21T01:04:58Z","published":"2025-04-16T22:25:57Z","title":"Event Quality Score (EQS): Assessing the Realism of Simulated Event\n  Camera Streams via Distances in Latent Space","summary":"  Event cameras promise a paradigm shift in vision sensing with their low\nlatency, high dynamic range, and asynchronous nature of events. Unfortunately,\nthe scarcity of high-quality labeled datasets hinders their widespread adoption\nin deep learning-driven computer vision. To mitigate this, several simulators\nhave been proposed to generate synthetic event data for training models for\ndetection and estimation tasks. However, the fundamentally different sensor\ndesign of event cameras compared to traditional frame-based cameras poses a\nchallenge for accurate simulation. As a result, most simulated data fail to\nmimic data captured by real event cameras. Inspired by existing work on using\ndeep features for image comparison, we introduce event quality score (EQS), a\nquality metric that utilizes activations of the RVT architecture. Through\nsim-to-real experiments on the DSEC driving dataset, it is shown that a higher\nEQS implies improved generalization to real-world data after training on\nsimulated events. Thus, optimizing for EQS can lead to developing more\nrealistic event camera simulators, effectively reducing the simulation gap. EQS\nis available at https://github.com/eventbasedvision/EQS.\n","authors":["Kaustav Chanda","Aayush Atul Verma","Arpitsinh Vaghela","Yezhou Yang","Bharatesh Chakravarthi"],"pdf_url":"https://arxiv.org/pdf/2504.12515v2.pdf","comment":"Accepted at 2025 IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition Workshops (CVPRW); Fifth International Workshop on Event-Based\n  Vision"},{"id":"http://arxiv.org/abs/2504.14785v1","updated":"2025-04-21T00:56:57Z","published":"2025-04-21T00:56:57Z","title":"When Cloud Removal Meets Diffusion Model in Remote Sensing","summary":"  Cloud occlusion significantly hinders remote sensing applications by\nobstructing surface information and complicating analysis. To address this, we\npropose DC4CR (Diffusion Control for Cloud Removal), a novel multimodal\ndiffusion-based framework for cloud removal in remote sensing imagery. Our\nmethod introduces prompt-driven control, allowing selective removal of thin and\nthick clouds without relying on pre-generated cloud masks, thereby enhancing\npreprocessing efficiency and model adaptability. Additionally, we integrate\nlow-rank adaptation for computational efficiency, subject-driven generation for\nimproved generalization, and grouped learning to enhance performance on small\ndatasets. Designed as a plug-and-play module, DC4CR seamlessly integrates into\nexisting cloud removal models, providing a scalable and robust solution.\nExtensive experiments on the RICE and CUHK-CR datasets demonstrate\nstate-of-the-art performance, achieving superior cloud removal across diverse\nconditions. This work presents a practical and efficient approach for remote\nsensing image processing with broad real-world applications.\n","authors":["Zhenyu Yu","Mohd Yamani Idna Idris","Pei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14783v1","updated":"2025-04-21T00:46:31Z","published":"2025-04-21T00:46:31Z","title":"How Effective Can Dropout Be in Multiple Instance Learning ?","summary":"  Multiple Instance Learning (MIL) is a popular weakly-supervised method for\nvarious applications, with a particular interest in histological whole slide\nimage (WSI) classification. Due to the gigapixel resolution of WSI,\napplications of MIL in WSI typically necessitate a two-stage training scheme:\nfirst, extract features from the pre-trained backbone and then perform MIL\naggregation. However, it is well-known that this suboptimal training scheme\nsuffers from \"noisy\" feature embeddings from the backbone and inherent weak\nsupervision, hindering MIL from learning rich and generalizable features.\nHowever, the most commonly used technique (i.e., dropout) for mitigating this\nissue has yet to be explored in MIL. In this paper, we empirically explore how\neffective the dropout can be in MIL. Interestingly, we observe that dropping\nthe top-k most important instances within a bag leads to better performance and\ngeneralization even under noise attack. Based on this key observation, we\npropose a novel MIL-specific dropout method, termed MIL-Dropout, which\nsystematically determines which instances to drop. Experiments on five MIL\nbenchmark datasets and two WSI datasets demonstrate that MIL-Dropout boosts the\nperformance of current MIL methods with a negligible computational cost. The\ncode is available at https://github.com/ChongQingNoSubway/MILDropout.\n","authors":["Wenhui Zhu","Peijie Qiu","Xiwen Chen","Zhangsihao Yang","Aristeidis Sotiras","Abolfazl Razi","Yalin Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14783v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11686v2","updated":"2025-04-21T00:32:35Z","published":"2024-09-18T03:56:56Z","title":"Detecting underdiagnosed medical conditions with opportunistic imaging","summary":"  Abdominal computed tomography (CT) scans are frequently performed in clinical\nsettings. Opportunistic CT involves repurposing routine CT images to extract\ndiagnostic information and is an emerging tool for detecting underdiagnosed\nconditions such as sarcopenia, hepatic steatosis, and ascites. This study\nutilizes deep learning methods to promote accurate diagnosis and clinical\ndocumentation. We analyze 2,674 inpatient CT scans to identify discrepancies\nbetween imaging phenotypes (characteristics derived from opportunistic CT\nscans) and their corresponding documentation in radiology reports and ICD\ncoding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans\ndiagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)\nthrough either opportunistic imaging or radiology reports were ICD-coded. Our\nfindings demonstrate opportunistic CT's potential to enhance diagnostic\nprecision and accuracy of risk adjustment models, offering advancements in\nprecision medicine.\n","authors":["Asad Aali","Andrew Johnston","Louis Blankemeier","Dave Van Veen","Laura T Derry","David Svec","Jason Hom","Robert D. Boutin","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2409.11686v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2504.15205v1","updated":"2025-04-21T16:20:43Z","published":"2025-04-21T16:20:43Z","title":"Support Evaluation for the TREC 2024 RAG Track: Comparing Human versus\n  LLM Judges","summary":"  Retrieval-augmented generation (RAG) enables large language models (LLMs) to\ngenerate answers with citations from source documents containing \"ground\ntruth\", thereby reducing system hallucinations. A crucial factor in RAG\nevaluation is \"support\", whether the information in the cited documents\nsupports the answer. To this end, we conducted a large-scale comparative study\nof 45 participant submissions on 36 topics to the TREC 2024 RAG Track,\ncomparing an automatic LLM judge (GPT-4o) against human judges for support\nassessment. We considered two conditions: (1) fully manual assessments from\nscratch and (2) manual assessments with post-editing of LLM predictions. Our\nresults indicate that for 56% of the manual from-scratch assessments, human and\nGPT-4o predictions match perfectly (on a three-level scale), increasing to 72%\nin the manual with post-editing condition. Furthermore, by carefully analyzing\nthe disagreements in an unbiased study, we found that an independent human\njudge correlates better with GPT-4o than a human judge, suggesting that LLM\njudges can be a reliable alternative for support assessment. To conclude, we\nprovide a qualitative analysis of human and GPT-4o errors to help guide future\niterations of support assessment.\n","authors":["Nandan Thakur","Ronak Pradeep","Shivani Upadhyay","Daniel Campos","Nick Craswell","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2504.15205v1.pdf","comment":"Accepted at SIGIR 2025 (short)"},{"id":"http://arxiv.org/abs/2504.15135v1","updated":"2025-04-21T14:38:44Z","published":"2025-04-21T14:38:44Z","title":"KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking","summary":"  Entity linking (EL) aligns textual mentions with their corresponding entities\nin a knowledge base, facilitating various applications such as semantic search\nand question answering. Recent advances in multimodal entity linking (MEL) have\nshown that combining text and images can reduce ambiguity and improve alignment\naccuracy. However, most existing MEL methods overlook the rich structural\ninformation available in the form of knowledge-graph (KG) triples. In this\npaper, we propose KGMEL, a novel framework that leverages KG triples to enhance\nMEL. Specifically, it operates in three stages: (1) Generation: Produces\nhigh-quality triples for each mention by employing vision-language models based\non its text and images. (2) Retrieval: Learns joint mention-entity\nrepresentations, via contrastive learning, that integrate text, images, and\n(generated or KG) triples to retrieve candidate entities for each mention. (3)\nReranking: Refines the KG triples of the candidate entities and employs large\nlanguage models to identify the best-matching entity for the mention. Extensive\nexperiments on benchmark datasets demonstrate that KGMEL outperforms existing\nmethods. Our code and datasets are available at:\nhttps://github.com/juyeonnn/KGMEL.\n","authors":["Juyeon Kim","Geon Lee","Taeuk Kim","Kijung Shin"],"pdf_url":"https://arxiv.org/pdf/2504.15135v1.pdf","comment":"SIGIR 2025 (Short)"},{"id":"http://arxiv.org/abs/2504.09428v2","updated":"2025-04-21T14:37:39Z","published":"2025-04-13T04:27:10Z","title":"FROG: Effective Friend Recommendation in Online Games via Modality-aware\n  User Preferences","summary":"  Due to the convenience of mobile devices, the online games have become an\nimportant part for user entertainments in reality, creating a demand for friend\nrecommendation in online games. However, none of existing approaches can\neffectively incorporate the multi-modal user features (e.g., images and texts)\nwith the structural information in the friendship graph, due to the following\nlimitations: (1) some of them ignore the high-order structural proximity\nbetween users, (2) some fail to learn the pairwise relevance between users at\nmodality-specific level, and (3) some cannot capture both the local and global\nuser preferences on different modalities. By addressing these issues, in this\npaper, we propose an end-to-end model FROG that better models the user\npreferences on potential friends. Comprehensive experiments on both offline\nevaluation and online deployment at Tencent have demonstrated the superiority\nof FROG over existing approaches.\n","authors":["Qiwei Wang","Dandan Lin","Wenqing Lin","Ziming Wu"],"pdf_url":"https://arxiv.org/pdf/2504.09428v2.pdf","comment":"Accepted in SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.15068v1","updated":"2025-04-21T12:55:06Z","published":"2025-04-21T12:55:06Z","title":"The Great Nugget Recall: Automating Fact Extraction and RAG Evaluation\n  with Large Language Models","summary":"  Large Language Models (LLMs) have significantly enhanced the capabilities of\ninformation access systems, especially with retrieval-augmented generation\n(RAG). Nevertheless, the evaluation of RAG systems remains a barrier to\ncontinued progress, a challenge we tackle in this work by proposing an\nautomatic evaluation framework that is validated against human annotations. We\nbelieve that the nugget evaluation methodology provides a solid foundation for\nevaluating RAG systems. This approach, originally developed for the TREC\nQuestion Answering (QA) Track in 2003, evaluates systems based on atomic facts\nthat should be present in good answers. Our efforts focus on \"refactoring\" this\nmethodology, where we describe the AutoNuggetizer framework that specifically\napplies LLMs to both automatically create nuggets and automatically assign\nnuggets to system answers. In the context of the TREC 2024 RAG Track, we\ncalibrate a fully automatic approach against strategies where nuggets are\ncreated manually or semi-manually by human assessors and then assigned manually\nto system answers. Based on results from a community-wide evaluation, we\nobserve strong agreement at the run level between scores derived from fully\nautomatic nugget evaluation and human-based variants. The agreement is stronger\nwhen individual framework components such as nugget assignment are automated\nindependently. This suggests that our evaluation framework provides tradeoffs\nbetween effort and quality that can be used to guide the development of future\nRAG systems. However, further research is necessary to refine our approach,\nparticularly in establishing robust per-topic agreement to diagnose system\nfailures effectively.\n","authors":["Ronak Pradeep","Nandan Thakur","Shivani Upadhyay","Daniel Campos","Nick Craswell","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2504.15068v1.pdf","comment":"To appear in SIGIR 2025. Significant updates and revisions to\n  arXiv:2411.09607"},{"id":"http://arxiv.org/abs/2504.15057v1","updated":"2025-04-21T12:34:57Z","published":"2025-04-21T12:34:57Z","title":"Linear Item-Item Model with Neural Knowledge for Session-based\n  Recommendation","summary":"  Session-based recommendation (SBR) aims to predict users' subsequent actions\nby modeling short-term interactions within sessions. Existing neural models\nprimarily focus on capturing complex dependencies for sequential item\ntransitions. As an alternative solution, linear item-item models mainly\nidentify strong co-occurrence patterns across items and support faster\ninference speed. Although each paradigm has been actively studied in SBR, their\nfundamental differences in capturing item relationships and how to bridge these\ndistinct modeling paradigms effectively remain unexplored. In this paper, we\npropose a novel SBR model, namely Linear Item-Item model with Neural Knowledge\n(LINK), which integrates both types of knowledge into a unified linear\nframework. Specifically, we design two specialized components of LINK: (i)\nLinear knowledge-enhanced Item-item Similarity model (LIS), which refines the\nitem similarity correlation via self-distillation, and (ii) Neural\nknowledge-enhanced Item-item Transition model (NIT), which seamlessly\nincorporates complicated neural knowledge distilled from the off-the-shelf\nneural model. Extensive experiments demonstrate that LINK outperforms\nstate-of-the-art linear SBR models across six real-world datasets, achieving\nimprovements of up to 14.78% and 11.04% in Recall@20 and MRR@20 while showing\nup to 813x fewer inference FLOPs. Our code is available at\nhttps://github.com/jin530/LINK.\n","authors":["Minjin Choi","Sunkyung Lee","Seongmin Park","Jongwuk Lee"],"pdf_url":"https://arxiv.org/pdf/2504.15057v1.pdf","comment":"SIGIR 2025, 9 pages"},{"id":"http://arxiv.org/abs/2502.04176v2","updated":"2025-04-21T12:02:08Z","published":"2025-02-06T16:07:24Z","title":"MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal\n  Retrieval-Augmented Multimodal Generation","summary":"  Recent advances in Retrieval-Augmented Generation (RAG) have significantly\nimproved response accuracy and relevance by incorporating external knowledge\ninto Large Language Models (LLMs). However, existing RAG methods primarily\nfocus on generating text-only answers, even in Multimodal Retrieval-Augmented\nGeneration (MRAG) scenarios, where multimodal elements are retrieved to assist\nin generating text answers. To address this, we introduce the Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) task, in which we aim to\ngenerate multimodal answers that combine both text and images, fully leveraging\nthe multimodal data within a corpus. Despite growing attention to this\nchallenging task, a notable lack of a comprehensive benchmark persists for\neffectively evaluating its performance. To bridge this gap, we provide\nMRAMG-Bench, a meticulously curated, human-annotated benchmark comprising 4,346\ndocuments, 14,190 images, and 4,800 QA pairs, distributed across six distinct\ndatasets and spanning three domains: Web, Academia, and Lifestyle. The datasets\nincorporate diverse difficulty levels and complex multi-image scenarios,\nproviding a robust foundation for evaluating the MRAMG task. To facilitate\nrigorous evaluation, MRAMG-Bench incorporates a comprehensive suite of both\nstatistical and LLM-based metrics, enabling a thorough analysis of the\nperformance of generative models in the MRAMG task. Additionally, we propose an\nefficient and flexible multimodal answer generation framework that can leverage\nLLMs/MLLMs to generate multimodal responses. Our datasets and complete\nevaluation results for 11 popular generative models are available at\nhttps://github.com/MRAMG-Bench/MRAMG.\n","authors":["Qinhan Yu","Zhiyou Xiao","Binghui Li","Zhengren Wang","Chong Chen","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.04176v2.pdf","comment":"Published as a conference paper at SIGIR 2025; 11 pages"},{"id":"http://arxiv.org/abs/2504.14991v1","updated":"2025-04-21T09:41:08Z","published":"2025-04-21T09:41:08Z","title":"Understanding Accuracy-Fairness Trade-offs in Re-ranking through\n  Elasticity in Economics","summary":"  Fairness is an increasingly important factor in re-ranking tasks. Prior work\nhas identified a trade-off between ranking accuracy and item fairness. However,\nthe underlying mechanisms are still not fully understood. An analogy can be\ndrawn between re-ranking and the dynamics of economic transactions. The\naccuracy-fairness trade-off parallels the coupling of the commodity tax\ntransfer process. Fairness considerations in re-ranking, similar to a commodity\ntax on suppliers, ultimately translate into a cost passed on to consumers.\nAnalogously, item-side fairness constraints result in a decline in user-side\naccuracy. In economics, the extent to which commodity tax on the supplier (item\nfairness) transfers to commodity tax on users (accuracy loss) is formalized\nusing the notion of elasticity. The re-ranking fairness-accuracy trade-off is\nsimilarly governed by the elasticity of utility between item groups. This\ninsight underscores the limitations of current fair re-ranking evaluations,\nwhich often rely solely on a single fairness metric, hindering comprehensive\nassessment of fair re-ranking algorithms. Centered around the concept of\nelasticity, this work presents two significant contributions. We introduce the\nElastic Fairness Curve (EF-Curve) as an evaluation framework. This framework\nenables a comparative analysis of algorithm performance across different\nelasticity levels, facilitating the selection of the most suitable approach.\nFurthermore, we propose ElasticRank, a fair re-ranking algorithm that employs\nelasticity calculations to adjust inter-item distances within a curved space.\nExperiments on three widely used ranking datasets demonstrate its effectiveness\nand efficiency.\n","authors":["Chen Xu","Jujia Zhao","Wenjie Wang","Liang Pang","Jun Xu","Tat-Seng Chua","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2504.14991v1.pdf","comment":"Accepted in SIGIR2025"},{"id":"http://arxiv.org/abs/2504.14903v1","updated":"2025-04-21T07:18:09Z","published":"2025-04-21T07:18:09Z","title":"ColBERT-serve: Efficient Multi-Stage Memory-Mapped Scoring","summary":"  We study serving retrieval models, specifically late interaction models like\nColBERT, to many concurrent users at once and under a small budget, in which\nthe index may not fit in memory. We present ColBERT-serve, a novel serving\nsystem that applies a memory-mapping strategy to the ColBERT index, reducing\nRAM usage by 90% and permitting its deployment on cheap servers, and\nincorporates a multi-stage architecture with hybrid scoring, reducing ColBERT's\nquery latency and supporting many concurrent queries in parallel.\n","authors":["Kaili Huang","Thejas Venkatesh","Uma Dingankar","Antonio Mallia","Daniel Campos","Jian Jiao","Christopher Potts","Matei Zaharia","Kwabena Boahen","Omar Khattab","Saarthak Sarup","Keshav Santhanam"],"pdf_url":"https://arxiv.org/pdf/2504.14903v1.pdf","comment":"Accepted by ECIR 2025"},{"id":"http://arxiv.org/abs/2504.14861v1","updated":"2025-04-21T05:01:58Z","published":"2025-04-21T05:01:58Z","title":"Stitching Inner Product and Euclidean Metrics for Topology-aware Maximum\n  Inner Product Search","summary":"  Maximum Inner Product Search (MIPS) is a fundamental challenge in machine\nlearning and information retrieval, particularly in high-dimensional data\napplications. Existing approaches to MIPS either rely solely on Inner Product\n(IP) similarity, which faces issues with local optima and redundant\ncomputations, or reduce the MIPS problem to the Nearest Neighbor Search under\nthe Euclidean metric via space projection, leading to topology destruction and\ninformation loss. Despite the divergence of the two paradigms, we argue that\nthere is no inherent binary opposition between IP and Euclidean metrics. By\nstitching IP and Euclidean in the design of indexing and search algorithms, we\ncan significantly enhance MIPS performance. Specifically, this paper explores\nthe theoretical and empirical connections between these two metrics from the\nMIPS perspective. Our investigation, grounded in graph-based search, reveals\nthat different indexing and search strategies offer distinct advantages for\nMIPS, depending on the underlying data topology. Building on these insights, we\nintroduce a novel graph-based index called Metric-Amphibious Graph (MAG) and a\ncorresponding search algorithm, Adaptive Navigation with Metric Switch (ANMS).\nTo facilitate parameter tuning for optimal performance, we identify three\nstatistical indicators that capture essential data topology properties and\ncorrelate strongly with parameter tuning. Extensive experiments on 12\nreal-world datasets demonstrate that MAG outperforms existing state-of-the-art\nmethods, achieving up to 4x search speedup while maintaining adaptability and\nscalability.\n","authors":["Tingyang Chen","Cong Fu","Xiangyu Ke","Yunjun Gao","Yabo Ni","Anxiang Zeng"],"pdf_url":"https://arxiv.org/pdf/2504.14861v1.pdf","comment":"Accepted by SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.14845v1","updated":"2025-04-21T03:56:56Z","published":"2025-04-21T03:56:56Z","title":"Enhancing the Patent Matching Capability of Large Language Models via\n  the Memory Graph","summary":"  Intellectual Property (IP) management involves strategically protecting and\nutilizing intellectual assets to enhance organizational innovation,\ncompetitiveness, and value creation. Patent matching is a crucial task in\nintellectual property management, which facilitates the organization and\nutilization of patents. Existing models often rely on the emergent capabilities\nof Large Language Models (LLMs) and leverage them to identify related patents\ndirectly. However, these methods usually depend on matching keywords and\noverlook the hierarchical classification and categorical relationships of\npatents. In this paper, we propose MemGraph, a method that augments the patent\nmatching capabilities of LLMs by incorporating a memory graph derived from\ntheir parametric memory. Specifically, MemGraph prompts LLMs to traverse their\nmemory to identify relevant entities within patents, followed by attributing\nthese entities to corresponding ontologies. After traversing the memory graph,\nwe utilize extracted entities and ontologies to improve the capability of LLM\nin comprehending the semantics of patents. Experimental results on the\nPatentMatch dataset demonstrate the effectiveness of MemGraph, achieving a\n17.68% performance improvement over baseline LLMs. The further analysis\nhighlights the generalization ability of MemGraph across various LLMs, both\nin-domain and out-of-domain, and its capacity to enhance the internal reasoning\nprocesses of LLMs during patent matching. All data and codes are available at\nhttps://github.com/NEUIR/MemGraph.\n","authors":["Qiushi Xiong","Zhipeng Xu","Zhenghao Liu","Mengjia Wang","Zulong Chen","Yue Sun","Yu Gu","Xiaohua Li","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2504.14845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05441v4","updated":"2025-04-21T03:45:36Z","published":"2024-07-07T17:05:24Z","title":"Language Representations Can be What Recommenders Need: Findings and\n  Potentials","summary":"  Recent studies empirically indicate that language models (LMs) encode rich\nworld knowledge beyond mere semantics, attracting significant attention across\nvarious fields. However, in the recommendation domain, it remains uncertain\nwhether LMs implicitly encode user preference information. Contrary to\nprevailing understanding that LMs and traditional recommenders learn two\ndistinct representation spaces due to the huge gap in language and behavior\nmodeling objectives, this work re-examines such understanding and explores\nextracting a recommendation space directly from the language representation\nspace. Surprisingly, our findings demonstrate that item representations, when\nlinearly mapped from advanced LM representations, yield superior recommendation\nperformance. This outcome suggests the possible homomorphism between the\nadvanced language representation space and an effective item representation\nspace for recommendation, implying that collaborative signals may be implicitly\nencoded within LMs. Motivated by these findings, we explore the possibility of\ndesigning advanced collaborative filtering (CF) models purely based on language\nrepresentations without ID-based embeddings. To be specific, we incorporate\nseveral crucial components to build a simple yet effective model, with item\ntitles as the input. Empirical results show that such a simple model can\noutperform leading ID-based CF models, which sheds light on using language\nrepresentations for better recommendation. Moreover, we systematically analyze\nthis simple model and find several key features for using advanced language\nrepresentations: a good initialization for item representations, zero-shot\nrecommendation abilities, and being aware of user intention. Our findings\nhighlight the connection between language modeling and behavior modeling, which\ncan inspire both natural language processing and recommender system\ncommunities.\n","authors":["Leheng Sheng","An Zhang","Yi Zhang","Yuxin Chen","Xiang Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2407.05441v4.pdf","comment":"ICLR 2025 (Oral). Codes are available at\n  https://github.com/LehengTHU/AlphaRec"},{"id":"http://arxiv.org/abs/2504.14839v1","updated":"2025-04-21T03:40:43Z","published":"2025-04-21T03:40:43Z","title":"Exploring $\\ell_0$ Sparsification for Inference-free Sparse Retrievers","summary":"  With increasing demands for efficiency, information retrieval has developed a\nbranch of sparse retrieval, further advancing towards inference-free retrieval\nwhere the documents are encoded during indexing time and there is no\nmodel-inference for queries. Existing sparse retrieval models rely on FLOPS\nregularization for sparsification, while this mechanism was originally designed\nfor Siamese encoders, it is considered to be suboptimal in inference-free\nscenarios which is asymmetric. Previous attempts to adapt FLOPS for\ninference-free scenarios have been limited to rule-based methods, leaving the\npotential of sparsification approaches for inference-free retrieval models\nlargely unexplored. In this paper, we explore $\\ell_0$ inspired sparsification\nmanner for inference-free retrievers. Through comprehensive out-of-domain\nevaluation on the BEIR benchmark, our method achieves state-of-the-art\nperformance among inference-free sparse retrieval models and is comparable to\nleading Siamese sparse retrieval models. Furthermore, we provide insights into\nthe trade-off between retrieval effectiveness and computational efficiency,\ndemonstrating practical value for real-world applications.\n","authors":["Xinjie Shen","Zhichao Geng","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2504.14839v1.pdf","comment":"Accepted by SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.14808v1","updated":"2025-04-21T02:17:19Z","published":"2025-04-21T02:17:19Z","title":"On Self-improving Token Embeddings","summary":"  This article introduces a novel and fast method for refining pre-trained\nstatic word or, more generally, token embeddings. By incorporating the\nembeddings of neighboring tokens in text corpora, it continuously updates the\nrepresentation of each token, including those without pre-assigned embeddings.\nThis approach effectively addresses the out-of-vocabulary problem, too.\nOperating independently of large language models and shallow neural networks,\nit enables versatile applications such as corpus exploration, conceptual\nsearch, and word sense disambiguation. The method is designed to enhance token\nrepresentations within topically homogeneous corpora, where the vocabulary is\nrestricted to a specific domain, resulting in more meaningful embeddings\ncompared to general-purpose pre-trained vectors. As an example, the methodology\nis applied to explore storm events and their impacts on infrastructure and\ncommunities using narratives from a subset of the NOAA Storm Events database.\nThe article also demonstrates how the approach improves the representation of\nstorm-related terms over time, providing valuable insights into the evolving\nnature of disaster narratives.\n","authors":["Mario M. Kubek","Shiraj Pokharel","Thomas Böhme","Emma L. McDaniel","Herwig Unger","Armin R. Mikler"],"pdf_url":"https://arxiv.org/pdf/2504.14808v1.pdf","comment":"18 pages, 4 figures, 3 tables, accepted at the 2025 25th\n  International Conference on Innovations for Community Services (I4CS), June\n  11 - 13, Munich, Germany, 2025"},{"id":"http://arxiv.org/abs/2504.14788v1","updated":"2025-04-21T01:10:59Z","published":"2025-04-21T01:10:59Z","title":"The 1st EReL@MIR Workshop on Efficient Representation Learning for\n  Multimodal Information Retrieval","summary":"  Multimodal representation learning has garnered significant attention in the\nAI community, largely due to the success of large pre-trained multimodal\nfoundation models like LLaMA, GPT, Mistral, and CLIP. These models have\nachieved remarkable performance across various tasks of multimodal information\nretrieval (MIR), including web search, cross-modal retrieval, and recommender\nsystems, etc. However, due to their enormous parameter sizes, significant\nefficiency challenges emerge across training, deployment, and inference stages\nwhen adapting these models' representation for IR tasks. These challenges\npresent substantial obstacles to the practical adaptation of foundation models\nfor representation learning in information retrieval tasks.\n  To address these pressing issues, we propose organizing the first EReL@MIR\nworkshop at the Web Conference 2025, inviting participants to explore novel\nsolutions, emerging problems, challenges, efficiency evaluation metrics and\nbenchmarks. This workshop aims to provide a platform for both academic and\nindustry researchers to engage in discussions, share insights, and foster\ncollaboration toward achieving efficient and effective representation learning\nfor multimodal information retrieval in the era of large foundation models.\n","authors":["Junchen Fu","Xuri Ge","Xin Xin","Haitao Yu","Yue Feng","Alexandros Karatzoglou","Ioannis Arapakis","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2504.14788v1.pdf","comment":"WWW2025 Workshop Summary"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2406.08472v4","updated":"2025-04-21T17:59:59Z","published":"2024-06-12T17:56:31Z","title":"RILe: Reinforced Imitation Learning","summary":"  Acquiring complex behaviors is essential for artificially intelligent agents,\nyet learning these behaviors in high-dimensional settings poses a significant\nchallenge due to the vast search space. Traditional reinforcement learning (RL)\nrequires extensive manual effort for reward function engineering. Inverse\nreinforcement learning (IRL) uncovers reward functions from expert\ndemonstrations but relies on an iterative process that is often computationally\nexpensive. Imitation learning (IL) provides a more efficient alternative by\ndirectly comparing an agent's actions to expert demonstrations; however, in\nhigh-dimensional environments, such direct comparisons often offer insufficient\nfeedback for effective learning. We introduce RILe (Reinforced Imitation\nLearning), a framework that combines the strengths of imitation learning and\ninverse reinforcement learning to learn a dense reward function efficiently and\nachieve strong performance in high-dimensional tasks. RILe employs a novel\ntrainer-student framework: the trainer learns an adaptive reward function, and\nthe student uses this reward signal to imitate expert behaviors. By dynamically\nadjusting its guidance as the student evolves, the trainer provides nuanced\nfeedback across different phases of learning. Our framework produces\nhigh-performing policies in high-dimensional tasks where direct imitation fails\nto replicate complex behaviors. We validate RILe in challenging robotic\nlocomotion tasks, demonstrating that it significantly outperforms existing\nmethods and achieves near-expert performance across multiple settings.\n","authors":["Mert Albaba","Sammy Christen","Thomas Langarek","Christoph Gebhardt","Otmar Hilliges","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2406.08472v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15275v1","updated":"2025-04-21T17:59:02Z","published":"2025-04-21T17:59:02Z","title":"Stop Summation: Min-Form Credit Assignment Is All Process Reward Model\n  Needs for Reasoning","summary":"  Process reward models (PRMs) have proven effective for test-time scaling of\nLarge Language Models (LLMs) on challenging reasoning tasks. However, reward\nhacking issues with PRMs limit their successful application in reinforcement\nfine-tuning. In this paper, we identify the main cause of PRM-induced reward\nhacking: the canonical summation-form credit assignment in reinforcement\nlearning (RL), which defines the value as cumulative gamma-decayed future\nrewards, easily induces LLMs to hack steps with high rewards. To address this,\nwe propose PURE: Process sUpervised Reinforcement lEarning. The key innovation\nof PURE is a min-form credit assignment that formulates the value function as\nthe minimum of future rewards. This method significantly alleviates reward\nhacking by limiting the value function range and distributing advantages more\nreasonably. Through extensive experiments on 3 base models, we show that\nPRM-based approaches enabling min-form credit assignment achieve comparable\nreasoning performance to verifiable reward-based methods within only 30% steps.\nIn contrast, the canonical sum-form credit assignment collapses training even\nat the beginning! Additionally, when we supplement PRM-based fine-tuning with\njust 10% verifiable rewards, we further alleviate reward hacking and produce\nthe best fine-tuned model based on Qwen2.5-Math-7B in our experiments,\nachieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5\nbenchmarks. Moreover, we summarize the observed reward hacking cases and\nanalyze the causes of training collapse. Code and models are available at\nhttps://github.com/CJReinforce/PURE.\n","authors":["Jie Cheng","Ruixi Qiao","Lijun Li","Chao Guo","Junle Wang","Gang Xiong","Yisheng Lv","Fei-Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2504.15275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11794v4","updated":"2025-04-21T17:48:15Z","published":"2024-06-17T17:42:57Z","title":"DataComp-LM: In search of the next generation of training sets for\n  language models","summary":"  We introduce DataComp for Language Models (DCLM), a testbed for controlled\ndataset experiments with the goal of improving language models. As part of\nDCLM, we provide a standardized corpus of 240T tokens extracted from Common\nCrawl, effective pretraining recipes based on the OpenLM framework, and a broad\nsuite of 53 downstream evaluations. Participants in the DCLM benchmark can\nexperiment with data curation strategies such as deduplication, filtering, and\ndata mixing at model scales ranging from 412M to 7B parameters. As a baseline\nfor DCLM, we conduct extensive experiments and find that model-based filtering\nis key to assembling a high-quality training set. The resulting dataset,\nDCLM-Baseline enables training a 7B parameter language model from scratch to\n64% 5-shot accuracy on MMLU with 2.6T training tokens. Compared to MAP-Neo, the\nprevious state-of-the-art in open-data language models, DCLM-Baseline\nrepresents a 6.6 percentage point improvement on MMLU while being trained with\n40% less compute. Our baseline model is also comparable to Mistral-7B-v0.3 and\nLlama 3 8B on MMLU (63% & 66%), and performs similarly on an average of 53\nnatural language understanding tasks while being trained with 6.6x less compute\nthan Llama 3 8B. Our results highlight the importance of dataset design for\ntraining language models and offer a starting point for further research on\ndata curation.\n","authors":["Jeffrey Li","Alex Fang","Georgios Smyrnis","Maor Ivgi","Matt Jordan","Samir Gadre","Hritik Bansal","Etash Guha","Sedrick Keh","Kushal Arora","Saurabh Garg","Rui Xin","Niklas Muennighoff","Reinhard Heckel","Jean Mercat","Mayee Chen","Suchin Gururangan","Mitchell Wortsman","Alon Albalak","Yonatan Bitton","Marianna Nezhurina","Amro Abbas","Cheng-Yu Hsieh","Dhruba Ghosh","Josh Gardner","Maciej Kilian","Hanlin Zhang","Rulin Shao","Sarah Pratt","Sunny Sanyal","Gabriel Ilharco","Giannis Daras","Kalyani Marathe","Aaron Gokaslan","Jieyu Zhang","Khyathi Chandu","Thao Nguyen","Igor Vasiljevic","Sham Kakade","Shuran Song","Sujay Sanghavi","Fartash Faghri","Sewoong Oh","Luke Zettlemoyer","Kyle Lo","Alaaeldin El-Nouby","Hadi Pouransari","Alexander Toshev","Stephanie Wang","Dirk Groeneveld","Luca Soldaini","Pang Wei Koh","Jenia Jitsev","Thomas Kollar","Alexandros G. Dimakis","Yair Carmon","Achal Dave","Ludwig Schmidt","Vaishaal Shankar"],"pdf_url":"https://arxiv.org/pdf/2406.11794v4.pdf","comment":"Project page: https://www.datacomp.ai/dclm/"},{"id":"http://arxiv.org/abs/2504.15266v1","updated":"2025-04-21T17:47:46Z","published":"2025-04-21T17:47:46Z","title":"Roll the dice & look before you leap: Going beyond the creative limits\n  of next-token prediction","summary":"  We design a suite of minimal algorithmic tasks that are a loose abstraction\nof open-ended real-world tasks. This allows us to cleanly and controllably\nquantify the creative limits of the present-day language model. Much like\nreal-world tasks that require a creative, far-sighted leap of thought, our\ntasks require an implicit, open-ended stochastic planning step that either (a)\ndiscovers new connections in an abstract knowledge graph (like in wordplay,\ndrawing analogies, or research) or (b) constructs new patterns (like in\ndesigning math problems or new proteins). In these tasks, we empirically and\nconceptually argue how next-token learning is myopic and memorizes excessively;\ncomparatively, multi-token approaches, namely teacherless training and\ndiffusion models, excel in producing diverse and original output. Secondly, in\nour tasks, we find that to elicit randomness from the Transformer without\nhurting coherence, it is better to inject noise right at the input layer (via a\nmethod we dub hash-conditioning) rather than defer to temperature sampling from\nthe output layer. Thus, our work offers a principled, minimal test-bed for\nanalyzing open-ended creative skills, and offers new arguments for going beyond\nnext-token learning and softmax-based sampling. We make part of the code\navailable under https://github.com/chenwu98/algorithmic-creativity\n","authors":["Vaishnavh Nagarajan","Chen Henry Wu","Charles Ding","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2504.15266v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2503.10566v2","updated":"2025-04-21T17:45:08Z","published":"2025-03-13T17:17:17Z","title":"ASIDE: Architectural Separation of Instructions and Data in Language\n  Models","summary":"  Despite their remarkable performance, large language models lack elementary\nsafety features, and this makes them susceptible to numerous malicious attacks.\nIn particular, previous work has identified the absence of an intrinsic\nseparation between instructions and data as a root cause for the success of\nprompt injection attacks. In this work, we propose a method, ASIDE, that allows\nthe model to clearly separate between instructions and data on the level of\nembeddings. ASIDE applies a fixed orthogonal rotation to the embeddings of data\ntokens, thus creating distinct representations of instructions and data tokens\nwithout introducing any additional parameters. We demonstrate the effectiveness\nof our method by instruct-tuning LLMs with ASIDE and showing (1) highly\nincreased instruction-data separation scores without a loss in model\ncapabilities and (2) competitive results on prompt injection benchmarks, even\nwithout dedicated safety training. Additionally, we study the working mechanism\nbehind our method through an analysis of model representations.\n","authors":["Egor Zverev","Evgenii Kortukov","Alexander Panfilov","Alexandra Volkova","Soroush Tabesh","Sebastian Lapuschkin","Wojciech Samek","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2503.10566v2.pdf","comment":"ICLR 2025 Workshop on Building Trust in Language Models and\n  Applications"},{"id":"http://arxiv.org/abs/2504.15261v1","updated":"2025-04-21T17:41:15Z","published":"2025-04-21T17:41:15Z","title":"Leveraging Language Models for Automated Patient Record Linkage","summary":"  Objective: Healthcare data fragmentation presents a major challenge for\nlinking patient data, necessitating robust record linkage to integrate patient\nrecords from diverse sources. This study investigates the feasibility of\nleveraging language models for automated patient record linkage, focusing on\ntwo key tasks: blocking and matching. Materials and Methods: We utilized\nreal-world healthcare data from the Missouri Cancer Registry and Research\nCenter, linking patient records from two independent sources using\nprobabilistic linkage as a baseline. A transformer-based model, RoBERTa, was\nfine-tuned for blocking using sentence embeddings. For matching, several\nlanguage models were experimented under fine-tuned and zero-shot settings,\nassessing their performance against ground truth labels. Results: The\nfine-tuned blocking model achieved a 92% reduction in the number of candidate\npairs while maintaining near-perfect recall. In the matching task, fine-tuned\nMistral-7B achieved the best performance with only 6 incorrect predictions.\nAmong zero-shot models, Mistral-Small-24B performed best, with a total of 55\nincorrect predictions. Discussion: Fine-tuned language models achieved strong\nperformance in patient record blocking and matching with minimal errors.\nHowever, they remain less accurate and efficient than a hybrid rule-based and\nprobabilistic approach for blocking. Additionally, reasoning models like\nDeepSeek-R1 are impractical for large-scale record linkage due to high\ncomputational costs. Conclusion: This study highlights the potential of\nlanguage models for automating patient record linkage, offering improved\nefficiency by eliminating the manual efforts required to perform patient record\nlinkage. Overall, language models offer a scalable solution that can enhance\ndata integration, reduce manual effort, and support disease surveillance and\nresearch.\n","authors":["Mohammad Beheshti","Lovedeep Gondara","Iris Zachary"],"pdf_url":"https://arxiv.org/pdf/2504.15261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15253v1","updated":"2025-04-21T17:33:23Z","published":"2025-04-21T17:33:23Z","title":"Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\n  Test-Time Scaling Evaluators","summary":"  Scaling test-time computation, or affording a generator large language model\n(LLM) extra compute during inference, typically employs the help of external\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\nmodels trained to generate evaluations and critiques (explanations) in natural\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\njudge empirical successes, their effectiveness as evaluators in test-time\nscaling settings is largely unknown. In this paper, we introduce the Judge\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\nperformance in three domains (math reasoning, code generation, and instruction\nfollowing) under three task settings: response reranking, step-level beam\nsearch, and critique-based response refinement. We evaluate 10 different judge\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\nparameters). Our benchmark shows that while judges are competitive with outcome\nreward models in reranking, they are consistently worse than process reward\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\ntheir natural language critiques are currently ineffective in guiding the\ngenerator towards better responses.\n","authors":["Yilun Zhou","Austin Xu","Peifeng Wang","Caiming Xiong","Shafiq Joty"],"pdf_url":"https://arxiv.org/pdf/2504.15253v1.pdf","comment":"The first two authors contributed equally. The codebase is at\n  https://github.com/SalesforceAIResearch/jetts-benchmark"},{"id":"http://arxiv.org/abs/2504.15252v1","updated":"2025-04-21T17:33:02Z","published":"2025-04-21T17:33:02Z","title":"SuoiAI: Building a Dataset for Aquatic Invertebrates in Vietnam","summary":"  Understanding and monitoring aquatic biodiversity is critical for ecological\nhealth and conservation efforts. This paper proposes SuoiAI, an end-to-end\npipeline for building a dataset of aquatic invertebrates in Vietnam and\nemploying machine learning (ML) techniques for species classification. We\noutline the methods for data collection, annotation, and model training,\nfocusing on reducing annotation effort through semi-supervised learning and\nleveraging state-of-the-art object detection and classification models. Our\napproach aims to overcome challenges such as data scarcity, fine-grained\nclassification, and deployment in diverse environmental conditions.\n","authors":["Tue Vo","Lakshay Sharma","Tuan Dinh","Khuong Dinh","Trang Nguyen","Trung Phan","Minh Do","Duong Vu"],"pdf_url":"https://arxiv.org/pdf/2504.15252v1.pdf","comment":"Published as a workshop paper at \"Tackling Climate Change with\n  Machine Learning\", ICLR 2025"},{"id":"http://arxiv.org/abs/2504.15251v1","updated":"2025-04-21T17:31:55Z","published":"2025-04-21T17:31:55Z","title":"On Learning Parallel Pancakes with Mostly Uniform Weights","summary":"  We study the complexity of learning $k$-mixtures of Gaussians ($k$-GMMs) on\n$\\mathbb{R}^d$. This task is known to have complexity $d^{\\Omega(k)}$ in full\ngenerality. To circumvent this exponential lower bound on the number of\ncomponents, research has focused on learning families of GMMs satisfying\nadditional structural properties. A natural assumption posits that the\ncomponent weights are not exponentially small and that the components have the\nsame unknown covariance. Recent work gave a $d^{O(\\log(1/w_{\\min}))}$-time\nalgorithm for this class of GMMs, where $w_{\\min}$ is the minimum weight. Our\nfirst main result is a Statistical Query (SQ) lower bound showing that this\nquasi-polynomial upper bound is essentially best possible, even for the special\ncase of uniform weights. Specifically, we show that it is SQ-hard to\ndistinguish between such a mixture and the standard Gaussian. We further\nexplore how the distribution of weights affects the complexity of this task.\nOur second main result is a quasi-polynomial upper bound for the aforementioned\ntesting task when most of the weights are uniform while a small fraction of the\nweights are potentially arbitrary.\n","authors":["Ilias Diakonikolas","Daniel M. Kane","Sushrut Karmalkar","Jasper C. H. Lee","Thanasis Pittas"],"pdf_url":"https://arxiv.org/pdf/2504.15251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00904v2","updated":"2025-04-21T17:27:05Z","published":"2025-04-01T15:33:28Z","title":"Explorable INR: An Implicit Neural Representation for Ensemble\n  Simulation Enabling Efficient Spatial and Parameter Exploration","summary":"  With the growing computational power available for high-resolution ensemble\nsimulations in scientific fields such as cosmology and oceanology, storage and\ncomputational demands present significant challenges. Current surrogate models\nfall short in the flexibility of point- or region-based predictions as the\nentire field reconstruction is required for each parameter setting, hence\nhindering the efficiency of parameter space exploration. Limitations exist in\ncapturing physical attribute distributions and pinpointing optimal parameter\nconfigurations. In this work, we propose Explorable INR, a novel implicit\nneural representation-based surrogate model, designed to facilitate exploration\nand allow point-based spatial queries without computing full-scale field data.\nIn addition, to further address computational bottlenecks of spatial\nexploration, we utilize probabilistic affine forms (PAFs) for uncertainty\npropagation through Explorable INR to obtain statistical summaries,\nfacilitating various ensemble analysis and visualization tasks that are\nexpensive with existing models. Furthermore, we reformulate the parameter\nexploration problem as optimization tasks using gradient descent and KL\ndivergence minimization that ensures scalability. We demonstrate that the\nExplorable INR with the proposed approach for spatial and parameter exploration\ncan significantly reduce computation and memory costs while providing effective\nensemble analysis.\n","authors":["Yi-Tang Chen","Haoyu Li","Neng Shi","Xihaier Luo","Wei Xu","Han-Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2504.00904v2.pdf","comment":"Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (TVCG)"},{"id":"http://arxiv.org/abs/2504.15244v1","updated":"2025-04-21T17:16:14Z","published":"2025-04-21T17:16:14Z","title":"Faster Algorithms for Agnostically Learning Disjunctions and their\n  Implications","summary":"  We study the algorithmic task of learning Boolean disjunctions in the\ndistribution-free agnostic PAC model. The best known agnostic learner for the\nclass of disjunctions over $\\{0, 1\\}^n$ is the $L_1$-polynomial regression\nalgorithm, achieving complexity $2^{\\tilde{O}(n^{1/2})}$. This complexity bound\nis known to be nearly best possible within the class of Correlational\nStatistical Query (CSQ) algorithms. In this work, we develop an agnostic\nlearner for this concept class with complexity $2^{\\tilde{O}(n^{1/3})}$. Our\nalgorithm can be implemented in the Statistical Query (SQ) model, providing the\nfirst separation between the SQ and CSQ models in distribution-free agnostic\nlearning.\n","authors":["Ilias Diakonikolas","Daniel M. Kane","Lisheng Ren"],"pdf_url":"https://arxiv.org/pdf/2504.15244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15243v1","updated":"2025-04-21T17:15:48Z","published":"2025-04-21T17:15:48Z","title":"Single-loop Algorithms for Stochastic Non-convex Optimization with\n  Weakly-Convex Constraints","summary":"  Constrained optimization with multiple functional inequality constraints has\nsignificant applications in machine learning. This paper examines a crucial\nsubset of such problems where both the objective and constraint functions are\nweakly convex. Existing methods often face limitations, including slow\nconvergence rates or reliance on double-loop algorithmic designs. To overcome\nthese challenges, we introduce a novel single-loop penalty-based stochastic\nalgorithm. Following the classical exact penalty method, our approach employs a\n{\\bf hinge-based penalty}, which permits the use of a constant penalty\nparameter, enabling us to achieve a {\\bf state-of-the-art complexity} for\nfinding an approximate Karush-Kuhn-Tucker (KKT) solution. We further extend our\nalgorithm to address finite-sum coupled compositional objectives, which are\nprevalent in artificial intelligence applications, establishing improved\ncomplexity over existing approaches. Finally, we validate our method through\nexperiments on fair learning with receiver operating characteristic (ROC)\nfairness constraints and continual learning with non-forgetting constraints.\n","authors":["Ming Yang","Gang Li","Quanqi Hu","Qihang Lin","Tianbao Yang"],"pdf_url":"https://arxiv.org/pdf/2504.15243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15240v1","updated":"2025-04-21T17:14:05Z","published":"2025-04-21T17:14:05Z","title":"Conformalized-KANs: Uncertainty Quantification with Coverage Guarantees\n  for Kolmogorov-Arnold Networks (KANs) in Scientific Machine Learning","summary":"  This paper explores uncertainty quantification (UQ) methods in the context of\nKolmogorov-Arnold Networks (KANs). We apply an ensemble approach to KANs to\nobtain a heuristic measure of UQ, enhancing interpretability and robustness in\nmodeling complex functions. Building on this, we introduce Conformalized-KANs,\nwhich integrate conformal prediction, a distribution-free UQ technique, with\nKAN ensembles to generate calibrated prediction intervals with guaranteed\ncoverage. Extensive numerical experiments are conducted to evaluate the\neffectiveness of these methods, focusing particularly on the robustness and\naccuracy of the prediction intervals under various hyperparameter settings. We\nshow that the conformal KAN predictions can be applied to recent extensions of\nKANs, including Finite Basis KANs (FBKANs) and multifideilty KANs (MFKANs). The\nresults demonstrate the potential of our approaches to improve the reliability\nand applicability of KANs in scientific machine learning.\n","authors":["Amirhossein Mollaali","Christian Bolivar Moya","Amanda A. Howard","Alexander Heinlein","Panos Stinis","Guang Lin"],"pdf_url":"https://arxiv.org/pdf/2504.15240v1.pdf","comment":"17 pages, 8 figures,"},{"id":"http://arxiv.org/abs/2504.15236v1","updated":"2025-04-21T17:13:16Z","published":"2025-04-21T17:13:16Z","title":"Values in the Wild: Discovering and Analyzing Values in Real-World\n  Language Model Interactions","summary":"  AI assistants can impart value judgments that shape people's decisions and\nworldviews, yet little is known empirically about what values these systems\nrely on in practice. To address this, we develop a bottom-up,\nprivacy-preserving method to extract the values (normative considerations\nstated or demonstrated in model responses) that Claude 3 and 3.5 models exhibit\nin hundreds of thousands of real-world interactions. We empirically discover\nand taxonomize 3,307 AI values and study how they vary by context. We find that\nClaude expresses many practical and epistemic values, and typically supports\nprosocial human values while resisting values like \"moral nihilism\". While some\nvalues appear consistently across contexts (e.g. \"transparency\"), many are more\nspecialized and context-dependent, reflecting the diversity of human\ninterlocutors and their varied contexts. For example, \"harm prevention\" emerges\nwhen Claude resists users, \"historical accuracy\" when responding to queries\nabout controversial events, \"healthy boundaries\" when asked for relationship\nadvice, and \"human agency\" in technology ethics discussions. By providing the\nfirst large-scale empirical mapping of AI values in deployment, our work\ncreates a foundation for more grounded evaluation and design of values in AI\nsystems.\n","authors":["Saffron Huang","Esin Durmus","Miles McCain","Kunal Handa","Alex Tamkin","Jerry Hong","Michael Stern","Arushi Somani","Xiuruo Zhang","Deep Ganguli"],"pdf_url":"https://arxiv.org/pdf/2504.15236v1.pdf","comment":"44 pages"},{"id":"http://arxiv.org/abs/2504.15225v1","updated":"2025-04-21T16:57:46Z","published":"2025-04-21T16:57:46Z","title":"M$^2$AD: Multi-Sensor Multi-System Anomaly Detection through Global\n  Scoring and Calibrated Thresholding","summary":"  With the widespread availability of sensor data across industrial and\noperational systems, we frequently encounter heterogeneous time series from\nmultiple systems. Anomaly detection is crucial for such systems to facilitate\npredictive maintenance. However, most existing anomaly detection methods are\ndesigned for either univariate or single-system multivariate data, making them\ninsufficient for these complex scenarios. To address this, we introduce\nM$^2$AD, a framework for unsupervised anomaly detection in multivariate time\nseries data from multiple systems. M$^2$AD employs deep models to capture\nexpected behavior under normal conditions, using the residuals as indicators of\npotential anomalies. These residuals are then aggregated into a global anomaly\nscore through a Gaussian Mixture Model and Gamma calibration. We theoretically\ndemonstrate that this framework can effectively address heterogeneity and\ndependencies across sensors and systems. Empirically, M$^2$AD outperforms\nexisting methods in extensive evaluations by 21% on average, and its\neffectiveness is demonstrated on a large-scale real-world case study on 130\nassets in Amazon Fulfillment Centers. Our code and results are available at\nhttps://github.com/sarahmish/M2AD.\n","authors":["Sarah Alnegheimish","Zelin He","Matthew Reimherr","Akash Chandrayan","Abhinav Pradhan","Luca D'Angelo"],"pdf_url":"https://arxiv.org/pdf/2504.15225v1.pdf","comment":"Accepted at AISTATS 2025"},{"id":"http://arxiv.org/abs/2504.15223v1","updated":"2025-04-21T16:53:02Z","published":"2025-04-21T16:53:02Z","title":"A Deep Learning Framework for Sequence Mining with Bidirectional LSTM\n  and Multi-Scale Attention","summary":"  This paper addresses the challenges of mining latent patterns and modeling\ncontextual dependencies in complex sequence data. A sequence pattern mining\nalgorithm is proposed by integrating Bidirectional Long Short-Term Memory\n(BiLSTM) with a multi-scale attention mechanism. The BiLSTM captures both\nforward and backward dependencies in sequences, enhancing the model's ability\nto perceive global contextual structures. At the same time, the multi-scale\nattention module assigns adaptive weights to key feature regions under\ndifferent window sizes. This improves the model's responsiveness to both local\nand global important information. Extensive experiments are conducted on a\npublicly available multivariate time series dataset. The proposed model is\ncompared with several mainstream sequence modeling methods. Results show that\nit outperforms existing models in terms of accuracy, precision, and recall.\nThis confirms the effectiveness and robustness of the proposed architecture in\ncomplex pattern recognition tasks. Further ablation studies and sensitivity\nanalyses are carried out to investigate the effects of attention scale and\ninput sequence length on model performance. These results provide empirical\nsupport for structural optimization of the model.\n","authors":["Tao Yang","Yu Cheng","Yaokun Ren","Yujia Lou","Minggu Wei","Honghui Xin"],"pdf_url":"https://arxiv.org/pdf/2504.15223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11689v2","updated":"2025-04-21T16:49:31Z","published":"2024-10-15T15:24:20Z","title":"BlendRL: A Framework for Merging Symbolic and Neural Policy Learning","summary":"  Humans can leverage both symbolic reasoning and intuitive reactions. In\ncontrast, reinforcement learning policies are typically encoded in either\nopaque systems like neural networks or symbolic systems that rely on predefined\nsymbols and rules. This disjointed approach severely limits the agents'\ncapabilities, as they often lack either the flexible low-level reaction\ncharacteristic of neural agents or the interpretable reasoning of symbolic\nagents. To overcome this challenge, we introduce BlendRL, a neuro-symbolic RL\nframework that harmoniously integrates both paradigms within RL agents that use\nmixtures of both logic and neural policies. We empirically demonstrate that\nBlendRL agents outperform both neural and symbolic baselines in standard Atari\nenvironments, and showcase their robustness to environmental changes.\nAdditionally, we analyze the interaction between neural and symbolic policies,\nillustrating how their hybrid use helps agents overcome each other's\nlimitations.\n","authors":["Hikaru Shindo","Quentin Delfosse","Devendra Singh Dhami","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2410.11689v2.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2504.15220v1","updated":"2025-04-21T16:46:07Z","published":"2025-04-21T16:46:07Z","title":"Fully Bayesian Approaches to Topics over Time","summary":"  The Topics over Time (ToT) model captures thematic changes in timestamped\ndatasets by explicitly modeling publication dates jointly with word\nco-occurrence patterns. However, ToT was not approached in a fully Bayesian\nfashion, a flaw that makes it susceptible to stability problems. To address\nthis issue, we propose a fully Bayesian Topics over Time (BToT) model via the\nintroduction of a conjugate prior to the Beta distribution. This prior acts as\na regularization that prevents the online version of the algorithm from\nunstable updates when a topic is poorly represented in a mini-batch. The\ncharacteristics of this prior to the Beta distribution are studied here for the\nfirst time. Still, this model suffers from a difference in scale between the\nsingle-time observations and the multiplicity of words per document. A\nvariation of BToT, Weighted Bayesian Topics over Time (WBToT), is proposed as a\nsolution. In WBToT, publication dates are repeated a certain number of times\nper document, which balances the relative influence of words and timestamps\nalong the inference process. We have tested our models on two datasets: a\ncollection of over 200 years of US state-of-the-union (SOTU) addresses and a\nlarge-scale COVID-19 Twitter corpus of 10 million tweets. The results show that\nWBToT captures events better than Latent Dirichlet Allocation and other SOTA\ntopic models like BERTopic: the median absolute deviation of the topic presence\nover time is reduced by $51\\%$ and $34\\%$, respectively. Our experiments also\ndemonstrate the superior coherence of WBToT over BToT, which highlights the\nimportance of balancing the time and word modalities. Finally, we illustrate\nthe stability of the online optimization algorithm in WBToT, which allows the\napplication of WBToT to problems that are intractable for standard ToT.\n","authors":["Julián Cendrero","Julio Gonzalo","Ivar Zapata"],"pdf_url":"https://arxiv.org/pdf/2504.15220v1.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2407.07890v3","updated":"2025-04-21T16:43:00Z","published":"2024-07-10T17:57:58Z","title":"Training on the Test Task Confounds Evaluation and Emergence","summary":"  We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of practices that\nutilize knowledge about evaluation tasks at training time. We demonstrate that\ntraining on the test task confounds both relative model evaluations and claims\nabout emergent capabilities. We argue that the seeming superiority of one model\nfamily over another may be explained by a different degree of training on the\ntest task. To this end, we propose an effective method to adjust for the effect\nof training on the test task on benchmark evaluations. Put simply, to fine-tune\neach model under comparison on the same task-relevant data prior to evaluation.\nWe then show that instances of emergent behavior disappear gradually as models\ntrain on the test task. Our work promotes a new perspective on the evaluation\nof large language models, with broad implications for benchmarking and the\nstudy of emergent capabilities.\n","authors":["Ricardo Dominguez-Olmedo","Florian E. Dorner","Moritz Hardt"],"pdf_url":"https://arxiv.org/pdf/2407.07890v3.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2504.15217v1","updated":"2025-04-21T16:41:40Z","published":"2025-04-21T16:41:40Z","title":"DRAGON: Distributional Rewards Optimize Diffusion Generative Models","summary":"  We present Distributional RewArds for Generative OptimizatioN (DRAGON), a\nversatile framework for fine-tuning media generation models towards a desired\noutcome. Compared with traditional reinforcement learning with human feedback\n(RLHF) or pairwise preference approaches such as direct preference optimization\n(DPO), DRAGON is more flexible. It can optimize reward functions that evaluate\neither individual examples or distributions of them, making it compatible with\na broad spectrum of instance-wise, instance-to-distribution, and\ndistribution-to-distribution rewards. Leveraging this versatility, we construct\nnovel reward functions by selecting an encoder and a set of reference examples\nto create an exemplar distribution. When cross-modality encoders such as CLAP\nare used, the reference examples may be of a different modality (e.g., text\nversus audio). Then, DRAGON gathers online and on-policy generations, scores\nthem to construct a positive demonstration set and a negative set, and\nleverages the contrast between the two sets to maximize the reward. For\nevaluation, we fine-tune an audio-domain text-to-music diffusion model with 20\ndifferent reward functions, including a custom music aesthetics model, CLAP\nscore, Vendi diversity, and Frechet audio distance (FAD). We further compare\ninstance-wise (per-song) and full-dataset FAD settings while ablating multiple\nFAD encoders and reference sets. Over all 20 target rewards, DRAGON achieves an\n81.45% average win rate. Moreover, reward functions based on exemplar sets\nindeed enhance generations and are comparable to model-based rewards. With an\nappropriate exemplar set, DRAGON achieves a 60.95% human-voted music quality\nwin rate without training on human preference annotations. As such, DRAGON\nexhibits a new approach to designing and optimizing reward functions for\nimproving human-perceived quality. Sound examples at\nhttps://ml-dragon.github.io/web.\n","authors":["Yatong Bai","Jonah Casebeer","Somayeh Sojoudi","Nicholas J. Bryan"],"pdf_url":"https://arxiv.org/pdf/2504.15217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15214v1","updated":"2025-04-21T16:36:38Z","published":"2025-04-21T16:36:38Z","title":"Histogram-based Parameter-efficient Tuning for Passive Sonar\n  Classification","summary":"  Parameter-efficient transfer learning (PETL) methods adapt large artificial\nneural networks to downstream tasks without fine-tuning the entire model.\nHowever, existing additive methods, such as adapters, sometimes struggle to\ncapture distributional shifts in intermediate feature embeddings. We propose a\nnovel histogram-based parameter-efficient tuning (HPT) technique that captures\nthe statistics of the target domain and modulates the embeddings. Experimental\nresults on three downstream passive sonar datasets (ShipsEar, DeepShip, VTUAD)\ndemonstrate that HPT outperforms conventional adapters. Notably, HPT achieves\n91.8% vs. 89.8% accuracy on VTUAD. Furthermore, HPT trains faster and yields\nfeature representations closer to those of fully fine-tuned models. Overall,\nHPT balances parameter savings and performance, providing a distribution-aware\nalternative to existing adapters and shows a promising direction for scalable\ntransfer learning in resource-constrained environments. The code is publicly\navailable:\nhttps://github.com/Advanced-Vision-and-Learning-Lab/HLAST_DeepShip_ParameterEfficient.\n","authors":["Amirmohammad Mohammadi","Davelle Carreiro","Alexandra Van Dine","Joshua Peeples"],"pdf_url":"https://arxiv.org/pdf/2504.15214v1.pdf","comment":"5 pages, 4 figures. Submitted to IEEE WASPAA 2025 for possible\n  publication"},{"id":"http://arxiv.org/abs/2411.00614v2","updated":"2025-04-21T16:33:30Z","published":"2024-11-01T14:23:19Z","title":"Fast and scalable Wasserstein-1 neural optimal transport solver for\n  single-cell perturbation prediction","summary":"  \\textbf{Motivation:} Predicting single-cell perturbation responses requires\nmapping between two unpaired single-cell data distributions. Optimal transport\n(OT) theory provides a principled framework for constructing such mappings by\nminimizing transport cost. Recently, Wasserstein-2 ($W_2$) neural optimal\ntransport solvers (\\textit{e.g.}, CellOT) have been employed for this\nprediction task. However, $W_2$ OT relies on the general Kantorovich dual\nformulation, which involves optimizing over two conjugate functions, leading to\na complex min-max optimization problem that converges slowly. \\\\\n\\textbf{Results:} To address these challenges, we propose a novel solver based\non the Wasserstein-1 ($W_1$) dual formulation. Unlike $W_2$, the $W_1$ dual\nsimplifies the optimization to a maximization problem over a single 1-Lipschitz\nfunction, thus eliminating the need for time-consuming min-max optimization.\nWhile solving the $W_1$ dual only reveals the transport direction and does not\ndirectly provide a unique optimal transport map, we incorporate an additional\nstep using adversarial training to determine an appropriate transport step\nsize, effectively recovering the transport map. Our experiments demonstrate\nthat the proposed $W_1$ neural optimal transport solver can mimic the $W_2$ OT\nsolvers in finding a unique and ``monotonic\" map on 2D datasets. Moreover, the\n$W_1$ OT solver achieves performance on par with or surpasses $W_2$ OT solvers\non real single-cell perturbation datasets. Furthermore, we show that $W_1$ OT\nsolver achieves $25 \\sim 45\\times$ speedup, scales better on high dimensional\ntransportation task, and can be directly applied on single-cell RNA-seq dataset\nwith highly variable genes. \\\\ \\textbf{Availability and Implementation:} Our\nimplementation and experiments are open-sourced at\nhttps://github.com/poseidonchan/w1ot.\n","authors":["Yanshuo Chen","Zhengmian Hu","Wei Chen","Heng Huang"],"pdf_url":"https://arxiv.org/pdf/2411.00614v2.pdf","comment":"ISMB/ECCB 2025"},{"id":"http://arxiv.org/abs/2504.15209v1","updated":"2025-04-21T16:27:16Z","published":"2025-04-21T16:27:16Z","title":"A Causal Convolutional Low-rank Representation Model for Imputation of\n  Water Quality Data","summary":"  The monitoring of water quality is a crucial part of environmental\nprotection, and a large number of monitors are widely deployed to monitor water\nquality. Due to unavoidable factors such as data acquisition breakdowns,\nsensors and communication failures, water quality monitoring data suffers from\nmissing values over time, resulting in High-Dimensional and Sparse (HDS) Water\nQuality Data (WQD). The simple and rough filling of the missing values leads to\ninaccurate results and affects the implementation of relevant measures.\nTherefore, this paper proposes a Causal convolutional Low-rank Representation\n(CLR) model for imputing missing WQD to improve the completeness of the WQD,\nwhich employs a two-fold idea: a) applying causal convolutional operation to\nconsider the temporal dependence of the low-rank representation, thus\nincorporating temporal information to improve the imputation accuracy; and b)\nimplementing a hyperparameters adaptation scheme to automatically adjust the\nbest hyperparameters during model training, thereby reducing the tedious manual\nadjustment of hyper-parameters. Experimental studies on three real-world water\nquality datasets demonstrate that the proposed CLR model is superior to some of\nthe existing state-of-the-art imputation models in terms of imputation accuracy\nand time cost, as well as indicating that the proposed model provides more\nreliable decision support for environmental monitoring.\n","authors":["Xin Liao","Bing Yang","Tan Dongli","Cai Yu"],"pdf_url":"https://arxiv.org/pdf/2504.15209v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.15208v1","updated":"2025-04-21T16:26:56Z","published":"2025-04-21T16:26:56Z","title":"Compute-Optimal LLMs Provably Generalize Better With Scale","summary":"  Why do larger language models generalize better? To investigate this\nquestion, we develop generalization bounds on the pretraining objective of\nlarge language models (LLMs) in the compute-optimal regime, as described by the\nChinchilla scaling laws. We introduce a novel, fully empirical Freedman-type\nmartingale concentration inequality that tightens existing bounds by accounting\nfor the variance of the loss function. This generalization bound can be\ndecomposed into three interpretable components: the number of parameters per\ntoken, the loss variance, and the quantization error at a fixed bitrate. As\ncompute-optimal language models are scaled up, the number of parameters per\ndata point remains constant; however, both the loss variance and the\nquantization error decrease, implying that larger models should have smaller\ngeneralization gaps. We examine why larger models tend to be more quantizable\nfrom an information theoretic perspective, showing that the rate at which they\ncan integrate new information grows more slowly than their capacity on the\ncompute-optimal frontier. From these findings we produce a scaling law for the\ngeneralization gap, with bounds that become predictably stronger with scale.\n","authors":["Marc Finzi","Sanyam Kapoor","Diego Granziol","Anming Gu","Christopher De Sa","J. Zico Kolter","Andrew Gordon Wilson"],"pdf_url":"https://arxiv.org/pdf/2504.15208v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2504.15206v1","updated":"2025-04-21T16:22:44Z","published":"2025-04-21T16:22:44Z","title":"How Global Calibration Strengthens Multiaccuracy","summary":"  Multiaccuracy and multicalibration are multigroup fairness notions for\nprediction that have found numerous applications in learning and computational\ncomplexity. They can be achieved from a single learning primitive: weak\nagnostic learning. Here we investigate the power of multiaccuracy as a learning\nprimitive, both with and without the additional assumption of calibration. We\nfind that multiaccuracy in itself is rather weak, but that the addition of\nglobal calibration (this notion is called calibrated multiaccuracy) boosts its\npower substantially, enough to recover implications that were previously known\nonly assuming the stronger notion of multicalibration.\n  We give evidence that multiaccuracy might not be as powerful as standard weak\nagnostic learning, by showing that there is no way to post-process a\nmultiaccurate predictor to get a weak learner, even assuming the best\nhypothesis has correlation $1/2$. Rather, we show that it yields a restricted\nform of weak agnostic learning, which requires some concept in the class to\nhave correlation greater than $1/2$ with the labels. However, by also requiring\nthe predictor to be calibrated, we recover not just weak, but strong agnostic\nlearning.\n  A similar picture emerges when we consider the derivation of hardcore\nmeasures from predictors satisfying multigroup fairness notions. On the one\nhand, while multiaccuracy only yields hardcore measures of density half the\noptimal, we show that (a weighted version of) calibrated multiaccuracy achieves\noptimal density.\n  Our results yield new insights into the complementary roles played by\nmultiaccuracy and calibration in each setting. They shed light on why\nmultiaccuracy and global calibration, although not particularly powerful by\nthemselves, together yield considerably stronger notions.\n","authors":["Sílvia Casacuberta","Parikshit Gopalan","Varun Kanade","Omer Reingold"],"pdf_url":"https://arxiv.org/pdf/2504.15206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15199v1","updated":"2025-04-21T16:16:19Z","published":"2025-04-21T16:16:19Z","title":"Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's\n  LLM-CLIP Framework for Image Captioning","summary":"  MILS (Multimodal Iterative LLM Solver) is a recently published framework that\nclaims \"LLMs can see and hear without any training\" by leveraging an iterative,\nLLM-CLIP based approach for zero-shot image captioning. While this MILS\napproach demonstrates good performance, our investigation reveals that this\nsuccess comes at a hidden, substantial computational cost due to its expensive\nmulti-step refinement process. In contrast, alternative models such as BLIP-2\nand GPT-4V achieve competitive results through a streamlined, single-pass\napproach. We hypothesize that the significant overhead inherent in MILS's\niterative process may undermine its practical benefits, thereby challenging the\nnarrative that zero-shot performance can be attained without incurring heavy\nresource demands. This work is the first to expose and quantify the trade-offs\nbetween output quality and computational cost in MILS, providing critical\ninsights for the design of more efficient multimodal models.\n","authors":["Yassir Benhammou","Alessandro Tiberio","Gabriel Trautmann","Suman Kalyan"],"pdf_url":"https://arxiv.org/pdf/2504.15199v1.pdf","comment":"9 pages, 2 tables, 1 figure"},{"id":"http://arxiv.org/abs/2504.15193v1","updated":"2025-04-21T16:02:26Z","published":"2025-04-21T16:02:26Z","title":"Automated Measurement of Eczema Severity with Self-Supervised Learning","summary":"  Automated diagnosis of eczema using images acquired from digital camera can\nenable individuals to self-monitor their recovery. The process entails first\nsegmenting out the eczema region from the image and then measuring the severity\nof eczema in the segmented region. The state-of-the-art methods for automated\neczema diagnosis rely on deep neural networks such as convolutional neural\nnetwork (CNN) and have shown impressive performance in accurately measuring the\nseverity of eczema. However, these methods require massive volume of annotated\ndata to train which can be hard to obtain. In this paper, we propose a\nself-supervised learning framework for automated eczema diagnosis under limited\ntraining data regime. Our framework consists of two stages: i) Segmentation,\nwhere we use an in-context learning based algorithm called SegGPT for few-shot\nsegmentation of eczema region from the image; ii) Feature extraction and\nclassification, where we extract DINO features from the segmented regions and\nfeed it to a multi-layered perceptron (MLP) for 4-class classification of\neczema severity. When evaluated on a dataset of annotated \"in-the-wild\" eczema\nimages, we show that our method outperforms (Weighted F1: 0.67 $\\pm$ 0.01) the\nstate-of-the-art deep learning methods such as finetuned Resnet-18 (Weighted\nF1: 0.44 $\\pm$ 0.16) and Vision Transformer (Weighted F1: 0.40 $\\pm$ 0.22). Our\nresults show that self-supervised learning can be a viable solution for\nautomated skin diagnosis where labeled data is scarce.\n","authors":["Neelesh Kumar","Oya Aran"],"pdf_url":"https://arxiv.org/pdf/2504.15193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07728v4","updated":"2025-04-21T15:51:01Z","published":"2024-03-12T15:07:20Z","title":"CAP: A General Algorithm for Online Selective Conformal Prediction with\n  FCR Control","summary":"  We study the problem of post-selection predictive inference in an online\nfashion. To avoid devoting resources to unimportant units, a preliminary\nselection of the current individual before reporting its prediction interval is\ncommon and meaningful in online predictive tasks. Since the online selection\ncauses a temporal multiplicity in the selected prediction intervals, it is\nimportant to control the real-time false coverage-statement rate (FCR) which\nmeasures the overall miscoverage level. We develop a general framework named\nCAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on\nhistorical data to construct a calibration set if the current individual is\nselected and then outputs a conformal prediction interval for the unobserved\nlabel. We provide tractable procedures for constructing the calibration set for\npopular online selection rules. We proved that CAP can achieve an exact\nselection-conditional coverage guarantee in the finite-sample and\ndistribution-free regimes. To account for the distribution shift in online\ndata, we also embed CAP into some recent dynamic conformal prediction\nalgorithms and show that the proposed method can deliver long-run FCR control.\nNumerical results on both synthetic and real data corroborate that CAP can\neffectively control FCR around the target level and yield more narrowed\nprediction intervals over existing baselines across various settings.\n","authors":["Yajie Bao","Yuyang Huo","Haojie Ren","Changliang Zou"],"pdf_url":"https://arxiv.org/pdf/2403.07728v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11201v2","updated":"2025-04-21T15:37:50Z","published":"2024-10-15T02:37:39Z","title":"Tree of Attributes Prompt Learning for Vision-Language Models","summary":"  Prompt learning has proven effective in adapting vision language models for\ndownstream tasks. However, existing methods usually append learnable prompt\ntokens solely with the category names to obtain textual features, which fails\nto fully leverage the rich context indicated in the category name. To address\nthis issue, we propose the Tree of Attributes Prompt learning (TAP), which\nfirst instructs LLMs to generate a tree of attributes with a \"concept -\nattribute - description\" structure for each category, and then learn the\nhierarchy with vision and text prompt tokens. Unlike existing methods that\nmerely augment category names with a set of unstructured descriptions, our\napproach essentially distills structured knowledge graphs associated with class\nnames from LLMs. Furthermore, our approach introduces text and vision prompts\ndesigned to explicitly learn the corresponding visual attributes, effectively\nserving as domain experts. Additionally, the general and diverse descriptions\ngenerated based on the class names may be wrong or absent in the specific given\nimages. To address this misalignment, we further introduce a vision-conditional\npooling module to extract instance-specific text features. Extensive\nexperimental results demonstrate that our approach outperforms state-of-the-art\nmethods on the zero-shot base-to-novel generalization, cross-dataset transfer,\nas well as few-shot classification across 11 diverse datasets. Code is\navailable at https://github.com/HHenryD/TAP.\n","authors":["Tong Ding","Wanhua Li","Zhongqi Miao","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2410.11201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15171v1","updated":"2025-04-21T15:24:34Z","published":"2025-04-21T15:24:34Z","title":"Audio-Visual Class-Incremental Learning for Fish Feeding intensity\n  Assessment in Aquaculture","summary":"  Fish Feeding Intensity Assessment (FFIA) is crucial in industrial aquaculture\nmanagement. Recent multi-modal approaches have shown promise in improving FFIA\nrobustness and efficiency. However, these methods face significant challenges\nwhen adapting to new fish species or environments due to catastrophic\nforgetting and the lack of suitable datasets. To address these limitations, we\nfirst introduce AV-CIL-FFIA, a new dataset comprising 81,932 labelled\naudio-visual clips capturing feeding intensities across six different fish\nspecies in real aquaculture environments. Then, we pioneer audio-visual class\nincremental learning (CIL) for FFIA and demonstrate through benchmarking on\nAV-CIL-FFIA that it significantly outperforms single-modality methods. Existing\nCIL methods rely heavily on historical data. Exemplar-based approaches store\nraw samples, creating storage challenges, while exemplar-free methods avoid\ndata storage but struggle to distinguish subtle feeding intensity variations\nacross different fish species. To overcome these limitations, we introduce\nHAIL-FFIA, a novel audio-visual class-incremental learning framework that\nbridges this gap with a prototype-based approach that achieves exemplar-free\nefficiency while preserving essential knowledge through compact feature\nrepresentations. Specifically, HAIL-FFIA employs hierarchical representation\nlearning with a dual-path knowledge preservation mechanism that separates\ngeneral intensity knowledge from fish-specific characteristics. Additionally,\nit features a dynamic modality balancing system that adaptively adjusts the\nimportance of audio versus visual information based on feeding behaviour\nstages. Experimental results show that HAIL-FFIA is superior to SOTA methods on\nAV-CIL-FFIA, achieving higher accuracy with lower storage needs while\neffectively mitigating catastrophic forgetting in incremental fish species\nlearning.\n","authors":["Meng Cui","Xianghu Yue","Xinyuan Qian","Jinzheng Zhao","Haohe Liu","Xubo Liu","Daoliang Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2504.15171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09597v3","updated":"2025-04-21T15:18:42Z","published":"2025-04-13T14:31:52Z","title":"Understanding LLM Behaviors via Compression: Data Generation, Knowledge\n  Acquisition and Scaling Laws","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nnumerous tasks, yet principled explanations for their underlying mechanisms and\nseveral phenomena, such as scaling laws, hallucinations, and related behaviors,\nremain elusive. In this work, we revisit the classical relationship between\ncompression and prediction, grounded in Kolmogorov complexity and Shannon\ninformation theory, to provide deeper insights into LLM behaviors. By\nleveraging the Kolmogorov Structure Function and interpreting LLM compression\nas a two-part coding process, we offer a detailed view of how LLMs acquire and\nstore information across increasing model and data scales -- from pervasive\nsyntactic patterns to progressively rarer knowledge elements. Motivated by this\ntheoretical perspective and natural assumptions inspired by Heap's and Zipf's\nlaws, we introduce a simplified yet representative hierarchical data-generation\nframework called the Syntax-Knowledge model. Under the Bayesian setting, we\nshow that prediction and compression within this model naturally lead to\ndiverse learning and scaling behaviors of LLMs. In particular, our theoretical\nanalysis offers intuitive and principled explanations for both data and model\nscaling laws, the dynamics of knowledge acquisition during training and\nfine-tuning, factual knowledge hallucinations in LLMs. The experimental results\nvalidate our theoretical predictions.\n","authors":["Zhixuan Pan","Shaowen Wang","Jian Li"],"pdf_url":"https://arxiv.org/pdf/2504.09597v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14866v2","updated":"2025-04-21T15:13:44Z","published":"2025-02-20T18:59:52Z","title":"LServe: Efficient Long-sequence LLM Serving with Unified Sparse\n  Attention","summary":"  Large language models (LLMs) have shown remarkable potential in processing\nlong sequences and complex reasoning tasks, yet efficiently serving these\nmodels remains challenging due to the quadratic computational complexity of\nattention in the prefilling stage and the large memory footprint of the KV\ncache in the decoding stage. To address these issues, we introduce LServe, an\nefficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity\npatterns for both prefilling and decoding attention into a single framework,\nwhere computations on less important tokens are skipped block-wise. LServe\ndemonstrates the compatibility of static and dynamic sparsity in long-context\nLLM attention. This design enables multiplicative speedups by combining these\noptimizations. Specifically, we convert half of the attention heads to nearly\nfree streaming heads in both the prefilling and decoding stages. Additionally,\nwe find that only a constant number of KV pages is required to preserve\nlong-context and reasoning capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV\npages based on query-centric similarity. On average, LServe accelerates LLM\nprefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining\nlong-context accuracy. Code is released at\nhttps://github.com/mit-han-lab/omniserve.\n","authors":["Shang Yang","Junxian Guo","Haotian Tang","Qinghao Hu","Guangxuan Xiao","Jiaming Tang","Yujun Lin","Zhijian Liu","Yao Lu","Song Han"],"pdf_url":"https://arxiv.org/pdf/2502.14866v2.pdf","comment":"Accepted by MLSys 2025. Code available at:\n  https://github.com/mit-han-lab/omniserve"},{"id":"http://arxiv.org/abs/2504.15163v1","updated":"2025-04-21T15:09:40Z","published":"2025-04-21T15:09:40Z","title":"Survey of Loss Augmented Knowledge Tracing","summary":"  The training of artificial neural networks is heavily dependent on the\ncareful selection of an appropriate loss function. While commonly used loss\nfunctions, such as cross-entropy and mean squared error (MSE), generally\nsuffice for a broad range of tasks, challenges often emerge due to limitations\nin data quality or inefficiencies within the learning process. In such\ncircumstances, the integration of supplementary terms into the loss function\ncan serve to address these challenges, enhancing both model performance and\nrobustness. Two prominent techniques, loss regularization and contrastive\nlearning, have been identified as effective strategies for augmenting the\ncapacity of loss functions in artificial neural networks.\n  Knowledge tracing is a compelling area of research that leverages predictive\nartificial intelligence to facilitate the automation of personalized and\nefficient educational experiences for students. In this paper, we provide a\ncomprehensive review of the deep learning-based knowledge tracing (DKT)\nalgorithms trained using advanced loss functions and discuss their improvements\nover prior techniques. We discuss contrastive knowledge tracing algorithms,\nsuch as Bi-CLKT, CL4KT, SP-CLKT, CoSKT, and prediction-consistent DKT,\nproviding performance benchmarks and insights into real-world deployment\nchallenges. The survey concludes with future research directions, including\nhybrid loss strategies and context-aware modeling.\n","authors":["Altun Shukurlu"],"pdf_url":"https://arxiv.org/pdf/2504.15163v1.pdf","comment":"14 pages, no figures"},{"id":"http://arxiv.org/abs/2504.15156v1","updated":"2025-04-21T14:58:35Z","published":"2025-04-21T14:58:35Z","title":"Advanced posterior analyses of hidden Markov models: finite Markov chain\n  imbedding and hybrid decoding","summary":"  Two major tasks in applications of hidden Markov models are to (i) compute\ndistributions of summary statistics of the hidden state sequence, and (ii)\ndecode the hidden state sequence. We describe finite Markov chain imbedding\n(FMCI) and hybrid decoding to solve each of these two tasks. In the first part\nof our paper we use FMCI to compute posterior distributions of summary\nstatistics such as the number of visits to a hidden state, the total time spent\nin a hidden state, the dwell time in a hidden state, and the longest run\nlength. We use simulations from the hidden state sequence, conditional on the\nobserved sequence, to establish the FMCI framework. In the second part of our\npaper we apply hybrid segmentation for improved decoding of a HMM. We\ndemonstrate that hybrid decoding shows increased performance compared to\nViterbi or Posterior decoding (often also referred to as global or local\ndecoding), and we introduce a novel procedure for choosing the tuning parameter\nin the hybrid procedure. Furthermore, we provide an alternative derivation of\nthe hybrid loss function based on weighted geometric means. We demonstrate and\napply FMCI and hybrid decoding on various classical data sets, and supply\naccompanying code for reproducibility.\n","authors":["Zenia Elise Damgaard Bæk","Moisès Coll Macià","Laurits Skov","Asger Hobolth"],"pdf_url":"https://arxiv.org/pdf/2504.15156v1.pdf","comment":"23 pages, 14 figures"},{"id":"http://arxiv.org/abs/2410.21060v2","updated":"2025-04-21T14:37:40Z","published":"2024-10-28T14:18:32Z","title":"CTINexus: Automatic Cyber Threat Intelligence Knowledge Graph\n  Construction Using Large Language Models","summary":"  Textual descriptions in cyber threat intelligence (CTI) reports, such as\nsecurity articles and news, are rich sources of knowledge about cyber threats,\ncrucial for organizations to stay informed about the rapidly evolving threat\nlandscape. However, current CTI knowledge extraction methods lack flexibility\nand generalizability, often resulting in inaccurate and incomplete knowledge\nextraction. Syntax parsing relies on fixed rules and dictionaries, while model\nfine-tuning requires large annotated datasets, making both paradigms\nchallenging to adapt to new threats and ontologies. To bridge the gap, we\npropose CTINexus, a novel framework leveraging optimized in-context learning\n(ICL) of large language models (LLMs) for data-efficient CTI knowledge\nextraction and high-quality cybersecurity knowledge graph (CSKG) construction.\nUnlike existing methods, CTINexus requires neither extensive data nor parameter\ntuning and can adapt to various ontologies with minimal annotated examples.\nThis is achieved through: (1) a carefully designed automatic prompt\nconstruction strategy with optimal demonstration retrieval for extracting a\nwide range of cybersecurity entities and relations; (2) a hierarchical entity\nalignment technique that canonicalizes the extracted knowledge and removes\nredundancy; (3) an long-distance relation prediction technique to further\ncomplete the CSKG with missing links. Our extensive evaluations using 150\nreal-world CTI reports collected from 10 platforms demonstrate that CTINexus\nsignificantly outperforms existing methods in constructing accurate and\ncomplete CSKG, highlighting its potential to transform CTI analysis with an\nefficient and adaptable solution for the dynamic threat landscape.\n","authors":["Yutong Cheng","Osama Bajaber","Saimon Amanuel Tsegai","Dawn Song","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2410.21060v2.pdf","comment":"Accepted at 2025 IEEE European Symposium on Security and Privacy\n  (Euro S&P)"},{"id":"http://arxiv.org/abs/2504.15133v1","updated":"2025-04-21T14:33:55Z","published":"2025-04-21T14:33:55Z","title":"EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language\n  Models","summary":"  In this paper, we introduce EasyEdit2, a framework designed to enable\nplug-and-play adjustability for controlling Large Language Model (LLM)\nbehaviors. EasyEdit2 supports a wide range of test-time interventions,\nincluding safety, sentiment, personality, reasoning patterns, factuality, and\nlanguage features. Unlike its predecessor, EasyEdit2 features a new\narchitecture specifically designed for seamless model steering. It comprises\nkey modules such as the steering vector generator and the steering vector\napplier, which enable automatic generation and application of steering vectors\nto influence the model's behavior without modifying its parameters. One of the\nmain advantages of EasyEdit2 is its ease of use-users do not need extensive\ntechnical knowledge. With just a single example, they can effectively guide and\nadjust the model's responses, making precise control both accessible and\nefficient. Empirically, we report model steering performance across different\nLLMs, demonstrating the effectiveness of these techniques. We have released the\nsource code on GitHub at https://github.com/zjunlp/EasyEdit along with a\ndemonstration notebook. In addition, we provide a demo video at\nhttps://zjunlp.github.io/project/EasyEdit2/video for a quick introduction.\n","authors":["Ziwen Xu","Shuxun Wang","Kewei Xu","Haoming Xu","Mengru Wang","Xinle Deng","Yunzhi Yao","Guozhou Zheng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.15133v1.pdf","comment":"Work in progress. Demo:\n  https://zjunlp.github.io/project/EasyEdit2/video; code:\n  https://github.com/zjunlp/EasyEdit"},{"id":"http://arxiv.org/abs/2504.15129v1","updated":"2025-04-21T14:25:23Z","published":"2025-04-21T14:25:23Z","title":"A General Infrastructure and Workflow for Quadrotor Deep Reinforcement\n  Learning and Reality Deployment","summary":"  Deploying robot learning methods to a quadrotor in unstructured outdoor\nenvironments is an exciting task. Quadrotors operating in real-world\nenvironments by learning-based methods encounter several challenges: a large\namount of simulator generated data required for training, strict demands for\nreal-time processing onboard, and the sim-to-real gap caused by dynamic and\nnoisy conditions. Current works have made a great breakthrough in applying\nlearning-based methods to end-to-end control of quadrotors, but rarely mention\nthe infrastructure system training from scratch and deploying to reality, which\nmakes it difficult to reproduce methods and applications. To bridge this gap,\nwe propose a platform that enables the seamless transfer of end-to-end deep\nreinforcement learning (DRL) policies. We integrate the training environment,\nflight dynamics control, DRL algorithms, the MAVROS middleware stack, and\nhardware into a comprehensive workflow and architecture that enables\nquadrotors' policies to be trained from scratch to real-world deployment in\nseveral minutes. Our platform provides rich types of environments including\nhovering, dynamic obstacle avoidance, trajectory tracking, balloon hitting, and\nplanning in unknown environments, as a physical experiment benchmark. Through\nextensive empirical validation, we demonstrate the efficiency of proposed\nsim-to-real platform, and robust outdoor flight performance under real-world\nperturbations. Details can be found from our website\nhttps://emnavi.tech/AirGym/.\n","authors":["Kangyao Huang","Hao Wang","Yu Luo","Jingyu Chen","Jintao Chen","Xiangkui Zhang","Xiangyang Ji","Huaping Liu"],"pdf_url":"https://arxiv.org/pdf/2504.15129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15110v1","updated":"2025-04-21T14:02:59Z","published":"2025-04-21T14:02:59Z","title":"Kolmogorov-Arnold Networks: Approximation and Learning Guarantees for\n  Functions and their Derivatives","summary":"  Inspired by the Kolmogorov-Arnold superposition theorem, Kolmogorov-Arnold\nNetworks (KANs) have recently emerged as an improved backbone for most deep\nlearning frameworks, promising more adaptivity than their multilayer perception\n(MLP) predecessor by allowing for trainable spline-based activation functions.\nIn this paper, we probe the theoretical foundations of the KAN architecture by\nshowing that it can optimally approximate any Besov function in\n$B^{s}_{p,q}(\\mathcal{X})$ on a bounded open, or even fractal, domain\n$\\mathcal{X}$ in $\\mathbb{R}^d$ at the optimal approximation rate with respect\nto any weaker Besov norm $B^{\\alpha}_{p,q}(\\mathcal{X})$; where $\\alpha < s$.\nWe complement our approximation guarantee with a dimension-free estimate on the\nsample complexity of a residual KAN model when learning a function of Besov\nregularity from $N$ i.i.d. noiseless samples. Our KAN architecture incorporates\ncontemporary deep learning wisdom by leveraging residual/skip connections\nbetween layers.\n","authors":["Anastasis Kratsios","Takashi Furuya"],"pdf_url":"https://arxiv.org/pdf/2504.15110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15100v1","updated":"2025-04-21T13:41:20Z","published":"2025-04-21T13:41:20Z","title":"Application of Sensitivity Analysis Methods for Studying Neural Network\n  Models","summary":"  This study demonstrates the capabilities of several methods for analyzing the\nsensitivity of neural networks to perturbations of the input data and\ninterpreting their underlying mechanisms. The investigated approaches include\nthe Sobol global sensitivity analysis, the local sensitivity method for input\npixel perturbations and the activation maximization technique. As examples, in\nthis study we consider a small feedforward neural network for analyzing an open\ntabular dataset of clinical diabetes data, as well as two classical\nconvolutional architectures, VGG-16 and ResNet-18, which are widely used in\nimage processing and classification. Utilization of the global sensitivity\nanalysis allows us to identify the leading input parameters of the chosen tiny\nneural network and reduce their number without significant loss of the\naccuracy. As far as global sensitivity analysis is not applicable to larger\nmodels we try the local sensitivity analysis and activation maximization method\nin application to the convolutional neural networks. These methods show\ninteresting patterns for the convolutional models solving the image\nclassification problem. All in all, we compare the results of the activation\nmaximization method with popular Grad-CAM technique in the context of\nultrasound data analysis.\n","authors":["Jiaxuan Miao","Sergey Matveev"],"pdf_url":"https://arxiv.org/pdf/2504.15100v1.pdf","comment":"11 pages, 16 figures, 32 references"},{"id":"http://arxiv.org/abs/2504.15099v1","updated":"2025-04-21T13:41:09Z","published":"2025-04-21T13:41:09Z","title":"Fast-Slow Co-advancing Optimizer: Toward Harmonious Adversarial Training\n  of GAN","summary":"  Up to now, the training processes of typical Generative Adversarial Networks\n(GANs) are still particularly sensitive to data properties and hyperparameters,\nwhich may lead to severe oscillations, difficulties in convergence, or even\nfailures to converge, especially when the overall variances of the training\nsets are large. These phenomena are often attributed to the training\ncharacteristics of such networks. Aiming at the problem, this paper develops a\nnew intelligent optimizer, Fast-Slow Co-advancing Optimizer (FSCO), which\nemploys reinforcement learning in the training process of GANs to make training\neasier. Specifically, this paper allows the training step size to be controlled\nby an agent to improve training stability, and makes the training process more\nintelligent with variable learning rates, making GANs less sensitive to step\nsize. Experiments have been conducted on three benchmark datasets to verify the\neffectiveness of the developed FSCO.\n","authors":["Lin Wang","Xiancheng Wang","Rui Wang","Zhibo Zhang","Minghang Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.15099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15093v1","updated":"2025-04-21T13:25:55Z","published":"2025-04-21T13:25:55Z","title":"Rethinking the Potential of Multimodality in Collaborative Problem\n  Solving Diagnosis with Large Language Models","summary":"  Detecting collaborative and problem-solving behaviours from digital traces to\ninterpret students' collaborative problem solving (CPS) competency is a\nlong-term goal in the Artificial Intelligence in Education (AIEd) field.\nAlthough multimodal data and advanced models are argued to have the potential\nto detect complex CPS behaviours, empirical evidence on their value remains\nlimited with some contrasting evidence. In this study, we investigated the\npotential of multimodal data to improve model performance in diagnosing 78\nsecondary school students' CPS subskills and indicators in authentic\neducational settings. In particular, text embeddings from verbal data and\nacoustic embeddings from audio data were used in a multimodal classification\nmodel for CPS diagnosis. Both unimodal and multimodal transformer-based models\noutperformed traditional models in detecting CPS classes. Although the\ninclusion of multimodality did not improve the performance of traditional\nunimodal models, its integration into transformer-based models demonstrated\nimproved performance for diagnosing social-cognitive CPS classes compared to\nunimodal transformer-based models. Based on the results, the paper argues that\nmultimodality and the selection of a particular modelling technique should not\nbe taken for granted to achieve the best performance in the automated detection\nof every CPS subskill and indicator. Rather, their value is limited to certain\ntypes of CPS indicators, affected by the complexity of the labels, and\ndependent on the composition of indicators in the dataset. We conclude the\npaper by discussing the required nuance when considering the value of LLMs and\nmultimodality in automated CPS diagnosis, highlighting the need for human-AI\ncomplementarity, and proposing the exploration of relevant model architectures\nand techniques to improve CPS diagnosis in authentic educational contexts.\n","authors":["K. Wong","B. Wu","S. Bulathwela","M. Cukurova"],"pdf_url":"https://arxiv.org/pdf/2504.15093v1.pdf","comment":"Accepted for 26th International Conference on Artificial Intelligence\n  in Education (AIED 2025), 22 - 26 July 2025, Palermo, Italy. 17 pages, 1\n  figure"},{"id":"http://arxiv.org/abs/2504.15090v1","updated":"2025-04-21T13:24:30Z","published":"2025-04-21T13:24:30Z","title":"Federated Latent Factor Model for Bias-Aware Recommendation with\n  Privacy-Preserving","summary":"  A recommender system (RS) aims to provide users with personalized item\nrecommendations, enhancing their overall experience. Traditional RSs collect\nand process all user data on a central server. However, this centralized\napproach raises significant privacy concerns, as it increases the risk of data\nbreaches and privacy leakages, which are becoming increasingly unacceptable to\nprivacy-sensitive users. To address these privacy challenges, federated\nlearning has been integrated into RSs, ensuring that user data remains secure.\nIn centralized RSs, the issue of rating bias is effectively addressed by\njointly analyzing all users' raw interaction data. However, this becomes a\nsignificant challenge in federated RSs, as raw data is no longer accessible due\nto privacy-preserving constraints. To overcome this problem, we propose a\nFederated Bias-Aware Latent Factor (FBALF) model. In FBALF, training bias is\nexplicitly incorporated into every local model's loss function, allowing for\nthe effective elimination of rating bias without compromising data privacy.\nExtensive experiments conducted on three real-world datasets demonstrate that\nFBALF achieves significantly higher recommendation accuracy compared to other\nstate-of-the-art federated RSs.\n","authors":["Junxiang Gao","Yixin Ran","Jia Chen"],"pdf_url":"https://arxiv.org/pdf/2504.15090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15077v1","updated":"2025-04-21T13:05:26Z","published":"2025-04-21T13:05:26Z","title":"Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL","summary":"  Large Language Models (LLMs) have shown impressive capabilities in\ntransforming natural language questions about relational databases into SQL\nqueries. Despite recent improvements, small LLMs struggle to handle questions\ninvolving multiple tables and complex SQL patterns under a Zero-Shot Learning\n(ZSL) setting. Supervised Fine-Tuning (SFT) partially compensate the knowledge\ndeficits in pretrained models but falls short while dealing with queries\ninvolving multi-hop reasoning. To bridge this gap, different LLM training\nstrategies to reinforce reasoning capabilities have been proposed, ranging from\nleveraging a thinking process within ZSL, including reasoning traces in SFT, or\nadopt Reinforcement Learning (RL) strategies. However, the influence of\nreasoning on Text2SQL performance is still largely unexplored. This paper\ninvestigates to what extent LLM reasoning capabilities influence their Text2SQL\nperformance on four benchmark datasets. To this end, it considers the following\nLLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT,\nwith and without task-specific reasoning traces; (3) RL, leveraging execution\naccuracy as primary reward function; (4) SFT+RL, i.e, a two-stage approach that\ncombines SFT and RL. The results show that general-purpose reasoning under ZSL\nproves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit\nfrom SFT with reasoning much more than larger ones, bridging the gap of their\n(weaker) model pretraining. RL is generally beneficial across all tested models\nand datasets, particularly when SQL queries involve multi-hop reasoning and\nmultiple tables. Small LLMs with SFT+RL excel on most complex datasets thanks\nto a strategic balance between generality of the reasoning process and\noptimization of the execution accuracy. Thanks to RL, the7B Qwen-Coder-2.5\nmodel performs on par with 100+ Billion ones on the Bird dataset.\n","authors":["Simone Papicchio","Simone Rossi","Luca Cagliero","Paolo Papotti"],"pdf_url":"https://arxiv.org/pdf/2504.15077v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2503.10664v2","updated":"2025-04-21T13:04:29Z","published":"2025-03-09T08:23:31Z","title":"Semantic Wave Functions: Exploring Meaning in Large Language Models\n  Through Quantum Formalism","summary":"  Large Language Models (LLMs) encode semantic relationships in\nhigh-dimensional vector embeddings. This paper explores the analogy between LLM\nembedding spaces and quantum mechanics, positing that LLMs operate within a\nquantized semantic space where words and phrases behave as quantum states. To\ncapture nuanced semantic interference effects, we extend the standard\nreal-valued embedding space to the complex domain, drawing parallels to the\ndouble-slit experiment. We introduce a \"semantic wave function\" to formalize\nthis quantum-derived representation and utilize potential landscapes, such as\nthe double-well potential, to model semantic ambiguity. Furthermore, we propose\na complex-valued similarity measure that incorporates both magnitude and phase\ninformation, enabling a more sensitive comparison of semantic representations.\nWe develop a path integral formalism, based on a nonlinear Schr\\\"odinger\nequation with a gauge field and Mexican hat potential, to model the dynamic\nevolution of LLM behavior. This interdisciplinary approach offers a new\ntheoretical framework for understanding and potentially manipulating LLMs, with\nthe goal of advancing both artificial and natural language understanding.\n","authors":["Timo Aukusti Laine"],"pdf_url":"https://arxiv.org/pdf/2503.10664v2.pdf","comment":"29 pages, 4 figures. Some corrections added"},{"id":"http://arxiv.org/abs/2504.15075v1","updated":"2025-04-21T13:03:40Z","published":"2025-04-21T13:03:40Z","title":"Mitigating Degree Bias in Graph Representation Learning with Learnable\n  Structural Augmentation and Structural Self-Attention","summary":"  Graph Neural Networks (GNNs) update node representations through message\npassing, which is primarily based on the homophily principle, assuming that\nadjacent nodes share similar features. However, in real-world graphs with\nlong-tailed degree distributions, high-degree nodes dominate message passing,\ncausing a degree bias where low-degree nodes remain under-represented due to\ninadequate messages. The main challenge in addressing degree bias is how to\ndiscover non-adjacent nodes to provide additional messages to low-degree nodes\nwhile reducing excessive messages for high-degree nodes. Nevertheless,\nexploiting non-adjacent nodes to provide valuable messages is challenging, as\nit could generate noisy information and disrupt the original graph structures.\nTo solve it, we propose a novel Degree Fairness Graph Transformer, named\nDegFairGT, to mitigate degree bias by discovering structural similarities\nbetween non-adjacent nodes through learnable structural augmentation and\nstructural self-attention. Our key idea is to exploit non-adjacent nodes with\nsimilar roles in the same community to generate informative edges under our\naugmentation, which could provide informative messages between nodes with\nsimilar roles while ensuring that the homophily principle is maintained within\nthe community. To enable DegFairGT to learn such structural similarities, we\nthen propose a structural self-attention to capture the similarities between\nnode pairs. To preserve global graph structures and prevent graph augmentation\nfrom hindering graph structure, we propose a Self-Supervised Learning task to\npreserve p-step transition probability and regularize graph augmentation.\nExtensive experiments on six datasets showed that DegFairGT outperformed\nstate-of-the-art baselines in degree fairness analysis, node classification,\nand node clustering tasks.\n","authors":["Van Thuy Hoang","Hyeon-Ju Jeon","O-Joun Lee"],"pdf_url":"https://arxiv.org/pdf/2504.15075v1.pdf","comment":"Accepted at IEEE TNSE"},{"id":"http://arxiv.org/abs/2405.16639v4","updated":"2025-04-21T12:53:26Z","published":"2024-05-26T17:30:44Z","title":"A direct proof of a unified law of robustness for Bregman divergence\n  losses","summary":"  In contemporary deep learning practice, models are often trained to near zero\nloss i.e. to nearly interpolate the training data. However, the number of\nparameters in the model is usually far more than the number of data points n,\nthe theoretical minimum needed for interpolation: a phenomenon referred to as\noverparameterization. In an interesting piece of work, Bubeck and Sellke\nconsidered a natural notion of interpolation: the model is said to interpolate\nwhen the model's training loss goes below the loss of the conditional\nexpectation of the response given the covariate. For this notion of\ninterpolation and for a broad class of covariate distributions (specifically\nthose satisfying a natural notion of concentration of measure), they showed\nthat overparameterization is necessary for robust interpolation i.e. if the\ninterpolating function is required to be Lipschitz. Their main proof technique\napplies to regression with square loss against a scalar response, but they\nremark that via a connection to Rademacher complexity and using tools such as\nthe Ledoux-Talagrand contraction inequality, their result can be extended to\nmore general losses, at least in the case of scalar response variables. In this\nwork, we recast the original proof technique of Bubeck and Sellke in terms of a\nbias-variance type decomposition, and show that this view directly unlocks a\ngeneralization to Bregman divergence losses (even for vector-valued responses),\nwithout the use of tools such as Rademacher complexity or the Ledoux-Talagrand\ncontraction principle. Bregman divergences are a natural class of losses since\nfor these, the best estimator is the conditional expectation of the response\ngiven the covariate, and include other practical losses such as the cross\nentropy loss. Our work thus gives a more general understanding of the main\nproof technique of Bubeck and Sellke and demonstrates its broad utility.\n","authors":["Santanu Das","Jatin Batra","Piyush Srivastava"],"pdf_url":"https://arxiv.org/pdf/2405.16639v4.pdf","comment":"18 pages; fixed a typo in a citation"},{"id":"http://arxiv.org/abs/2405.11958v2","updated":"2025-04-21T12:22:55Z","published":"2024-05-20T11:28:32Z","title":"Exploring Commonalities in Explanation Frameworks: A Multi-Domain Survey\n  Analysis","summary":"  This study presents insights gathered from surveys and discussions with\nspecialists in three domains, aiming to find essential elements for a universal\nexplanation framework that could be applied to these and other similar use\ncases. The insights are incorporated into a software tool that utilizes GP\nalgorithms, known for their interpretability. The applications analyzed include\na medical scenario (involving predictive ML), a retail use case (involving\nprescriptive ML), and an energy use case (also involving predictive ML). We\ninterviewed professionals from each sector, transcribing their conversations\nfor further analysis. Additionally, experts and non-experts in these fields\nfilled out questionnaires designed to probe various dimensions of explanatory\nmethods. The findings indicate a universal preference for sacrificing a degree\nof accuracy in favor of greater explainability. Additionally, we highlight the\nsignificance of feature importance and counterfactual explanations as critical\ncomponents of such a framework. Our questionnaires are publicly available to\nfacilitate the dissemination of knowledge in the field of XAI.\n","authors":["Eduard Barbu","Marharyta Domnich","Raul Vicente","Nikos Sakkas","André Morim"],"pdf_url":"https://arxiv.org/pdf/2405.11958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.15051v1","updated":"2025-04-21T12:20:46Z","published":"2025-04-21T12:20:46Z","title":"VeLU: Variance-enhanced Learning Unit for Deep Neural Networks","summary":"  Activation functions are fundamental in deep neural networks and directly\nimpact gradient flow, optimization stability, and generalization. Although ReLU\nremains standard because of its simplicity, it suffers from vanishing gradients\nand lacks adaptability. Alternatives like Swish and GELU introduce smooth\ntransitions, but fail to dynamically adjust to input statistics. We propose\nVeLU, a Variance-enhanced Learning Unit as an activation function that\ndynamically scales based on input variance by integrating ArcTan-Sin\ntransformations and Wasserstein-2 regularization, effectively mitigating\ncovariate shifts and stabilizing optimization. Extensive experiments on\nViT_B16, VGG19, ResNet50, DenseNet121, MobileNetV2, and EfficientNetB3 confirm\nVeLU's superiority over ReLU, ReLU6, Swish, and GELU on six vision benchmarks.\nThe codes of VeLU are publicly available on GitHub.\n","authors":["Ashkan Shakarami","Yousef Yeganeh","Azade Farshad","Lorenzo Nicolè","Stefano Ghidoni","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2504.15051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02073v2","updated":"2025-04-21T12:09:08Z","published":"2024-10-02T22:42:20Z","title":"Depth Pro: Sharp Monocular Metric Depth in Less Than a Second","summary":"  We present a foundation model for zero-shot metric monocular depth\nestimation. Our model, Depth Pro, synthesizes high-resolution depth maps with\nunparalleled sharpness and high-frequency details. The predictions are metric,\nwith absolute scale, without relying on the availability of metadata such as\ncamera intrinsics. And the model is fast, producing a 2.25-megapixel depth map\nin 0.3 seconds on a standard GPU. These characteristics are enabled by a number\nof technical contributions, including an efficient multi-scale vision\ntransformer for dense prediction, a training protocol that combines real and\nsynthetic datasets to achieve high metric accuracy alongside fine boundary\ntracing, dedicated evaluation metrics for boundary accuracy in estimated depth\nmaps, and state-of-the-art focal length estimation from a single image.\nExtensive experiments analyze specific design choices and demonstrate that\nDepth Pro outperforms prior work along multiple dimensions. We release code and\nweights at https://github.com/apple/ml-depth-pro\n","authors":["Aleksei Bochkovskii","Amaël Delaunoy","Hugo Germain","Marcel Santos","Yichao Zhou","Stephan R. Richter","Vladlen Koltun"],"pdf_url":"https://arxiv.org/pdf/2410.02073v2.pdf","comment":"Published at ICLR 2025. Code and weights available at\n  https://github.com/apple/ml-depth-pro"},{"id":"http://arxiv.org/abs/2405.08190v3","updated":"2025-04-21T12:07:03Z","published":"2024-05-13T21:12:31Z","title":"Barren plateaus are amplified by the dimension of qudits","summary":"  Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for\nattaining quantum advantage in diverse scientific and technological domains,\nnotably within Quantum Neural Networks. However, despite their potential, VQAs\nencounter significant obstacles, chief among them being the vanishing gradient\nproblem, commonly referred to as barren plateaus. In this article, through\nmeticulous analysis, we demonstrate that existing literature implicitly\nsuggests the intrinsic influence of qudit dimensionality on barren plateaus. To\ninstantiate these findings, we present numerical results that exemplify the\nimpact of qudit dimensionality on barren plateaus. Therefore, despite the\nproposition of various error mitigation techniques, our results call for\nfurther scrutiny about their efficacy in the context of VQAs with qudits.\n","authors":["Lucas Friedrich","Tiago de Souza Farias","Jonas Maziero"],"pdf_url":"https://arxiv.org/pdf/2405.08190v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.04176v2","updated":"2025-04-21T12:02:08Z","published":"2025-02-06T16:07:24Z","title":"MRAMG-Bench: A Comprehensive Benchmark for Advancing Multimodal\n  Retrieval-Augmented Multimodal Generation","summary":"  Recent advances in Retrieval-Augmented Generation (RAG) have significantly\nimproved response accuracy and relevance by incorporating external knowledge\ninto Large Language Models (LLMs). However, existing RAG methods primarily\nfocus on generating text-only answers, even in Multimodal Retrieval-Augmented\nGeneration (MRAG) scenarios, where multimodal elements are retrieved to assist\nin generating text answers. To address this, we introduce the Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) task, in which we aim to\ngenerate multimodal answers that combine both text and images, fully leveraging\nthe multimodal data within a corpus. Despite growing attention to this\nchallenging task, a notable lack of a comprehensive benchmark persists for\neffectively evaluating its performance. To bridge this gap, we provide\nMRAMG-Bench, a meticulously curated, human-annotated benchmark comprising 4,346\ndocuments, 14,190 images, and 4,800 QA pairs, distributed across six distinct\ndatasets and spanning three domains: Web, Academia, and Lifestyle. The datasets\nincorporate diverse difficulty levels and complex multi-image scenarios,\nproviding a robust foundation for evaluating the MRAMG task. To facilitate\nrigorous evaluation, MRAMG-Bench incorporates a comprehensive suite of both\nstatistical and LLM-based metrics, enabling a thorough analysis of the\nperformance of generative models in the MRAMG task. Additionally, we propose an\nefficient and flexible multimodal answer generation framework that can leverage\nLLMs/MLLMs to generate multimodal responses. Our datasets and complete\nevaluation results for 11 popular generative models are available at\nhttps://github.com/MRAMG-Bench/MRAMG.\n","authors":["Qinhan Yu","Zhiyou Xiao","Binghui Li","Zhengren Wang","Chong Chen","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.04176v2.pdf","comment":"Published as a conference paper at SIGIR 2025; 11 pages"},{"id":"http://arxiv.org/abs/2504.15037v1","updated":"2025-04-21T11:48:39Z","published":"2025-04-21T11:48:39Z","title":"A Call for New Recipes to Enhance Spatial Reasoning in MLLMs","summary":"  Multimodal Large Language Models (MLLMs) have demonstrated impressive\nperformance in general vision-language tasks. However, recent studies have\nexposed critical limitations in their spatial reasoning capabilities. This\ndeficiency in spatial reasoning significantly constrains MLLMs' ability to\ninteract effectively with the physical world, thereby limiting their broader\napplications. We argue that spatial reasoning capabilities will not naturally\nemerge from merely scaling existing architectures and training methodologies.\nInstead, this challenge demands dedicated attention to fundamental\nmodifications in the current MLLM development approach. In this position paper,\nwe first establish a comprehensive framework for spatial reasoning within the\ncontext of MLLMs. We then elaborate on its pivotal role in real-world\napplications. Through systematic analysis, we examine how individual components\nof the current methodology-from training data to reasoning mechanisms-influence\nspatial reasoning capabilities. This examination reveals critical limitations\nwhile simultaneously identifying promising avenues for advancement. Our work\naims to direct the AI research community's attention toward these crucial yet\nunderexplored aspects. By highlighting these challenges and opportunities, we\nseek to catalyze progress toward achieving human-like spatial reasoning\ncapabilities in MLLMs.\n","authors":["Huanyu Zhang","Chengzu Li","Wenshan Wu","Shaoguang Mao","Yan xia","Ivan Vulić","Zhang Zhang","Liang Wang","Tieniu Tan","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2504.15037v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04756v2","updated":"2025-04-21T11:42:20Z","published":"2025-04-07T06:08:59Z","title":"Continuous Locomotive Crowd Behavior Generation","summary":"  Modeling and reproducing crowd behaviors are important in various domains\nincluding psychology, robotics, transport engineering and virtual environments.\nConventional methods have focused on synthesizing momentary scenes, which have\ndifficulty in replicating the continuous nature of real-world crowds. In this\npaper, we introduce a novel method for automatically generating continuous,\nrealistic crowd trajectories with heterogeneous behaviors and interactions\namong individuals. We first design a crowd emitter model. To do this, we obtain\nspatial layouts from single input images, including a segmentation map,\nappearance map, population density map and population probability, prior to\ncrowd generation. The emitter then continually places individuals on the\ntimeline by assigning independent behavior characteristics such as agents'\ntype, pace, and start/end positions using diffusion models. Next, our crowd\nsimulator produces their long-term locomotions. To simulate diverse actions, it\ncan augment their behaviors based on a Markov chain. As a result, our overall\nframework populates the scenes with heterogeneous crowd behaviors by\nalternating between the proposed emitter and simulator. Note that all the\ncomponents in the proposed framework are user-controllable. Lastly, we propose\na benchmark protocol to evaluate the realism and quality of the generated\ncrowds in terms of the scene-level population dynamics and the individual-level\ntrajectory accuracy. We demonstrate that our approach effectively models\ndiverse crowd behavior patterns and generalizes well across different\ngeographical environments. Code is publicly available at\nhttps://github.com/InhwanBae/CrowdES .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2504.04756v2.pdf","comment":"Accepted at CVPR 2025. Project page:\n  https://ihbae.com/publication/crowdes/"},{"id":"http://arxiv.org/abs/2410.14592v2","updated":"2025-04-21T11:13:32Z","published":"2024-10-18T16:43:10Z","title":"Contractivity and linear convergence in bilinear saddle-point problems:\n  An operator-theoretic approach","summary":"  We study the convex-concave bilinear saddle-point problem $\\min_x \\max_y f(x)\n+ y^\\top Ax - g(y)$, where both, only one, or none of the functions $f$ and $g$\nare strongly convex, and suitable rank conditions on the matrix $A$ hold. The\nsolution of this problem is at the core of many machine learning tasks. By\nemploying tools from monotone operator theory, we systematically prove the\ncontractivity (in turn, the linear convergence) of several first-order\nprimal-dual algorithms, including the Chambolle-Pock method. Our approach\nresults in concise proofs, and it yields new convergence guarantees and tighter\nbounds compared to known results.\n","authors":["Colin Dirren","Mattia Bianchi","Panagiotis D. Grontas","John Lygeros","Florian Dörfler"],"pdf_url":"https://arxiv.org/pdf/2410.14592v2.pdf","comment":"AISTATS 2025"},{"id":"http://arxiv.org/abs/2408.11052v3","updated":"2025-04-21T11:10:56Z","published":"2024-08-20T17:58:40Z","title":"Accelerating Goal-Conditioned RL Algorithms and Research","summary":"  Self-supervision has the potential to transform reinforcement learning (RL),\nparalleling the breakthroughs it has enabled in other areas of machine\nlearning. While self-supervised learning in other domains aims to find patterns\nin a fixed dataset, self-supervised goal-conditioned reinforcement learning\n(GCRL) agents discover new behaviors by learning from the goals achieved during\nunstructured interaction with the environment. However, these methods have\nfailed to see similar success, both due to a lack of data from slow environment\nsimulations as well as a lack of stable algorithms. We take a step toward\naddressing both of these issues by releasing a high-performance codebase and\nbenchmark (JaxGCRL) for self-supervised GCRL, enabling researchers to train\nagents for millions of environment steps in minutes on a single GPU. By\nutilizing GPU-accelerated replay buffers, environments, and a stable\ncontrastive RL algorithm, we reduce training time by up to $22\\times$.\nAdditionally, we assess key design choices in contrastive RL, identifying those\nthat most effectively stabilize and enhance training performance. With this\napproach, we provide a foundation for future research in self-supervised GCRL,\nenabling researchers to quickly iterate on new ideas and evaluate them in\ndiverse and challenging environments. Website + Code:\nhttps://github.com/MichalBortkiewicz/JaxGCRL\n","authors":["Michał Bortkiewicz","Władysław Pałucki","Vivek Myers","Tadeusz Dziarmaga","Tomasz Arczewski","Łukasz Kuciński","Benjamin Eysenbach"],"pdf_url":"https://arxiv.org/pdf/2408.11052v3.pdf","comment":"Website: https://michalbortkiewicz.github.io/JaxGCRL/ Code:\n  https://github.com/MichalBortkiewicz/JaxGCRL"},{"id":"http://arxiv.org/abs/2504.15021v1","updated":"2025-04-21T11:09:43Z","published":"2025-04-21T11:09:43Z","title":"Is Intelligence the Right Direction in New OS Scheduling for Multiple\n  Resources in Cloud Environments?","summary":"  Making it intelligent is a promising way in System/OS design. This paper\nproposes OSML+, a new ML-based resource scheduling mechanism for co-located\ncloud services. OSML+ intelligently schedules the cache and main memory\nbandwidth resources at the memory hierarchy and the computing core resources\nsimultaneously. OSML+ uses a multi-model collaborative learning approach during\nits scheduling and thus can handle complicated cases, e.g., avoiding resource\ncliffs, sharing resources among applications, enabling different scheduling\npolicies for applications with different priorities, etc. OSML+ can converge\nfaster using ML models than previous studies. Moreover, OSML+ can automatically\nlearn on the fly and handle dynamically changing workloads accordingly. Using\ntransfer learning technologies, we show our design can work well across various\ncloud servers, including the latest off-the-shelf large-scale servers. Our\nexperimental results show that OSML+ supports higher loads and meets QoS\ntargets with lower overheads than previous studies.\n","authors":["Xinglei Dou","Lei Liu","Limin Xiao"],"pdf_url":"https://arxiv.org/pdf/2504.15021v1.pdf","comment":"25 pages, 14 figures, to be published in ACM Transactions on Storage"},{"id":"http://arxiv.org/abs/2501.15544v3","updated":"2025-04-21T11:09:11Z","published":"2025-01-26T14:31:03Z","title":"Advancing Generative Artificial Intelligence and Large Language Models\n  for Demand Side Management with Internet of Electric Vehicles","summary":"  Generative artificial intelligence, particularly through large language\nmodels (LLMs), is poised to transform energy optimization and demand side\nmanagement (DSM) within microgrids. This paper explores the integration of LLMs\ninto energy management, emphasizing their roles in automating the optimization\nof DSM strategies with Internet of electric vehicles. We investigate challenges\nand solutions associated with DSM and explore the new opportunities presented\nby leveraging LLMs. Then, we propose an innovative solution that enhances LLMs\nwith retrieval-augmented generation for automatic problem formulation, code\ngeneration, and customizing optimization. We present a case study to\ndemonstrate the effectiveness of our proposed solution in charging scheduling\nand optimization for electric vehicles, highlighting our solution's significant\nadvancements in energy efficiency and user adaptability. This work underscores\nthe potential of LLMs for energy optimization and fosters a new era of\nintelligent DSM solutions.\n","authors":["Hanwen Zhang","Ruichen Zhang","Wei Zhang","Dusit Niyato","Yonggang Wen"],"pdf_url":"https://arxiv.org/pdf/2501.15544v3.pdf","comment":"9 Pages"},{"id":"http://arxiv.org/abs/2410.20197v3","updated":"2025-04-21T10:57:32Z","published":"2024-10-26T15:04:04Z","title":"Transferable Adversarial Attacks on SAM and Its Downstream Models","summary":"  The utilization of large foundational models has a dilemma: while fine-tuning\ndownstream tasks from them holds promise for making use of the well-generalized\nknowledge in practical applications, their open accessibility also poses\nthreats of adverse usage. This paper, for the first time, explores the\nfeasibility of adversarial attacking various downstream models fine-tuned from\nthe segment anything model (SAM), by solely utilizing the information from the\nopen-sourced SAM. In contrast to prevailing transfer-based adversarial attacks,\nwe demonstrate the existence of adversarial dangers even without accessing the\ndownstream task and dataset to train a similar surrogate model. To enhance the\neffectiveness of the adversarial attack towards models fine-tuned on unknown\ndatasets, we propose a universal meta-initialization (UMI) algorithm to extract\nthe intrinsic vulnerability inherent in the foundation model, which is then\nutilized as the prior knowledge to guide the generation of adversarial\nperturbations. Moreover, by formulating the gradient difference in the\nattacking process between the open-sourced SAM and its fine-tuned downstream\nmodels, we theoretically demonstrate that a deviation occurs in the adversarial\nupdate direction by directly maximizing the distance of encoded feature\nembeddings in the open-sourced SAM. Consequently, we propose a gradient robust\nloss that simulates the associated uncertainty with gradient-based noise\naugmentation to enhance the robustness of generated adversarial examples (AEs)\ntowards this deviation, thus improving the transferability. Extensive\nexperiments demonstrate the effectiveness of the proposed universal\nmeta-initialized and gradient robust adversarial attack (UMI-GRAT) toward SAMs\nand their downstream models. Code is available at\nhttps://github.com/xiasong0501/GRAT.\n","authors":["Song Xia","Wenhan Yang","Yi Yu","Xun Lin","Henghui Ding","Ling-Yu Duan","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2410.20197v3.pdf","comment":"update fig 1"},{"id":"http://arxiv.org/abs/2411.06291v3","updated":"2025-04-21T10:22:14Z","published":"2024-11-09T21:26:59Z","title":"TinyML NLP Scheme for Semantic Wireless Sentiment Classification with\n  Privacy Preservation","summary":"  Natural Language Processing (NLP) operations, such as semantic sentiment\nanalysis and text synthesis, often raise privacy concerns and demand\nsignificant on-device computational resources. Centralized learning (CL) on the\nedge provides an energy-efficient alternative but requires collecting raw data,\ncompromising user privacy. While federated learning (FL) enhances privacy, it\nimposes high computational energy demands on resource-constrained devices. This\nstudy provides insights into deploying privacy-preserving, energy-efficient NLP\nmodels on edge devices. We introduce semantic split learning (SL) as an\nenergy-efficient, privacy-preserving tiny machine learning (TinyML) framework\nand compare it to FL and CL in the presence of Rayleigh fading and additive\nnoise. Our results show that SL significantly reduces computational power and\nCO2 emissions while enhancing privacy, as evidenced by a fourfold increase in\nreconstruction error compared to FL and nearly eighteen times that of CL. In\ncontrast, FL offers a balanced trade-off between privacy and efficiency. Our\ncode is available for replication at our GitHub repository:\nhttps://github.com/AhmedRadwan02/TinyEco2AI-NLP.\n","authors":["Ahmed Y. Radwan","Mohammad Shehab","Mohamed-Slim Alouini"],"pdf_url":"https://arxiv.org/pdf/2411.06291v3.pdf","comment":"Accepted at EuCNC & 6G Summit 2025"},{"id":"http://arxiv.org/abs/2410.10796v3","updated":"2025-04-21T10:19:21Z","published":"2024-10-14T17:57:09Z","title":"Context-Parametric Inversion: Why Instruction Finetuning Can Worsen\n  Context Reliance","summary":"  A standard practice when using large language models is for users to\nsupplement their instruction with an input context containing new information\nfor the model to process. However, models struggle to reliably follow the input\ncontext, especially when it conflicts with their parametric knowledge from\npretraining. In-principle, one would expect models to adapt to the user context\nbetter after instruction finetuning, particularly when handling knowledge\nconflicts. However, we observe a surprising failure mode: during instruction\ntuning, the context reliance under knowledge conflicts initially increases as\nexpected, but then gradually decreases as instruction finetuning progresses.\nThis happens while the performance on standard benchmarks keeps on increasing\nfar after this drop. We call this phenomenon context-parametric inversion and\nobserve it across multiple general purpose instruction tuning datasets such as\nTULU, Alpaca and Ultrachat, across different model families like Llama,\nMistral, and Pythia. We perform various controlled studies and theoretical\nanalysis to show that context-parametric inversion occurs due to examples in\nthe instruction finetuning data where the input context provides information\nthat aligns with model's parametric knowledge. Our analysis suggests some\nnatural mitigation strategies with limited but insightful gains, and serves as\na useful starting point in addressing this deficiency in instruction\nfinetuning.\n","authors":["Sachin Goyal","Christina Baek","J. Zico Kolter","Aditi Raghunathan"],"pdf_url":"https://arxiv.org/pdf/2410.10796v3.pdf","comment":"Published at ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2504.14995v1","updated":"2025-04-21T09:51:39Z","published":"2025-04-21T09:51:39Z","title":"Trainable Quantum Neural Network for Multiclass Image Classification\n  with the Power of Pre-trained Tree Tensor Networks","summary":"  Tree tensor networks (TTNs) offer powerful models for image classification.\nWhile these TTN image classifiers already show excellent performance on\nclassical hardware, embedding them into quantum neural networks (QNNs) may\nfurther improve the performance by leveraging quantum resources. However,\nembedding TTN classifiers into QNNs for multiclass classification remains\nchallenging. Key obstacles are the highorder gate operations required for large\nbond dimensions and the mid-circuit postselection with exponentially low\nsuccess rates necessary for the exact embedding. In this work, to address these\nchallenges, we propose forest tensor network (FTN)-classifiers, which aggregate\nmultiple small-bond-dimension TTNs. This allows us to handle multiclass\nclassification without requiring large gates in the embedded circuits. We then\nremove the overhead of mid-circuit postselection by extending the adiabatic\nencoding framework to our setting and smoothly encode the FTN-classifiers into\na quantum forest tensor network (qFTN)- classifiers. Numerical experiments on\nMNIST and CIFAR-10 demonstrate that we can successfully train FTN-classifiers\nand encode them into qFTN-classifiers, while maintaining or even improving the\nperformance of the pre-trained FTN-classifiers. These results suggest that\nsynergy between TTN classification models and QNNs can provide a robust and\nscalable framework for multiclass quantum-enhanced image classification.\n","authors":["Keisuke Murota","Takumi Kobori"],"pdf_url":"https://arxiv.org/pdf/2504.14995v1.pdf","comment":"11 pages, 12 figures, 2 tables. This work has been submitted to the\n  IEEE for possible publication"},{"id":"http://arxiv.org/abs/2504.14994v1","updated":"2025-04-21T09:51:24Z","published":"2025-04-21T09:51:24Z","title":"Learning Compositional Transferability of Time Series for Source-Free\n  Domain Adaptation","summary":"  Domain adaptation is challenging for time series classification due to the\nhighly dynamic nature. This study tackles the most difficult subtask when both\ntarget labels and source data are inaccessible, namely, source-free domain\nadaptation. To reuse the classification backbone pre-trained on source data,\ntime series reconstruction is a sound solution that aligns target and source\ntime series by minimizing the reconstruction errors of both. However, simply\nfine-tuning the source pre-trained reconstruction model on target data may lose\nthe learnt priori, and it struggles to accommodate domain varying temporal\npatterns in a single encoder-decoder. Therefore, this paper tries to\ndisentangle the composition of domain transferability by using a compositional\narchitecture for time series reconstruction. Here, the preceding component is a\nU-net frozen since pre-trained, the output of which during adaptation is the\ninitial reconstruction of a given target time series, acting as a coarse step\nto prompt the subsequent finer adaptation. The following pipeline for finer\nadaptation includes two parallel branches: The source replay branch using a\nresidual link to preserve the output of U-net, and the offset compensation\nbranch that applies an additional autoencoder (AE) to further warp U-net's\noutput. By deploying a learnable factor on either branch to scale their\ncomposition in the final output of reconstruction, the data transferability is\ndisentangled and the learnt reconstructive capability from source data is\nretained. During inference, aside from the batch-level optimization in the\ntraining, we search at test time stability-aware rescaling of source replay\nbranch to tolerate instance-wise variation. The experimental results show that\nsuch compositional architecture of time series reconstruction leads to SOTA\nperformance on 3 widely used benchmarks.\n","authors":["Hankang Sun","Guiming Li","Su Yang","Baoqi Li"],"pdf_url":"https://arxiv.org/pdf/2504.14994v1.pdf","comment":"Corresponding author: Su Yang"},{"id":"http://arxiv.org/abs/2411.03312v2","updated":"2025-04-21T09:34:59Z","published":"2024-11-05T18:54:21Z","title":"Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters","summary":"  Vision Language Models (VLMs) have demonstrated strong capabilities across\nvarious visual understanding and reasoning tasks, driven by incorporating image\nrepresentations into the token inputs of Large Language Models (LLMs). However,\ntheir real-world deployment is often constrained by high latency during\ninference due to the substantial compute required by the LLM to process the\nlarge number of input tokens, predominantly arising from the image. To reduce\ninference costs, one can either downsize the LLM or reduce the number of input\ntokens needed to represent the image, the latter of which has been the focus of\nmany recent efforts around token compression. However, it is unclear what the\noptimal trade-off is given a fixed inference budget. We first characterize this\noptimal trade-off between the number of visual tokens and LLM parameters by\nestablishing scaling laws that capture variations in performance with these two\nfactors. Our results reveal a surprising trend: for visual reasoning tasks, the\ninference-optimal behavior in VLMs is achieved by using the largest LLM that\nfits within the inference budget while minimizing visual token count - often to\na single token. While the token reduction literature has mainly focused on\nmaintaining base model performance by modestly reducing the token count (e.g.,\n$5-10\\times$), our results indicate that the compute-optimal inference regime\nrequires operating under even higher token compression ratios. Based on these\ninsights, we take the first steps toward designing token compression algorithms\ntailored for high-compression settings, utilizing prompt-based compression of\ntokens. Our work underscores the performance and efficiency benefits of\noperating in low visual token regimes and the importance of developing tailored\ntoken reduction algorithms for such conditions. Code is available at\nhttps://github.com/locuslab/llava-token-compression.\n","authors":["Kevin Y. Li","Sachin Goyal","Joao D. Semedo","J. Zico Kolter"],"pdf_url":"https://arxiv.org/pdf/2411.03312v2.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2407.03185v2","updated":"2025-04-21T08:58:24Z","published":"2024-07-03T15:07:16Z","title":"Multiple-Resolution Tokenization for Time Series Forecasting with an\n  Application to Pricing","summary":"  We propose a transformer architecture for time series forecasting with a\nfocus on time series tokenisation and apply it to a real-world prediction\nproblem from the pricing domain. Our architecture aims to learn effective\nrepresentations at many scales across all available data simultaneously. The\nmodel contains a number of novel modules: a differentiated form of time series\npatching which employs multiple resolutions, a multiple-resolution module for\ntime-varying known variables, a mixer-based module for capturing cross-series\ninformation, and a novel output head with favourable scaling to account for the\nincreased number of tokens. We present an application of this model to a real\nworld prediction problem faced by the markdown team at a very large retailer.\nOn the experiments conducted our model outperforms in-house models and the\nselected existing deep learning architectures.\n","authors":["Egon Peršak","Miguel F. Anjos","Sebastian Lautz","Aleksandar Kolev"],"pdf_url":"https://arxiv.org/pdf/2407.03185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14963v1","updated":"2025-04-21T08:44:33Z","published":"2025-04-21T08:44:33Z","title":"Speaker Fuzzy Fingerprints: Benchmarking Text-Based Identification in\n  Multiparty Dialogues","summary":"  Speaker identification using voice recordings leverages unique acoustic\nfeatures, but this approach fails when only textual data is available. Few\napproaches have attempted to tackle the problem of identifying speakers solely\nfrom text, and the existing ones have primarily relied on traditional methods.\nIn this work, we explore the use of fuzzy fingerprints from large pre-trained\nmodels to improve text-based speaker identification. We integrate\nspeaker-specific tokens and context-aware modeling, demonstrating that\nconversational context significantly boosts accuracy, reaching 70.6% on the\nFriends dataset and 67.7% on the Big Bang Theory dataset. Additionally, we show\nthat fuzzy fingerprints can approximate full fine-tuning performance with fewer\nhidden units, offering improved interpretability. Finally, we analyze ambiguous\nutterances and propose a mechanism to detect speaker-agnostic lines. Our\nfindings highlight key challenges and provide insights for future improvements\nin text-based speaker identification.\n","authors":["Rui Ribeiro","Luísa Coheur","Joao P. Carvalho"],"pdf_url":"https://arxiv.org/pdf/2504.14963v1.pdf","comment":"Paper accepted at the FUZZY IEEE 2025 conference"},{"id":"http://arxiv.org/abs/2504.14960v1","updated":"2025-04-21T08:39:47Z","published":"2025-04-21T08:39:47Z","title":"MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient\n  Large-Scale MoE Model Training with Megatron Core","summary":"  Mixture of Experts (MoE) models enhance neural network scalability by\ndynamically selecting relevant experts per input token, enabling larger model\nsizes while maintaining manageable computation costs. However, efficient\ntraining of large-scale MoE models across thousands of GPUs presents\nsignificant challenges due to limitations in existing parallelism strategies.\nWe introduce an end-to-end training framework for large-scale MoE models that\nutilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert\nParallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism.\nCentral to our approach is MoE Parallel Folding, a novel strategy that\ndecouples the parallelization of attention and MoE layers in Transformer\nmodels, allowing each layer type to adopt optimal parallel configurations.\nAdditionally, we develop a flexible token-level dispatcher that supports both\ntoken-dropping and token-dropless MoE training across all five dimensions of\nparallelism. This dispatcher accommodates dynamic tensor shapes and coordinates\ndifferent parallelism schemes for Attention and MoE layers, facilitating\ncomplex parallelism implementations. Our experiments demonstrate significant\nimprovements in training efficiency and scalability. We achieve up to 49.3%\nModel Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the\nQwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The\nframework scales efficiently up to 1,024 GPUs and maintains high performance\nwith sequence lengths up to 128K tokens, validating its effectiveness for\nlarge-scale MoE model training. The code is available in Megatron-Core.\n","authors":["Dennis Liu","Zijie Yan","Xin Yao","Tong Liu","Vijay Korthikanti","Evan Wu","Shiqing Fan","Gao Deng","Hongxiao Bai","Ashwath Aithal","Michael Andersch","Mohammad Shoeybi","Jiajie Yao","Chandler Zhou","David Wu","Xipeng Li","June Yang"],"pdf_url":"https://arxiv.org/pdf/2504.14960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09269v2","updated":"2025-04-21T08:35:02Z","published":"2024-08-17T18:53:17Z","title":"Enhancing Audio-Language Models through Self-Supervised Post-Training\n  with Text-Audio Pairs","summary":"  Research on multi-modal contrastive learning strategies for audio and text\nhas rapidly gained interest. Contrastively trained Audio-Language Models\n(ALMs), such as CLAP, which establish a unified representation across audio and\nlanguage modalities, have enhanced the efficacy in various subsequent tasks by\nproviding good text aligned audio encoders and vice versa. These improvements\nare evident in areas like zero-shot audio classification and audio retrieval,\namong others. However, the ability of these models to understand natural\nlanguage and temporal relations is still a largely unexplored and open field\nfor research. In this paper, we propose to equip the multi-modal ALMs with\ntemporal understanding without loosing their inherent prior capabilities of\naudio-language tasks with a temporal instillation method TeminAL. We implement\na two-stage training scheme TeminAL A $\\&$ B, where the model first learns to\ndifferentiate between multiple sounds in TeminAL A, followed by a phase that\ninstills a sense of time, thereby enhancing its temporal understanding in\nTeminAL B. This approach results in an average performance gain of $5.28\\%$ in\ntemporal understanding on the ESC-50 dataset, while the model remains\ncompetitive in zero-shot retrieval and classification tasks on the\nAudioCap/Clotho datasets. We also note the lack of proper evaluation techniques\nfor contrastive ALMs and propose a strategy for evaluating ALMs in zero-shot\nsettings. The general-purpose zero-shot model evaluation strategy ZSTE, is used\nto evaluate various prior models. ZSTE demonstrates a general strategy to\nevaluate all ZS contrastive models. The model trained with TeminAL successfully\noutperforms current models on most downstream tasks.\n","authors":["Anshuman Sinha","Camille Migozzi","Aubin Rey","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.09269v2.pdf","comment":"29 pages, 15 figures"},{"id":"http://arxiv.org/abs/2504.14955v1","updated":"2025-04-21T08:27:26Z","published":"2025-04-21T08:27:26Z","title":"Efficient Document Retrieval with G-Retriever","summary":"  Textual data question answering has gained significant attention due to its\ngrowing applicability. Recently, a novel approach leveraging the\nRetrieval-Augmented Generation (RAG) method was introduced, utilizing the\nPrize-Collecting Steiner Tree (PCST) optimization for sub-graph construction.\nHowever, this method focused solely on node attributes, leading to incomplete\ncontextual understanding. In this paper, we propose an enhanced approach that\nreplaces the PCST method with an attention-based sub-graph construction\ntechnique, enabling more efficient and context-aware retrieval. Additionally,\nwe encode both node and edge attributes, leading to richer graph\nrepresentations. Our method also incorporates an improved projection layer and\nmulti-head attention pooling for better alignment with Large Language Models\n(LLMs). Experimental evaluations on the WebQSP dataset demonstrate that our\napproach is competitive and achieves marginally better results compared to the\noriginal method, underscoring its potential for more accurate question\nanswering.\n","authors":["Manthankumar Solanki"],"pdf_url":"https://arxiv.org/pdf/2504.14955v1.pdf","comment":"Extended version of a paper presented at NeurIPS 2024\n  (arXiv:2402.07630)"},{"id":"http://arxiv.org/abs/2504.14946v1","updated":"2025-04-21T08:09:40Z","published":"2025-04-21T08:09:40Z","title":"Symmetry-Preserving Architecture for Multi-NUMA Environments (SPANE): A\n  Deep Reinforcement Learning Approach for Dynamic VM Scheduling","summary":"  As cloud computing continues to evolve, the adoption of multi-NUMA\n(Non-Uniform Memory Access) architecture by cloud service providers has\nintroduced new challenges in virtual machine (VM) scheduling. To address these\nchallenges and more accurately reflect the complexities faced by modern cloud\nenvironments, we introduce the Dynamic VM Allocation problem in Multi-NUMA PM\n(DVAMP). We formally define both offline and online versions of DVAMP as\nmixed-integer linear programming problems, providing a rigorous mathematical\nfoundation for analysis. A tight performance bound for greedy online algorithms\nis derived, offering insights into the worst-case optimality gap as a function\nof the number of physical machines and VM lifetime variability. To address the\nchallenges posed by DVAMP, we propose SPANE (Symmetry-Preserving Architecture\nfor Multi-NUMA Environments), a novel deep reinforcement learning approach that\nexploits the problem's inherent symmetries. SPANE produces invariant results\nunder arbitrary permutations of physical machine states, enhancing learning\nefficiency and solution quality. Extensive experiments conducted on the\nHuawei-East-1 dataset demonstrate that SPANE outperforms existing baselines,\nreducing average VM wait time by 45%. Our work contributes to the field of\ncloud resource management by providing both theoretical insights and practical\nsolutions for VM scheduling in multi-NUMA environments, addressing a critical\ngap in the literature and offering improved performance for real-world cloud\nsystems.\n","authors":["Tin Ping Chan","Yunlong Cheng","Yizhan Zhu","Xiaofeng Gao","Guihai Chen"],"pdf_url":"https://arxiv.org/pdf/2504.14946v1.pdf","comment":"10 pages, 7 figures. Accepted to IEEE INFOCOM 2025"},{"id":"http://arxiv.org/abs/2504.14945v1","updated":"2025-04-21T08:09:13Z","published":"2025-04-21T08:09:13Z","title":"Learning to Reason under Off-Policy Guidance","summary":"  Recent advances in large reasoning models (LRMs) demonstrate that\nsophisticated behaviors such as multi-step reasoning and self-reflection can\nemerge via reinforcement learning (RL) with simple rule-based rewards. However,\nexisting zero-RL approaches are inherently ``on-policy'', limiting learning to\na model's own outputs and failing to acquire reasoning abilities beyond its\ninitial capabilities. We introduce LUFFY (Learning to reason Under oFF-policY\nguidance), a framework that augments zero-RL with off-policy reasoning traces.\nLUFFY dynamically balances imitation and exploration by combining off-policy\ndemonstrations with on-policy rollouts during training. Notably, we propose\npolicy shaping via regularized importance sampling to avoid superficial and\nrigid imitation during mixed-policy training. Remarkably, LUFFY achieves an\nover +7.0 average gain across six math benchmarks and an advantage of over +6.2\npoints in out-of-distribution tasks. It also substantially surpasses\nimitation-based supervised fine-tuning (SFT), particularly in generalization.\nAnalysis shows LUFFY not only imitates effectively but also explores beyond\ndemonstrations, offering a scalable path to train generalizable reasoning\nmodels with off-policy guidance.\n","authors":["Jianhao Yan","Yafu Li","Zican Hu","Zhi Wang","Ganqu Cui","Xiaoye Qu","Yu Cheng","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.14945v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2404.09243v2","updated":"2025-04-21T08:07:50Z","published":"2024-04-14T13:08:21Z","title":"Learning Self-Growth Maps for Fast and Accurate Imbalanced Streaming\n  Data Clustering","summary":"  Streaming data clustering is a popular research topic in data mining and\nmachine learning. Since streaming data is usually analyzed in data chunks, it\nis more susceptible to encounter the dynamic cluster imbalance issue. That is,\nthe imbalance ratio of clusters changes over time, which can easily lead to\nfluctuations in either the accuracy or the efficiency of streaming data\nclustering. Therefore, we propose an accurate and efficient streaming data\nclustering approach to adapt the drifting and imbalanced cluster distributions.\nWe first design a Self-Growth Map (SGM) that can automatically arrange neurons\non demand according to local distribution, and thus achieve fast and\nincremental adaptation to the streaming distributions. Since SGM allocates an\nexcess number of density-sensitive neurons to describe the global distribution,\nit can avoid missing small clusters among imbalanced distributions. We also\npropose a fast hierarchical merging strategy to combine the neurons that break\nup the relatively large clusters. It exploits the maintained SGM to quickly\nretrieve the intra-cluster distribution pairs for merging, which circumvents\nthe most laborious global searching. It turns out that the proposed SGM can\nincrementally adapt to the distributions of new chunks, and the Self-grOwth\nmap-guided Hierarchical merging for Imbalanced data clustering (SOHI) approach\ncan quickly explore a true number of imbalanced clusters. Extensive experiments\ndemonstrate that SOHI can efficiently and accurately explore cluster\ndistributions for streaming data.\n","authors":["Yiqun Zhang","Sen Feng","Pengkai Wang","Zexi Tan","Xiaopeng Luo","Yuzhu Ji","Rong Zou","Yiu-ming Cheung"],"pdf_url":"https://arxiv.org/pdf/2404.09243v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07155v4","updated":"2025-04-21T08:07:07Z","published":"2025-01-13T09:28:47Z","title":"AlphaNet: Scaling Up Local-frame-based Atomistic Interatomic Potential","summary":"  Molecular dynamics simulations demand an unprecedented combination of\naccuracy and scalability to tackle grand challenges in catalysis and materials\ndesign. To bridge this gap, we present AlphaNet, a local-frame-based\nequivariant model that simultaneously improves computational efficiency and\npredictive precision for interatomic interactions. By constructing equivariant\nlocal frames with learnable geometric transitions, AlphaNet encodes atomic\nenvironments with enhanced representational capacity, achieving\nstate-of-the-art accuracy in energy and force predictions. Extensive benchmarks\non large-scale datasets spanning molecular reactions, crystal stability, and\nsurface catalysis (Matbench Discovery and OC2M) demonstrate its superior\nperformance over existing neural network interatomic potentials while ensuring\nscalability across diverse system sizes with varying types of interatomic\ninteractions. The synergy of accuracy, efficiency, and transferability\npositions AlphaNet as a transformative tool for modeling multiscale phenomena,\ndecoding dynamics in catalysis and functional interfaces, with direct\nimplications for accelerating the discovery of complex molecular systems and\nfunctional materials.\n","authors":["Bangchen Yin","Jiaao Wang","Weitao Du","Pengbo Wang","Penghua Ying","Haojun Jia","Zisheng Zhang","Yuanqi Du","Carla P. Gomes","Chenru Duan","Graeme Henkelman","Hai Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.07155v4.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.14938v1","updated":"2025-04-21T08:01:44Z","published":"2025-04-21T08:01:44Z","title":"Integrating Response Time and Attention Duration in Bayesian Preference\n  Learning for Multiple Criteria Decision Aiding","summary":"  We introduce a multiple criteria Bayesian preference learning framework\nincorporating behavioral cues for decision aiding. The framework integrates\npairwise comparisons, response time, and attention duration to deepen insights\ninto decision-making processes. The approach employs an additive value function\nmodel and utilizes a Bayesian framework to derive the posterior distribution of\npotential ranking models by defining the likelihood of observed preference data\nand specifying a prior on the preference structure. This distribution\nhighlights each model's ability to reconstruct Decision-Makers' holistic\npairwise comparisons. By leveraging both response time as a proxy for cognitive\neffort and alternative discriminability as well as attention duration as an\nindicator of criterion importance, the proposed model surpasses traditional\nmethods by uncovering richer behavioral patterns. We report the results of a\nlaboratory experiment on mobile phone contract selection involving 30 real\nsubjects using a dedicated application with time-, eye-, and mouse-tracking\ncomponents. We validate the novel method's ability to reconstruct complete\npreferences. The detailed ablation studies reveal time- and attention-related\nbehavioral patterns, confirming that integrating comprehensive data leads to\ndeveloping models that better align with the DM's actual preferences.\n","authors":["Jiaxuan Jiang","Jiapeng Liu","Miłosz Kadziński","Xiuwu Liao","Jingyu Dong"],"pdf_url":"https://arxiv.org/pdf/2504.14938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14937v1","updated":"2025-04-21T08:01:32Z","published":"2025-04-21T08:01:32Z","title":"Causal DAG Summarization (Full Version)","summary":"  Causal inference aids researchers in discovering cause-and-effect\nrelationships, leading to scientific insights. Accurate causal estimation\nrequires identifying confounding variables to avoid false discoveries. Pearl's\ncausal model uses causal DAGs to identify confounding variables, but incorrect\nDAGs can lead to unreliable causal conclusions. However, for high dimensional\ndata, the causal DAGs are often complex beyond human verifiability. Graph\nsummarization is a logical next step, but current methods for general-purpose\ngraph summarization are inadequate for causal DAG summarization. This paper\naddresses these challenges by proposing a causal graph summarization objective\nthat balances graph simplification for better understanding while retaining\nessential causal information for reliable inference. We develop an efficient\ngreedy algorithm and show that summary causal DAGs can be directly used for\ninference and are more robust to misspecification of assumptions, enhancing\nrobustness for causal inference. Experimenting with six real-life datasets, we\ncompared our algorithm to three existing solutions, showing its effectiveness\nin handling high-dimensional data and its ability to generate summary DAGs that\nensure both reliable causal inference and robustness against misspecifications.\n","authors":["Anna Zeng","Michael Cafarella","Batya Kenig","Markos Markakis","Brit Youngmann","Babak Salimi"],"pdf_url":"https://arxiv.org/pdf/2504.14937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14278v2","updated":"2025-04-21T07:58:09Z","published":"2025-01-24T06:46:58Z","title":"Active Learning for Continual Learning: Keeping the Past Alive in the\n  Present","summary":"  Continual learning (CL) enables deep neural networks to adapt to\never-changing data distributions. In practice, there may be scenarios where\nannotation is costly, leading to active continual learning (ACL), which\nperforms active learning (AL) for the CL scenarios when reducing the labeling\ncost by selecting the most informative subset is preferable. However,\nconventional AL strategies are not suitable for ACL, as they focus solely on\nlearning the new knowledge, leading to catastrophic forgetting of previously\nlearned tasks. Therefore, ACL requires a new AL strategy that can balance the\nprevention of catastrophic forgetting and the ability to quickly learn new\ntasks. In this paper, we propose AccuACL, Accumulated informativeness-based\nActive Continual Learning, by the novel use of the Fisher information matrix as\na criterion for sample selection, derived from a theoretical analysis of the\nFisher-optimality preservation properties within the framework of ACL, while\nalso addressing the scalability issue of Fisher information-based AL. Extensive\nexperiments demonstrate that AccuACL significantly outperforms AL baselines\nacross various CL algorithms, increasing the average accuracy and forgetting by\n23.8% and 17.0%, respectively, on average.\n","authors":["Jaehyun Park","Dongmin Park","Jae-Gil Lee"],"pdf_url":"https://arxiv.org/pdf/2501.14278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00142v2","updated":"2025-04-21T07:52:27Z","published":"2025-03-31T18:49:34Z","title":"Lorentzian Graph Isomorphic Network","summary":"  We introduce the Lorentzian Graph Isomorphic Network (LGIN), a novel graph\nneural network (GNN) designed to operate in hyperbolic spaces, leveraging the\nLorentzian model to enhance graph representation learning. Existing GNNs\nprimarily operate in Euclidean spaces, which can limit their ability to capture\nhierarchical and multi-relational structures inherent to complex graphs. LGIN\naddresses this by incorporating curvature-aware aggregation functions that\npreserve the Lorentzian metric tensor, ensuring embeddings remain constrained\nwithin the hyperbolic space by proposing a new update rule that effectively\ncaptures both local neighborhood interactions and global structural properties,\nenabling LGIN to distinguish non-isomorphic graphs with expressiveness at least\nas powerful as the Weisfeiler-Lehman test. Through extensive evaluation across\nnine benchmark datasets, including molecular and protein structures, LGIN\nconsistently outperforms or matches state-of-the-art GNNs, demonstrating its\nrobustness and efficacy in modeling complex graph structures. To the best of\nour knowledge, this is the first study to extend the concept of a powerful\ngraph neural network to Riemannian manifolds, paving the way for future\nadvancements in hyperbolic graph learning. The code for our paper can be found\nat https://github.com/Deceptrax123/LGIN.\n","authors":["Srinitish Srinivasan","Omkumar CU"],"pdf_url":"https://arxiv.org/pdf/2504.00142v2.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2504.13521v2","updated":"2025-04-21T07:36:33Z","published":"2025-04-18T07:19:44Z","title":"Deep Learning Models Meet Financial Data Modalities","summary":"  Algorithmic trading relies on extracting meaningful signals from diverse\nfinancial data sources, including candlestick charts, order statistics on put\nand canceled orders, traded volume data, limit order books, and news flow.\nWhile deep learning has demonstrated remarkable success in processing\nunstructured data and has significantly advanced natural language processing,\nits application to structured financial data remains an ongoing challenge. This\nstudy investigates the integration of deep learning models with financial data\nmodalities, aiming to enhance predictive performance in trading strategies and\nportfolio optimization. We present a novel approach to incorporating limit\norder book analysis into algorithmic trading by developing embedding techniques\nand treating sequential limit order book snapshots as distinct input channels\nin an image-based representation. Our methodology for processing limit order\nbook data achieves state-of-the-art performance in high-frequency trading\nalgorithms, underscoring the effectiveness of deep learning in financial\napplications.\n","authors":["Kasymkhan Khubiev","Mikhail Semenov"],"pdf_url":"https://arxiv.org/pdf/2504.13521v2.pdf","comment":"15 pages, 14 images, 7 tables"},{"id":"http://arxiv.org/abs/2504.14917v1","updated":"2025-04-21T07:35:24Z","published":"2025-04-21T07:35:24Z","title":"POLYRAG: Integrating Polyviews into Retrieval-Augmented Generation for\n  Medical Applications","summary":"  Large language models (LLMs) have become a disruptive force in the industry,\nintroducing unprecedented capabilities in natural language processing, logical\nreasoning and so on. However, the challenges of knowledge updates and\nhallucination issues have limited the application of LLMs in medical scenarios,\nwhere retrieval-augmented generation (RAG) can offer significant assistance.\nNevertheless, existing retrieve-then-read approaches generally digest the\nretrieved documents, without considering the timeliness, authoritativeness and\ncommonality of retrieval. We argue that these approaches can be suboptimal,\nespecially in real-world applications where information from different sources\nmight conflict with each other and even information from the same source in\ndifferent time scale might be different, and totally relying on this would\ndeteriorate the performance of RAG approaches. We propose PolyRAG that\ncarefully incorporate judges from different perspectives and finally integrate\nthe polyviews for retrieval augmented generation in medical applications. Due\nto the scarcity of real-world benchmarks for evaluation, to bridge the gap we\npropose PolyEVAL, a benchmark consists of queries and documents collected from\nreal-world medical scenarios (including medical policy, hospital & doctor\ninquiry and healthcare) with multiple tagging (e.g., timeliness,\nauthoritativeness) on them. Extensive experiments and analysis on PolyEVAL have\ndemonstrated the superiority of PolyRAG.\n","authors":["Chunjing Gan","Dan Yang","Binbin Hu","Ziqi Liu","Yue Shen","Zhiqiang Zhang","Jian Wang","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.14917v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12322v2","updated":"2025-04-21T07:29:28Z","published":"2025-04-11T06:13:43Z","title":"A Strategic Coordination Framework of Small LLMs Matches Large LLMs in\n  Data Synthesis","summary":"  While data synthesis and distillation are promising strategies to enhance\nsmall language models, current approaches heavily rely on Large Language Models\n(LLMs), which suffer from high computational costs, environmental inefficiency,\nand potential biases inherited from monolithic architectures. In contrast,\nsmaller LLMs are more accessible and sustainable, but their individual\ncapabilities often fall short in generating high-quality, diverse, and reliable\ndata. Inspired by collaborative human processes (e.g., peer review), we propose\na multiple small LLMs involved framework, GRA, that aggregates specialized\nroles across small LLMs to iterative refinement and quality control typically\nachieved by a single large LLM. In this collaborative framework, multiple small\nLLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a\npeer-review-inspired data synthesis pipeline. The Generator proposes initial\ndata samples, the Reviewer critiques their quality and diversity, and the\nAdjudicator resolves conflicts to finalize the output. By decomposing the\nsynthesis process into specialized sub-tasks, collaborative small LLMs can\nachieve data-level parity with large LLM-based distillation. Through\nexperiments across multiple benchmarks, we demonstrate that GRA-produced data\nmatches or exceeds the quality of single large LLM outputs, e.g.,\nQwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large\nmodels for high-quality data synthesis, advocating instead for strategic\ncoordination of smaller agents. Our datasets, models, and code are publicly\navailable at https://github.com/GX-XinGao/GRA.\n","authors":["Xin Gao","Qizhi Pei","Zinan Tang","Yu Li","Honglin Lin","Jiang Wu","Lijun Wu","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2504.12322v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12210v2","updated":"2025-04-21T07:27:59Z","published":"2025-04-16T15:56:57Z","title":"Communication Optimization for Decentralized Learning atop\n  Bandwidth-limited Edge Networks","summary":"  Decentralized federated learning (DFL) is a promising machine learning\nparadigm for bringing artificial intelligence (AI) capabilities to the network\nedge. Running DFL on top of edge networks, however, faces severe performance\nchallenges due to the extensive parameter exchanges between agents. Most\nexisting solutions for these challenges were based on simplistic communication\nmodels, which cannot capture the case of learning over a multi-hop\nbandwidth-limited network. In this work, we address this problem by jointly\ndesigning the communication scheme for the overlay network formed by the agents\nand the mixing matrix that controls the communication demands between the\nagents. By carefully analyzing the properties of our problem, we cast each\ndesign problem into a tractable optimization and develop an efficient algorithm\nwith guaranteed performance. Our evaluations based on real topology and data\nshow that the proposed algorithm can reduce the total training time by over\n$80\\%$ compared to the baseline without sacrificing accuracy, while\nsignificantly improving the computational efficiency over the state of the art.\n","authors":["Tingyang Sun","Tuan Nguyen","Ting He"],"pdf_url":"https://arxiv.org/pdf/2504.12210v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2408.04705"},{"id":"http://arxiv.org/abs/2504.14907v1","updated":"2025-04-21T07:22:11Z","published":"2025-04-21T07:22:11Z","title":"Dynamic Graph-Like Learning with Contrastive Clustering on\n  Temporally-Factored Ship Motion Data for Imbalanced Sea State Estimation in\n  Autonomous Vessel","summary":"  Accurate sea state estimation is crucial for the real-time control and future\nstate prediction of autonomous vessels. However, traditional methods struggle\nwith challenges such as data imbalance and feature redundancy in ship motion\ndata, limiting their effectiveness. To address these challenges, we propose the\nTemporal-Graph Contrastive Clustering Sea State Estimator (TGC-SSE), a novel\ndeep learning model that combines three key components: a time dimension\nfactorization module to reduce data redundancy, a dynamic graph-like learning\nmodule to capture complex variable interactions, and a contrastive clustering\nloss function to effectively manage class imbalance. Our experiments\ndemonstrate that TGC-SSE significantly outperforms existing methods across 14\npublic datasets, achieving the highest accuracy in 9 datasets, with a 20.79%\nimprovement over EDI. Furthermore, in the field of sea state estimation,\nTGC-SSE surpasses five benchmark methods and seven deep learning models.\nAblation studies confirm the effectiveness of each module, demonstrating their\nrespective roles in enhancing overall model performance. Overall, TGC-SSE not\nonly improves the accuracy of sea state estimation but also exhibits strong\ngeneralization capabilities, providing reliable support for autonomous vessel\noperations.\n","authors":["Kexin Wang","Mengna Liu","Xu Cheng","Fan Shi","Shanshan Qi","Shengyong Chen"],"pdf_url":"https://arxiv.org/pdf/2504.14907v1.pdf","comment":"13 pages,15 figures"},{"id":"http://arxiv.org/abs/2504.05347v2","updated":"2025-04-21T07:10:54Z","published":"2025-04-06T12:25:40Z","title":"Structuring Multiple Simple Cycle Reservoirs with Particle Swarm\n  Optimization","summary":"  Reservoir Computing (RC) is a time-efficient computational paradigm derived\nfrom Recurrent Neural Networks (RNNs). The Simple Cycle Reservoir (SCR) is an\nRC model that stands out for its minimalistic design, offering extremely low\nconstruction complexity and proven capability of universally approximating\ntime-invariant causal fading memory filters, even in the linear dynamics\nregime. This paper introduces Multiple Simple Cycle Reservoirs (MSCRs), a\nmulti-reservoir framework that extends Echo State Networks (ESNs) by replacing\na single large reservoir with multiple interconnected SCRs. We demonstrate that\noptimizing MSCR using Particle Swarm Optimization (PSO) outperforms existing\nmulti-reservoir models, achieving competitive predictive performance with a\nlower-dimensional state space. By modeling interconnections as a weighted\nDirected Acyclic Graph (DAG), our approach enables flexible, task-specific\nnetwork topology adaptation. Numerical simulations on three benchmark\ntime-series prediction tasks confirm these advantages over rival algorithms.\nThese findings highlight the potential of MSCR-PSO as a promising framework for\noptimizing multi-reservoir systems, providing a foundation for further\nadvancements and applications of interconnected SCRs for developing efficient\nAI devices.\n","authors":["Ziqiang Li","Robert Simon Fong","Kantaro Fujiwara","Kazuyuki Aihara","Gouhei Tanaka"],"pdf_url":"https://arxiv.org/pdf/2504.05347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14898v1","updated":"2025-04-21T07:09:05Z","published":"2025-04-21T07:09:05Z","title":"Expected Free Energy-based Planning as Variational Inference","summary":"  We address the problem of planning under uncertainty, where an agent must\nchoose actions that not only achieve desired outcomes but also reduce\nuncertainty. Traditional methods often treat exploration and exploitation as\nseparate objectives, lacking a unified inferential foundation. Active\ninference, grounded in the Free Energy Principle, offers such a foundation by\nminimizing Expected Free Energy (EFE), a cost function that combines utility\nwith epistemic drives like ambiguity resolution and novelty seeking. However,\nthe computational burden of EFE minimization has remained a major obstacle to\nits scalability. In this paper, we show that EFE-based planning arises\nnaturally from minimizing a variational free energy functional on a generative\nmodel augmented with preference and epistemic priors. This result reinforces\ntheoretical consistency with the Free Energy Principle, by casting planning\nitself as variational inference. Our formulation yields optimal policies that\njointly support goal achievement and information gain, while incorporating a\ncomplexity term that accounts for bounded computational resources. This\nunifying framework connects and extends existing methods, enabling scalable,\nresource-aware implementations of active inference agents.\n","authors":["Bert de Vries","Wouter Nuijten","Thijs van de Laar","Wouter Kouw","Sepideh Adamiat","Tim Nisslbeck","Mykola Lukashchuk","Hoang Minh Huu Nguyen","Marco Hidalgo Araya","Raphael Tresor","Thijs Jenneskens","Ivana Nikoloska","Raaja Subramanian","Bart van Erp","Dmitry Bagaev","Albert Podusenko"],"pdf_url":"https://arxiv.org/pdf/2504.14898v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2411.03641v2","updated":"2025-04-21T07:04:56Z","published":"2024-11-06T03:38:00Z","title":"Constrained Multi-objective Bayesian Optimization through Optimistic\n  Constraints Estimation","summary":"  Multi-objective Bayesian optimization has been widely adopted in scientific\nexperiment design, including drug discovery and hyperparameter optimization. In\npractice, regulatory or safety concerns often impose additional thresholds on\ncertain attributes of the experimental outcomes. Previous work has primarily\nfocused on constrained single-objective optimization tasks or active search\nunder constraints. The existing constrained multi-objective algorithms address\nthe issue with heuristics and approximations, posing challenges to the analysis\nof the sample efficiency. We propose a novel constrained multi-objective\nBayesian optimization algorithm COMBOO that balances active learning of the\nlevel-set defined on multiple unknowns with multi-objective optimization within\nthe feasible region. We provide both theoretical analysis and empirical\nevidence, demonstrating the efficacy of our approach on various synthetic\nbenchmarks and real-world applications.\n","authors":["Diantong Li","Fengxue Zhang","Chong Liu","Yuxin Chen"],"pdf_url":"https://arxiv.org/pdf/2411.03641v2.pdf","comment":"This paper is accepted to AISTATS 2025"},{"id":"http://arxiv.org/abs/2412.07197v2","updated":"2025-04-21T06:52:09Z","published":"2024-12-10T05:20:49Z","title":"Hierarchical Split Federated Learning: Convergence Analysis and System\n  Optimization","summary":"  As AI models expand in size, it has become increasingly challenging to deploy\nfederated learning (FL) on resource-constrained edge devices. To tackle this\nissue, split federated learning (SFL) has emerged as an FL framework with\nreduced workload on edge devices via model splitting; it has received extensive\nattention from the research community in recent years. Nevertheless, most prior\nworks on SFL focus only on a two-tier architecture without harnessing\nmulti-tier cloudedge computing resources. In this paper, we intend to analyze\nand optimize the learning performance of SFL under multi-tier systems.\nSpecifically, we propose the hierarchical SFL (HSFL) framework and derive its\nconvergence bound. Based on the theoretical results, we formulate a joint\noptimization problem for model splitting (MS) and model aggregation (MA). To\nsolve this rather hard problem, we then decompose it into MS and MA subproblems\nthat can be solved via an iterative descending algorithm. Simulation results\ndemonstrate that the tailored algorithm can effectively optimize MS and MA for\nSFL within virtually any multi-tier system.\n","authors":["Zheng Lin","Wei Wei","Zhe Chen","Chan-Tong Lam","Xianhao Chen","Yue Gao","Jun Luo"],"pdf_url":"https://arxiv.org/pdf/2412.07197v2.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2504.14889v1","updated":"2025-04-21T06:36:09Z","published":"2025-04-21T06:36:09Z","title":"Latent Bayesian Optimization via Autoregressive Normalizing Flows","summary":"  Bayesian Optimization (BO) has been recognized for its effectiveness in\noptimizing expensive and complex objective functions. Recent advancements in\nLatent Bayesian Optimization (LBO) have shown promise by integrating generative\nmodels such as variational autoencoders (VAEs) to manage the complexity of\nhigh-dimensional and structured data spaces. However, existing LBO approaches\noften suffer from the value discrepancy problem, which arises from the\nreconstruction gap between input and latent spaces. This value discrepancy\nproblem propagates errors throughout the optimization process, leading to\nsuboptimal outcomes. To address this issue, we propose a Normalizing Flow-based\nBayesian Optimization (NF-BO), which utilizes normalizing flow as a generative\nmodel to establish one-to-one encoding function from the input space to the\nlatent space, along with its left-inverse decoding function, eliminating the\nreconstruction gap. Specifically, we introduce SeqFlow, an autoregressive\nnormalizing flow for sequence data. In addition, we develop a new candidate\nsampling strategy that dynamically adjusts the exploration probability for each\ntoken based on its importance. Through extensive experiments, our NF-BO method\ndemonstrates superior performance in molecule generation tasks, significantly\noutperforming both traditional and recent LBO approaches.\n","authors":["Seunghun Lee","Jinyoung Park","Jaewon Chu","Minseo Yoon","Hyunwoo J. Kim"],"pdf_url":"https://arxiv.org/pdf/2504.14889v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2504.14882v1","updated":"2025-04-21T06:20:50Z","published":"2025-04-21T06:20:50Z","title":"Some Optimizers are More Equal: Understanding the Role of Optimizers in\n  Group Fairness","summary":"  We study whether and how the choice of optimization algorithm can impact\ngroup fairness in deep neural networks. Through stochastic differential\nequation analysis of optimization dynamics in an analytically tractable setup,\nwe demonstrate that the choice of optimization algorithm indeed influences\nfairness outcomes, particularly under severe imbalance. Furthermore, we show\nthat when comparing two categories of optimizers, adaptive methods and\nstochastic methods, RMSProp (from the adaptive category) has a higher\nlikelihood of converging to fairer minima than SGD (from the stochastic\ncategory). Building on this insight, we derive two new theoretical guarantees\nshowing that, under appropriate conditions, RMSProp exhibits fairer parameter\nupdates and improved fairness in a single optimization step compared to SGD. We\nthen validate these findings through extensive experiments on three publicly\navailable datasets, namely CelebA, FairFace, and MS-COCO, across different\ntasks as facial expression recognition, gender classification, and multi-label\nclassification, using various backbones. Considering multiple fairness\ndefinitions including equalized odds, equal opportunity, and demographic\nparity, adaptive optimizers like RMSProp and Adam consistently outperform SGD\nin terms of group fairness, while maintaining comparable predictive accuracy.\nOur results highlight the role of adaptive updates as a crucial yet overlooked\nmechanism for promoting fair outcomes.\n","authors":["Mojtaba Kolahdouzi","Hatice Gunes","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2504.14882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14879v1","updated":"2025-04-21T06:15:07Z","published":"2025-04-21T06:15:07Z","title":"Impact of Latent Space Dimension on IoT Botnet Detection Performance:\n  VAE-Encoder Versus ViT-Encoder","summary":"  The rapid evolution of Internet of Things (IoT) technology has led to a\nsignificant increase in the number of IoT devices, applications, and services.\nThis surge in IoT devices, along with their widespread presence, has made them\na prime target for various cyber-attacks, particularly through IoT botnets. As\na result, security has become a major concern within the IoT ecosystem. This\nstudy focuses on investigating how the latent dimension impacts the performance\nof different deep learning classifiers when trained on latent vector\nrepresentations of the train dataset. The primary objective is to compare the\noutcomes of these models when encoder components from two cutting-edge\narchitectures: the Vision Transformer (ViT) and the Variational Auto-Encoder\n(VAE) are utilized to project the high dimensional train dataset to the learned\nlow dimensional latent space. The encoder components are employed to project\nhigh-dimensional structured .csv IoT botnet traffic datasets to various latent\nsizes. Evaluated on N-BaIoT and CICIoT2022 datasets, findings reveal that\nVAE-encoder based dimension reduction outperforms ViT-encoder based dimension\nreduction for both datasets in terms of four performance metrics including\naccuracy, precision, recall, and F1-score for all models which can be\nattributed to absence of spatial patterns in the datasets the ViT model\nattempts to learn and extract from image instances.\n","authors":["Hassan Wasswa","Aziida Nanyonga","Timothy Lynar"],"pdf_url":"https://arxiv.org/pdf/2504.14879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10403v2","updated":"2025-04-21T06:05:23Z","published":"2025-04-14T16:52:34Z","title":"Satellite Federated Fine-Tuning for Foundation Models in Space Computing\n  Power Networks","summary":"  Advancements in artificial intelligence (AI) and low-earth orbit (LEO)\nsatellites have promoted the application of large remote sensing foundation\nmodels for various downstream tasks. However, direct downloading of these\nmodels for fine-tuning on the ground is impeded by privacy concerns and limited\nbandwidth. Satellite federated learning (FL) offers a solution by enabling\nmodel fine-tuning directly on-board satellites and aggregating model updates\nwithout data downloading. Nevertheless, for large foundation models, the\ncomputational capacity of satellites is insufficient to support effective\non-board fine-tuning in traditional satellite FL frameworks. To address these\nchallenges, we propose a satellite-ground collaborative federated fine-tuning\nframework. The key of the framework lies in how to reasonably decompose and\nallocate model components to alleviate insufficient on-board computation\ncapabilities. During fine-tuning, satellites exchange intermediate results with\nground stations or other satellites for forward propagation and back\npropagation, which brings communication challenges due to the special\ncommunication topology of space transmission networks, such as intermittent\nsatellite-ground communication, short duration of satellite-ground\ncommunication windows, and unstable inter-orbit inter-satellite links (ISLs).\nTo reduce transmission delays, we further introduce tailored communication\nstrategies that integrate both communication and computing resources.\nSpecifically, we propose a parallel intra-orbit communication strategy, a\ntopology-aware satellite-ground communication strategy, and a\nlatency-minimalization inter-orbit communication strategy to reduce space\ncommunication costs. Simulation results demonstrate significant reductions in\ntraining time with improvements of approximately 33%.\n","authors":["Yan Zhu","Jingyang Zhu","Ting Wang","Yuanming Shi","Chunxiao Jiang","Khaled Ben Letaief"],"pdf_url":"https://arxiv.org/pdf/2504.10403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14875v1","updated":"2025-04-21T06:02:03Z","published":"2025-04-21T06:02:03Z","title":"ReSpec: Relevance and Specificity Grounded Online Filtering for Learning\n  on Video-Text Data Streams","summary":"  The rapid growth of video-text data presents challenges in storage and\ncomputation during training. Online learning, which processes streaming data in\nreal-time, offers a promising solution to these issues while also allowing\nswift adaptations in scenarios demanding real-time responsiveness. One strategy\nto enhance the efficiency and effectiveness of learning involves identifying\nand prioritizing data that enhances performance on target downstream tasks. We\npropose Relevance and Specificity-based online filtering framework (ReSpec)\nthat selects data based on four criteria: (i) modality alignment for clean\ndata, (ii) task relevance for target focused data, (iii) specificity for\ninformative and detailed data, and (iv) efficiency for low-latency processing.\nRelevance is determined by the probabilistic alignment of incoming data with\ndownstream tasks, while specificity employs the distance to a root embedding\nrepresenting the least specific data as an efficient proxy for informativeness.\nBy establishing reference points from target task data, ReSpec filters incoming\ndata in real-time, eliminating the need for extensive storage and compute.\nEvaluating on large-scale datasets WebVid2M and VideoCC3M, ReSpec attains\nstate-of-the-art performance on five zeroshot video retrieval tasks, using as\nlittle as 5% of the data while incurring minimal compute. The source code is\navailable at https://github.com/cdjkim/ReSpec.\n","authors":["Chris Dongjoo Kim","Jihwan Moon","Sangwoo Moon","Heeseung Yun","Sihaeng Lee","Aniruddha Kembhavi","Soonyoung Lee","Gunhee Kim","Sangho Lee","Christopher Clark"],"pdf_url":"https://arxiv.org/pdf/2504.14875v1.pdf","comment":"CVPR 2025 (main conference)"},{"id":"http://arxiv.org/abs/2410.14815v2","updated":"2025-04-21T05:29:01Z","published":"2024-10-18T18:35:19Z","title":"Adapting Multilingual LLMs to Low-Resource Languages using Continued\n  Pre-training and Synthetic Corpus","summary":"  Multilingual LLMs support a variety of languages; however, their performance\nis suboptimal for low-resource languages. In this work, we emphasize the\nimportance of continued pre-training of multilingual LLMs and the use of\ntranslation-based synthetic pre-training corpora for improving LLMs in\nlow-resource languages. We conduct our study in the context of the low-resource\nIndic language Hindi. We introduce Nemotron-Mini-Hindi 4B, a bilingual SLM\nsupporting both Hindi and English, based on Nemotron-Mini 4B. The model is\ntrained using a mix of real and synthetic Hindi + English tokens, with\ncontinuous pre-training performed on 400B tokens. We demonstrate that both the\nbase and instruct models achieve state-of-the-art results on Hindi benchmarks\nwhile remaining competitive on English tasks. Additionally, we observe that the\ncontinued pre-training approach enhances the model's overall factual accuracy.\nWe perform an ablation study to highlight the impact of Hindi pre-training,\nshowing significant improvements in Hindi chat capabilities and factual\naccuracy, which cannot be achieved through Hindi alignment alone.\n","authors":["Raviraj Joshi","Kanishk Singla","Anusha Kamath","Raunak Kalani","Rakesh Paul","Utkarsh Vaidya","Sanjay Singh Chauhan","Niranjan Wartikar","Eileen Long"],"pdf_url":"https://arxiv.org/pdf/2410.14815v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12791v2","updated":"2025-04-21T05:17:15Z","published":"2025-02-18T11:52:25Z","title":"Activation-wise Propagation: A Universal Strategy to Break Timestep\n  Constraints in Spiking Neural Networks for 3D Data Processing","summary":"  Due to their event-driven and parameter-efficient effect, spiking neural\nnetworks (SNNs) show potential in tasks requiring real-time multi-sensor\nperception, such as autonomous driving. The spiking mechanism facilitates\nsparse encoding, enabling spatial and temporal data to be represented in a\ndiscrete manner. However, SNNs still lag behind artificial neural networks\n(ANNs) in terms of performance and computational efficiency. One major\nchallenge in SNNs is the timestep-wise iterative update of neuronal states,\nwhich makes it difficult to achieve an optimal trade-off among accuracy,\nlatency, and training cost. Although some methods perform well with shorter\ntimesteps, few propose strategies to overcome such constraint effectively.\nMoreover, many recent SNN advancements rely on either optimizations tailored to\nspecific architectures or a collection of specialized neuron-level strategies.\nWhile these approaches can enhance performance, they often lead to increased\ncomputational expense and restrict their application to particular\narchitectures or modalities. This leaves room for further exploration of\nsimple, universal, and structure-agnostic strategies that could offer broader\napplicability and efficiency. In this paper, we introduce Activation-wise\nMembrane Potential Propagation (AMP2), a novel state update mechanism for\nspiking neurons. Inspired by skip connections in deep networks, AMP2\nincorporates the membrane potential of neurons into network, eliminating the\nneed for iterative updates. Our method achieves significant improvements across\nvarious 3D modalities, including 3D point clouds and event streams, boosting\nSpiking PointNet's accuracy on ModelNet40 from 87.36% to 89.74% and surpassing\nANN PointNet in recognition accuracy on the DVS128 Gesture dataset.\n","authors":["Jian Song","Xiangfei Yang","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2502.12791v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15459v3","updated":"2025-04-21T05:15:17Z","published":"2024-06-11T03:36:00Z","title":"Large-Scale Contextual Market Equilibrium Computation through Deep\n  Learning","summary":"  Market equilibrium is one of the most fundamental solution concepts in\neconomics and social optimization analysis. Existing works on market\nequilibrium computation primarily focus on settings with relatively few buyers.\nMotivated by this, our paper investigates the computation of market equilibrium\nin scenarios with a large-scale buyer population, where buyers and goods are\nrepresented by their contexts. Building on this realistic and generalized\ncontextual market model, we introduce MarketFCNet, a deep learning-based method\nfor approximating market equilibrium. We start by parameterizing the allocation\nof each good to each buyer using a neural network, which depends solely on the\ncontext of the buyer and the good. Next, we propose an efficient method to\nunbiasedly estimate the loss function of the training algorithm, enabling us to\noptimize the network parameters through gradient. To evaluate the approximated\nsolution, we propose a metric called Nash Gap, which quantifies the deviation\nof the given allocation and price pair from the market equilibrium.\nExperimental results indicate that MarketFCNet delivers competitive performance\nand significantly lower running times compared to existing methods as the\nmarket scale expands, demonstrating the potential of deep learning-based\nmethods to accelerate the approximation of large-scale contextual market\nequilibrium.\n","authors":["Yunxuan Ma","Yide Bian","Hao Xu","Weitao Yang","Jingshu Zhao","Zhijian Duan","Feng Wang","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2406.15459v3.pdf","comment":"25 pages, 4 figures, recieved at IJTCS2025"},{"id":"http://arxiv.org/abs/2504.13426v2","updated":"2025-04-21T05:10:14Z","published":"2025-04-18T02:56:21Z","title":"Simplifying Graph Convolutional Networks with Redundancy-Free Neighbors","summary":"  In recent years, Graph Convolutional Networks (GCNs) have gained popularity\nfor their exceptional ability to process graph-structured data. Existing\nGCN-based approaches typically employ a shallow model architecture due to the\nover-smoothing phenomenon. Current approaches to mitigating over-smoothing\nprimarily involve adding supplementary components to GCN architectures, such as\nresidual connections and random edge-dropping strategies. However, these\nimprovements toward deep GCNs have achieved only limited success. In this work,\nwe analyze the intrinsic message passing mechanism of GCNs and identify a\ncritical issue: messages originating from high-order neighbors must traverse\nthrough low-order neighbors to reach the target node. This repeated reliance on\nlow-order neighbors leads to redundant information aggregation, a phenomenon we\nterm over-aggregation. Our analysis demonstrates that over-aggregation not only\nintroduces significant redundancy but also serves as the fundamental cause of\nover-smoothing in GCNs.\n","authors":["Jielong Lu","Zhihao Wu","Zhiling Cai","Yueyang Pi","Shiping Wang"],"pdf_url":"https://arxiv.org/pdf/2504.13426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14854v1","updated":"2025-04-21T04:45:40Z","published":"2025-04-21T04:45:40Z","title":"Uncertainty quantification of neural network models of evolving\n  processes via Langevin sampling","summary":"  We propose a scalable, approximate inference hypernetwork framework for a\ngeneral model of history-dependent processes. The flexible data model is based\non a neural ordinary differential equation (NODE) representing the evolution of\ninternal states together with a trainable observation model subcomponent. The\nposterior distribution corresponding to the data model parameters (weights and\nbiases) follows a stochastic differential equation with a drift term related to\nthe score of the posterior that is learned jointly with the data model\nparameters. This Langevin sampling approach offers flexibility in balancing the\ncomputational budget between the evaluation cost of the data model and the\napproximation of the posterior density of its parameters. We demonstrate\nperformance of the hypernetwork on chemical reaction and material physics data\nand compare it to mean-field variational inference.\n","authors":["Cosmin Safta","Reese E. Jones","Ravi G. Patel","Raelynn Wonnacot","Dan S. Bolintineanu","Craig M. Hamel","Sharlotte L. B. Kramer"],"pdf_url":"https://arxiv.org/pdf/2504.14854v1.pdf","comment":"23 pages, 15 figures"},{"id":"http://arxiv.org/abs/2410.21676v4","updated":"2025-04-21T04:19:56Z","published":"2024-10-29T02:54:06Z","title":"How Does Critical Batch Size Scale in Pre-training?","summary":"  Training large-scale models under given resources requires careful design of\nparallelism strategies. In particular, the efficiency notion of critical batch\nsize (CBS), concerning the compromise between time and compute, marks the\nthreshold beyond which greater data parallelism leads to diminishing returns.\nTo operationalize it, we propose a measure of CBS and pre-train a series of\nauto-regressive language models, ranging from 85 million to 1.2 billion\nparameters, on the C4 dataset. Through extensive hyper-parameter sweeps and\ncareful control of factors such as batch size, momentum, and learning rate\nalong with its scheduling, we systematically investigate the impact of scale on\nCBS. Then we fit scaling laws with respect to model and data sizes to decouple\ntheir effects. Overall, our results demonstrate that CBS scales primarily with\ndata size rather than model size, a finding we justify theoretically through\nthe analysis of infinite-width limits of neural networks and\ninfinite-dimensional least squares regression. Of independent interest, we\nhighlight the importance of common hyper-parameter choices and strategies for\nstudying large-scale pre-training beyond fixed training durations.\n","authors":["Hanlin Zhang","Depen Morwani","Nikhil Vyas","Jingfeng Wu","Difan Zou","Udaya Ghai","Dean Foster","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2410.21676v4.pdf","comment":"ICLR 2025, Blog post:\n  https://kempnerinstitute.harvard.edu/research/deeper-learning/how-does-critical-batch-size-scale-in-pre-training-decoupling-data-and-model-size"},{"id":"http://arxiv.org/abs/2409.01035v4","updated":"2025-04-21T03:58:50Z","published":"2024-09-02T08:10:51Z","title":"Task-Specific Directions: Definition, Exploration, and Utilization in\n  Parameter Efficient Fine-Tuning","summary":"  Large language models demonstrate impressive performance on downstream tasks,\nyet they require extensive resource consumption when fully fine-tuning all\nparameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT)\nstrategies, such as LoRA, have been developed. In this paper, we delve into the\nconcept of task-specific directions (TSDs), which are critical for\ntransitioning large models from pretrained states to task-specific enhancements\nin PEFT. We propose a framework to clearly define these directions and explore\ntheir properties and practical utilization challenges. We then introduce a\nnovel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the\nfine-tuning process, thereby enhancing model performance on targeted tasks.\nAdditionally, based on our exploration of TSD, we focus on an important issue\nin PEFT: the initialization of LoRA. While some works have pointed out the\nsignificance of initialization for LoRA's performance and proposed various\nstrategies, these methods are often empirical and not task-specific. To address\nthis issue, we propose LoRA-Init. Starting from TSD, we identify the directions\nthat require the most adjustment during fine-tuning for downstream tasks. By\ninitializing the matrices in LoRA with these directions, LoRA-Init\nsignificantly enhances LoRA's performance. Moreover, we can combine LoRA-Dash\nand LoRA-Init to create the final version of LoRA based on TSDs, which we refer\nto as LoRA-TSD. Extensive experiments have conclusively demonstrated the\neffectiveness of these methods, and in-depth analyses further reveal the\nunderlying mechanisms behind their success.\n","authors":["Chongjie Si","Zhiyi Shi","Shifan Zhang","Xiaokang Yang","Hanspeter Pfister","Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2409.01035v4.pdf","comment":"Codes in https://github.com/Chongjie-Si/Subspace-Tuning"},{"id":"http://arxiv.org/abs/2411.05735v2","updated":"2025-04-21T03:50:23Z","published":"2024-11-08T17:50:24Z","title":"Aioli: A Unified Optimization Framework for Language Model Data Mixing","summary":"  Language model performance depends on identifying the optimal mixture of data\ngroups to train on (e.g., law, code, math). Prior work has proposed a diverse\nset of methods to efficiently learn mixture proportions, ranging from fitting\nregression models over training runs to dynamically updating proportions\nthroughout training. Surprisingly, we find that no existing method consistently\noutperforms a simple stratified sampling baseline in terms of average test\nperplexity. To understand this inconsistency, we unify existing methods into a\nstandard framework, showing they are equivalent to solving a common\noptimization problem: minimize average loss subject to a method-specific mixing\nlaw -- an implicit assumption on the relationship between loss and mixture\nproportions. This framework suggests that measuring the fidelity of a method's\nmixing law can offer insights into its performance. Empirically, we find that\nexisting methods set their mixing law parameters inaccurately, resulting in the\ninconsistent mixing performance we observe. Using this insight, we derive a new\nonline method named Aioli, which directly estimates the mixing law parameters\nthroughout training and uses them to dynamically adjust proportions. Aioli\noutperforms stratified sampling on 6 out of 6 datasets by an average of 0.27\ntest perplexity points, whereas existing methods fail to consistently beat\nstratified sampling, doing up to 6.9 points worse. Moreover, in a practical\nsetting where proportions are learned on shorter runs due to computational\nconstraints, Aioli can dynamically adjust these proportions over the full\ntraining run, consistently improving performance over existing methods by up to\n12.012 test perplexity points.\n","authors":["Mayee F. Chen","Michael Y. Hu","Nicholas Lourie","Kyunghyun Cho","Christopher Ré"],"pdf_url":"https://arxiv.org/pdf/2411.05735v2.pdf","comment":"ICLR 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2504.13241v2","updated":"2025-04-21T03:47:31Z","published":"2025-04-17T17:39:35Z","title":"Recursive Deep Inverse Reinforcement Learning","summary":"  Inferring an adversary's goals from exhibited behavior is crucial for\ncounterplanning and non-cooperative multi-agent systems in domains like\ncybersecurity, military, and strategy games. Deep Inverse Reinforcement\nLearning (IRL) methods based on maximum entropy principles show promise in\nrecovering adversaries' goals but are typically offline, require large batch\nsizes with gradient descent, and rely on first-order updates, limiting their\napplicability in real-time scenarios. We propose an online Recursive Deep\nInverse Reinforcement Learning (RDIRL) approach to recover the cost function\ngoverning the adversary actions and goals. Specifically, we minimize an upper\nbound on the standard Guided Cost Learning (GCL) objective using sequential\nsecond-order Newton updates, akin to the Extended Kalman Filter (EKF), leading\nto a fast (in terms of convergence) learning algorithm. We demonstrate that\nRDIRL is able to recover cost and reward functions of expert agents in standard\nand adversarial benchmark tasks. Experiments on benchmark tasks show that our\nproposed approach outperforms several leading IRL algorithms.\n","authors":["Paul Ghanem","Michael Potter","Owen Howell","Pau Closas","Alireza Ramezani","Deniz Erdogmus","Tales Imbiriba"],"pdf_url":"https://arxiv.org/pdf/2504.13241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.01783v3","updated":"2025-04-21T03:40:10Z","published":"2024-11-04T04:15:36Z","title":"Context Parallelism for Scalable Million-Token Inference","summary":"  We present context parallelism for long-context large language model\ninference, which achieves near-linear scaling for long-context prefill latency\nwith up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M\ncontext prefill with Llama3 405B model in 77s (93% parallelization efficiency,\n63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two\nlossless exact ring attention variants: pass-KV and pass-Q to cover a wide\nrange of use cases with the state-of-the-art performance: full prefill,\npersistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected\nwith RDMA and TCP both show similar scalability for long-context prefill,\ndemonstrating that our method scales well using common commercial data center\nwith medium-to-low inter-host bandwidth.\n","authors":["Amy Yang","Jingyi Yang","Aya Ibrahim","Xinfeng Xie","Bangsheng Tang","Grigory Sizov","Jeremy Reizenstein","Jongsoo Park","Jianyu Huang"],"pdf_url":"https://arxiv.org/pdf/2411.01783v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14191v3","updated":"2025-04-21T03:04:15Z","published":"2024-06-20T10:51:06Z","title":"Temporal Knowledge Graph Question Answering: A Survey","summary":"  Knowledge Base Question Answering (KBQA) has been a long-standing field to\nanswer questions based on knowledge bases. Recently, the evolving dynamics of\nknowledge have attracted a growing interest in Temporal Knowledge Graph\nQuestion Answering (TKGQA), an emerging task to answer temporal questions.\nHowever, this field grapples with ambiguities in defining temporal questions\nand lacks a systematic categorization of existing methods for TKGQA. In\nresponse, this paper provides a thorough survey from two perspectives: the\ntaxonomy of temporal questions and the methodological categorization for TKGQA.\nSpecifically, we first establish a detailed taxonomy of temporal questions\nengaged in prior studies. Subsequently, we provide a comprehensive review of\nTKGQA techniques of two categories: semantic parsing-based and TKG\nembedding-based. Building on this review, the paper outlines potential research\ndirections aimed at advancing the field of TKGQA. This work aims to serve as a\ncomprehensive reference for TKGQA and to stimulate further research.\n","authors":["Miao Su","Zixuan Li","Zhuo Chen","Long Bai","Xiaolong Jin","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2406.14191v3.pdf","comment":"8 pages, 3 figures. This work has been submitted to the IEEE for\n  possible publication"},{"id":"http://arxiv.org/abs/2504.14815v1","updated":"2025-04-21T02:44:59Z","published":"2025-04-21T02:44:59Z","title":"What Lurks Within? Concept Auditing for Shared Diffusion Models at Scale","summary":"  Diffusion models (DMs) have revolutionized text-to-image generation, enabling\nthe creation of highly realistic and customized images from text prompts. With\nthe rise of parameter-efficient fine-tuning (PEFT) techniques like LoRA, users\ncan now customize powerful pre-trained models using minimal computational\nresources. However, the widespread sharing of fine-tuned DMs on open platforms\nraises growing ethical and legal concerns, as these models may inadvertently or\ndeliberately generate sensitive or unauthorized content, such as copyrighted\nmaterial, private individuals, or harmful content. Despite the increasing\nregulatory attention on generative AI, there are currently no practical tools\nfor systematically auditing these models before deployment. In this paper, we\naddress the problem of concept auditing: determining whether a fine-tuned DM\nhas learned to generate a specific target concept. Existing approaches\ntypically rely on prompt-based input crafting and output-based image\nclassification but suffer from critical limitations, including prompt\nuncertainty, concept drift, and poor scalability. To overcome these challenges,\nwe introduce Prompt-Agnostic Image-Free Auditing (PAIA), a novel, model-centric\nconcept auditing framework. By treating the DM as the object of inspection,\nPAIA enables direct analysis of internal model behavior, bypassing the need for\noptimized prompts or generated images. We evaluate PAIA on 320 controlled model\nand 690 real-world community models sourced from a public DM sharing platform.\nPAIA achieves over 90% detection accuracy while reducing auditing time by\n18-40x compared to existing baselines. To our knowledge, PAIA is the first\nscalable and practical solution for pre-deployment concept auditing of\ndiffusion models, providing a practical foundation for safer and more\ntransparent diffusion model sharing.\n","authors":["Xiaoyong Yuan","Xiaolong Ma","Linke Guo","Lan Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.14815v1.pdf","comment":"17 pages, 15 figures"},{"id":"http://arxiv.org/abs/2504.14814v1","updated":"2025-04-21T02:41:17Z","published":"2025-04-21T02:41:17Z","title":"A Basic Evaluation of Neural Networks Trained with the Error Diffusion\n  Learning Algorithm","summary":"  Artificial neural networks are powerful tools capable of addressing various\ntasks. Although the backpropagation algorithm has become a standard training\nmethod for these neural networks, its lack of biological plausibility has\ninspired the development of alternative learning approaches. One such\nalternative is Kaneko's Error Diffusion Learning Algorithm (EDLA), a\nbiologically motivated approach wherein a single global error signal diffuses\nthroughout a network composed of paired excitatory-inhibitory sublayers,\nthereby eliminating the necessity for layer-wise backpropagation. This study\npresents a contemporary formulation of the EDLA framework and evaluates its\neffectiveness through parity check, regression, and image classification tasks.\nOur experimental results indicate that EDLA networks can consistently achieve\nhigh accuracy across these benchmarks, with performance efficiency and\nconvergence speed notably influenced by the choice of learning rate, neuron\ncount, and network depth. Further investigation of the internal representations\nformed by EDLA networks reveals their capacity for meaningful feature\nextraction, similar to traditional neural networks. These results suggest that\nEDLA is a biologically motivated alternative for training feedforward networks\nand will motivate future work on extending this method to biologically inspired\nneural networks.\n","authors":["Kazuhisa Fujita"],"pdf_url":"https://arxiv.org/pdf/2504.14814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14810v1","updated":"2025-04-21T02:25:03Z","published":"2025-04-21T02:25:03Z","title":"DONOD: Robust and Generalizable Instruction Fine-Tuning for LLMs via\n  Model-Intrinsic Dataset Pruning","summary":"  Ad-hoc instruction fine-tuning of large language models (LLMs) is widely\nadopted for domain-specific adaptation. While domain-specific supervised\nfine-tuning (SFT) is effective and efficient, it often weakens cross-domain\ngeneralization and struggles with noisy training data. To address these\nchallenges, we propose DONOD, a lightweight model-intrinsic data pruning\nmethod. Our approach evaluates data using two model-parameter-based metrics:\nDelta of Norm (DON), which captures the cumulative influence on model weights,\nand Norm of Delta (NOD), which quantifies weight instability. Moreover, by\nemploying the Technique for Order of Preference by Similarity to Ideal Solution\n(TOPSIS) algorithm, we effectively filter noisy, unlearnable, and\ngeneralization-harming samples without relying on auxiliary models during the\nSFT process. Experiments on mathematical tasks demonstrate that data selected\nby DONOD achieve superior fine-tuning efficiency and improved robustness\nagainst noisy data. By filtering out 70% of the full dataset, we improve\ntarget-domain accuracy by 14.90% and cross-domain accuracy by 5.67%. Meanwhile,\nour selected data present superior cross-architecture generalization. Data\npruned by smaller models (e.g., Llama 3.1-8B) generalize effectively on larger\nmodels (e.g., Llama 2-13B). Compared to existing related methodologies, DONOD\ndemonstrates comparable or superior performance while remaining\ndataset-agnostic, enabling broader applicability.\n","authors":["Jucheng Hu","Surong Yang","Dongzhan Zhou","Lijun Wu"],"pdf_url":"https://arxiv.org/pdf/2504.14810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07582v2","updated":"2025-04-21T02:22:06Z","published":"2024-10-10T03:31:16Z","title":"Detecting Training Data of Large Language Models via Expectation\n  Maximization","summary":"  The advancement of large language models has grown parallel to the opacity of\ntheir training data. Membership inference attacks (MIAs) aim to determine\nwhether specific data was used to train a model. They offer valuable insights\ninto detecting data contamination and ensuring compliance with privacy and\ncopyright standards. However, MIA for LLMs is challenging due to the massive\nscale of training data and the inherent ambiguity of membership in texts.\nMoreover, creating realistic MIA evaluation benchmarks is difficult as training\nand test data distributions are often unknown. We introduce EM-MIA, a novel\nmembership inference method that iteratively refines membership scores and\nprefix scores via an expectation-maximization algorithm. Our approach leverages\nthe observation that these scores can improve each other: membership scores\nhelp identify effective prefixes for detecting training data, while prefix\nscores help determine membership. As a result, EM-MIA achieves state-of-the-art\nresults on WikiMIA. To enable comprehensive evaluation, we introduce OLMoMIA, a\nbenchmark built from OLMo resources, which allows controlling task difficulty\nthrough varying degrees of overlap between training and test data\ndistributions. Our experiments demonstrate EM-MIA is robust across different\nscenarios while also revealing fundamental limitations of current MIA\napproaches when member and non-member distributions are nearly identical.\n","authors":["Gyuwan Kim","Yang Li","Evangelia Spiliopoulou","Jie Ma","Miguel Ballesteros","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2410.07582v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2504.14808v1","updated":"2025-04-21T02:17:19Z","published":"2025-04-21T02:17:19Z","title":"On Self-improving Token Embeddings","summary":"  This article introduces a novel and fast method for refining pre-trained\nstatic word or, more generally, token embeddings. By incorporating the\nembeddings of neighboring tokens in text corpora, it continuously updates the\nrepresentation of each token, including those without pre-assigned embeddings.\nThis approach effectively addresses the out-of-vocabulary problem, too.\nOperating independently of large language models and shallow neural networks,\nit enables versatile applications such as corpus exploration, conceptual\nsearch, and word sense disambiguation. The method is designed to enhance token\nrepresentations within topically homogeneous corpora, where the vocabulary is\nrestricted to a specific domain, resulting in more meaningful embeddings\ncompared to general-purpose pre-trained vectors. As an example, the methodology\nis applied to explore storm events and their impacts on infrastructure and\ncommunities using narratives from a subset of the NOAA Storm Events database.\nThe article also demonstrates how the approach improves the representation of\nstorm-related terms over time, providing valuable insights into the evolving\nnature of disaster narratives.\n","authors":["Mario M. Kubek","Shiraj Pokharel","Thomas Böhme","Emma L. McDaniel","Herwig Unger","Armin R. Mikler"],"pdf_url":"https://arxiv.org/pdf/2504.14808v1.pdf","comment":"18 pages, 4 figures, 3 tables, accepted at the 2025 25th\n  International Conference on Innovations for Community Services (I4CS), June\n  11 - 13, Munich, Germany, 2025"},{"id":"http://arxiv.org/abs/2504.14807v1","updated":"2025-04-21T02:15:37Z","published":"2025-04-21T02:15:37Z","title":"Real-Time Sleepiness Detection for Driver State Monitoring System","summary":"  A driver face monitoring system can detect driver fatigue, which is a\nsignificant factor in many accidents, using computer vision techniques. In this\npaper, we present a real-time technique for driver eye state detection. First,\nthe face is detected, and the eyes are located within the face region for\ntracking. A normalized cross-correlation-based online dynamic template matching\ntechnique, combined with Kalman filter tracking, is proposed to track the\ndetected eye positions in subsequent image frames. A support vector machine\nwith histogram of oriented gradients (HOG) features is used to classify the\nstate of the eyes as open or closed. If the eyes remain closed for a specified\nperiod, the driver is considered to be asleep, and an alarm is triggered.\n","authors":["Deepak Ghimire","Sunghwan Jeong","Sunhong Yoon","Sanghyun Park","Juhwan Choi"],"pdf_url":"https://arxiv.org/pdf/2504.14807v1.pdf","comment":"8 pages, published in GST 2015"},{"id":"http://arxiv.org/abs/2504.14805v1","updated":"2025-04-21T02:11:39Z","published":"2025-04-21T02:11:39Z","title":"Dynamic Contrastive Skill Learning with State-Transition Based Skill\n  Clustering and Dynamic Length Adjustment","summary":"  Reinforcement learning (RL) has made significant progress in various domains,\nbut scaling it to long-horizon tasks with complex decision-making remains\nchallenging. Skill learning attempts to address this by abstracting actions\ninto higher-level behaviors. However, current approaches often fail to\nrecognize semantically similar behaviors as the same skill and use fixed skill\nlengths, limiting flexibility and generalization. To address this, we propose\nDynamic Contrastive Skill Learning (DCSL), a novel framework that redefines\nskill representation and learning. DCSL introduces three key ideas:\nstate-transition based skill representation, skill similarity function\nlearning, and dynamic skill length adjustment. By focusing on state transitions\nand leveraging contrastive learning, DCSL effectively captures the semantic\ncontext of behaviors and adapts skill lengths to match the appropriate temporal\nextent of behaviors. Our approach enables more flexible and adaptive skill\nextraction, particularly in complex or noisy datasets, and demonstrates\ncompetitive performance compared to existing methods in task completion and\nefficiency.\n","authors":["Jinwoo Choi","Seung-Woo Seo"],"pdf_url":"https://arxiv.org/pdf/2504.14805v1.pdf","comment":"ICLR 2025; 23 pages, 12 figures"},{"id":"http://arxiv.org/abs/2411.04761v2","updated":"2025-04-21T02:08:50Z","published":"2024-11-07T14:59:23Z","title":"Mining the Minoria: Unknown, Under-represented, and Under-performing\n  Minority Groups","summary":"  Due to a variety of reasons, such as privacy, data in the wild often misses\nthe grouping information required for identifying minorities. On the other\nhand, it is known that machine learning models are only as good as the data\nthey are trained on and, hence, may underperform for the under-represented\nminority groups. The missing grouping information presents a dilemma for\nresponsible data scientists who find themselves in an unknown-unknown\nsituation, where not only do they not have access to the grouping attributes\nbut do not also know what groups to consider.\n  This paper is an attempt to address this dilemma. Specifically, we propose a\nminority mining problem, where we find vectors in the attribute space that\nreveal potential groups that are under-represented and under-performing.\nTechnically speaking, we propose a geometric transformation of data into a dual\nspace and use notions such as the arrangement of hyperplanes to design an\nefficient algorithm for the problem in lower dimensions. Generalizing our\nsolution to the higher dimensions is cursed by dimensionality. Therefore, we\npropose a solution based on smart exploration of the search space for such\ncases. We conduct comprehensive experiments using real-world and synthetic\ndatasets alongside the theoretical analysis. Our experiment results demonstrate\nthe effectiveness of our proposed solutions in mining the unknown,\nunder-represented, and under-performing minorities.\n","authors":["Mohsen Dehghankar","Abolfazl Asudeh"],"pdf_url":"https://arxiv.org/pdf/2411.04761v2.pdf","comment":"To appear in VLDB 2025"},{"id":"http://arxiv.org/abs/2504.14800v1","updated":"2025-04-21T01:58:29Z","published":"2025-04-21T01:58:29Z","title":"A Survey on Small Sample Imbalance Problem: Metrics, Feature Analysis,\n  and Solutions","summary":"  The small sample imbalance (S&I) problem is a major challenge in machine\nlearning and data analysis. It is characterized by a small number of samples\nand an imbalanced class distribution, which leads to poor model performance. In\naddition, indistinct inter-class feature distributions further complicate\nclassification tasks. Existing methods often rely on algorithmic heuristics\nwithout sufficiently analyzing the underlying data characteristics. We argue\nthat a detailed analysis from the data perspective is essential before\ndeveloping an appropriate solution. Therefore, this paper proposes a systematic\nanalytical framework for the S\\&I problem. We first summarize imbalance metrics\nand complexity analysis methods, highlighting the need for interpretable\nbenchmarks to characterize S&I problems. Second, we review recent solutions for\nconventional, complexity-based, and extreme S&I problems, revealing\nmethodological differences in handling various data distributions. Our summary\nfinds that resampling remains a widely adopted solution. However, we conduct\nexperiments on binary and multiclass datasets, revealing that classifier\nperformance differences significantly exceed the improvements achieved through\nresampling. Finally, this paper highlights open questions and discusses future\ntrends.\n","authors":["Shuxian Zhao","Jie Gui","Minjing Dong","Baosheng Yu","Zhipeng Gui","Lu Dong","Yuan Yan Tang","James Tin-Yau Kwok"],"pdf_url":"https://arxiv.org/pdf/2504.14800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14798v1","updated":"2025-04-21T01:56:15Z","published":"2025-04-21T01:56:15Z","title":"Verifying Robust Unlearning: Probing Residual Knowledge in Unlearned\n  Models","summary":"  Machine Unlearning (MUL) is crucial for privacy protection and content\nregulation, yet recent studies reveal that traces of forgotten information\npersist in unlearned models, enabling adversaries to resurface removed\nknowledge. Existing verification methods only confirm whether unlearning was\nexecuted, failing to detect such residual information leaks. To address this,\nwe introduce the concept of Robust Unlearning, ensuring models are\nindistinguishable from retraining and resistant to adversarial recovery. To\nempirically evaluate whether unlearning techniques meet this security standard,\nwe propose the Unlearning Mapping Attack (UMA), a post-unlearning verification\nframework that actively probes models for forgotten traces using adversarial\nqueries. Extensive experiments on discriminative and generative tasks show that\nexisting unlearning techniques remain vulnerable, even when passing existing\nverification metrics. By establishing UMA as a practical verification tool,\nthis study sets a new standard for assessing and enhancing machine unlearning\nsecurity.\n","authors":["Hao Xuan","Xingyu Li"],"pdf_url":"https://arxiv.org/pdf/2504.14798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14796v1","updated":"2025-04-21T01:53:55Z","published":"2025-04-21T01:53:55Z","title":"Edge-boosted graph learning for functional brain connectivity analysis","summary":"  Predicting disease states from functional brain connectivity is critical for\nthe early diagnosis of severe neurodegenerative diseases such as Alzheimer's\nDisease and Parkinson's Disease. Existing studies commonly employ Graph Neural\nNetworks (GNNs) to infer clinical diagnoses from node-based brain connectivity\nmatrices generated through node-to-node similarities of regionally averaged\nfMRI signals. However, recent neuroscience studies found that such node-based\nconnectivity does not accurately capture ``functional connections\" within the\nbrain. This paper proposes a novel approach to brain network analysis that\nemphasizes edge functional connectivity (eFC), shifting the focus to inter-edge\nrelationships. Additionally, we introduce a co-embedding technique to integrate\nedge functional connections effectively. Experimental results on the ADNI and\nPPMI datasets demonstrate that our method significantly outperforms\nstate-of-the-art GNN methods in classifying functional brain networks.\n","authors":["David Yang","Mostafa Abdelmegeed","John Modl","Minjeong Kim"],"pdf_url":"https://arxiv.org/pdf/2504.14796v1.pdf","comment":"Accepted at IEEE International Symposium on Biomedical Imaging (ISBI)\n  2025, 4 pages"},{"id":"http://arxiv.org/abs/2501.12761v2","updated":"2025-04-21T01:50:29Z","published":"2025-01-22T09:54:43Z","title":"Modality Unified Attack for Omni-Modality Person Re-Identification","summary":"  Deep learning based person re-identification (re-id) models have been widely\nemployed in surveillance systems. Recent studies have demonstrated that\nblack-box single-modality and cross-modality re-id models are vulnerable to\nadversarial examples (AEs), leaving the robustness of multi-modality re-id\nmodels unexplored. Due to the lack of knowledge about the specific type of\nmodel deployed in the target black-box surveillance system, we aim to generate\nmodality unified AEs for omni-modality (single-, cross- and multi-modality)\nre-id models. Specifically, we propose a novel Modality Unified Attack method\nto train modality-specific adversarial generators to generate AEs that\neffectively attack different omni-modality models. A multi-modality model is\nadopted as the surrogate model, wherein the features of each modality are\nperturbed by metric disruption loss before fusion. To collapse the common\nfeatures of omni-modality models, Cross Modality Simulated Disruption approach\nis introduced to mimic the cross-modality feature embeddings by intentionally\nfeeding images to non-corresponding modality-specific subnetworks of the\nsurrogate model. Moreover, Multi Modality Collaborative Disruption strategy is\ndevised to facilitate the attacker to comprehensively corrupt the informative\ncontent of person images by leveraging a multi modality feature collaborative\nmetric disruption loss. Extensive experiments show that our MUA method can\neffectively attack the omni-modality re-id models, achieving 55.9%, 24.4%,\n49.0% and 62.7% mean mAP Drop Rate, respectively.\n","authors":["Yuan Bian","Min Liu","Yunqi Yi","Xueping Wang","Yunfeng Ma","Yaonan Wang"],"pdf_url":"https://arxiv.org/pdf/2501.12761v2.pdf","comment":"9 pages,3 figures"},{"id":"http://arxiv.org/abs/2504.14795v1","updated":"2025-04-21T01:50:10Z","published":"2025-04-21T01:50:10Z","title":"Segmentation with Noisy Labels via Spatially Correlated Distributions","summary":"  In semantic segmentation, the accuracy of models heavily depends on the\nhigh-quality annotations. However, in many practical scenarios such as medical\nimaging and remote sensing, obtaining true annotations is not straightforward\nand usually requires significant human labor. Relying on human labor often\nintroduces annotation errors, including mislabeling, omissions, and\ninconsistency between annotators. In the case of remote sensing, differences in\nprocurement time can lead to misaligned ground truth annotations. These label\nerrors are not independently distributed, and instead usually appear in\nspatially connected regions where adjacent pixels are more likely to share the\nsame errors. To address these issues, we propose an approximate Bayesian\nestimation based on a probabilistic model that assumes training data includes\nlabel errors, incorporating the tendency for these errors to occur with spatial\ncorrelations between adjacent pixels. Bayesian inference requires computing the\nposterior distribution of label errors, which becomes intractable when spatial\ncorrelations are present. We represent the correlation of label errors between\nadjacent pixels through a Gaussian distribution whose covariance is structured\nby a Kac-Murdock-Szeg\\\"{o} (KMS) matrix, solving the computational challenges.\nThrough experiments on multiple segmentation tasks, we confirm that leveraging\nthe spatial correlation of label errors significantly improves performance.\nNotably, in specific tasks such as lung segmentation, the proposed method\nachieves performance comparable to training with clean labels under moderate\nnoise levels. Code is available at\nhttps://github.com/pfnet-research/Bayesian_SpatialCorr.\n","authors":["Ryu Tadokoro","Tsukasa Takagi","Shin-ichi Maeda"],"pdf_url":"https://arxiv.org/pdf/2504.14795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14790v1","updated":"2025-04-21T01:33:56Z","published":"2025-04-21T01:33:56Z","title":"Enhanced Data-driven Topology Design Methodology with Multi-level Mesh\n  and Correlation-based Mutation for Stress-related Multi-objective\n  Optimization","summary":"  Topology optimization (TO) serves as a widely applied structural design\napproach to tackle various engineering problems. Nevertheless,\nsensitivity-based TO methods usually struggle with solving strongly nonlinear\noptimization problems. By leveraging high capacity of deep generative model,\nwhich is an influential machine learning technique, the sensitivity-free\ndata-driven topology design (DDTD) methodology is regarded as an effective\nmeans of overcoming these issues. The DDTD methodology depends on initial\ndataset with a certain regularity, making its results highly sensitive to\ninitial dataset quality. This limits its effectiveness and generalizability,\nespecially for optimization problems without priori information. In this\nresearch, we proposed a multi-level mesh DDTD-based method with\ncorrelation-based mutation module to escape from the limitation of the quality\nof the initial dataset on the results and enhance computational efficiency. The\ncore is to employ a correlation-based mutation module to assign new geometric\nfeatures with physical meaning to the generated data, while utilizing a\nmulti-level mesh strategy to progressively enhance the refinement of the\nstructural representation, thus avoiding the maintenance of a high\ndegree-of-freedom (DOF) representation throughout the iterative process. The\nproposed multi-level mesh DDTD-based method can be driven by a low quality\ninitial dataset without the need for time-consuming construction of a specific\ndataset, thus significantly increasing generality and reducing application\ndifficulty, while further lowering computational cost of DDTD methodology.\nVarious comparison experiments with the traditional sensitivity-based TO\nmethods on stress-related strongly nonlinear problems demonstrate the\ngenerality and effectiveness of the proposed method.\n","authors":["Jun Yang","Shintaro Yamasaki"],"pdf_url":"https://arxiv.org/pdf/2504.14790v1.pdf","comment":"23 pages, 22 figures"},{"id":"http://arxiv.org/abs/2312.05693v2","updated":"2025-04-21T01:24:31Z","published":"2023-12-09T22:12:52Z","title":"Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs\n  on the Edge","summary":"  Large Language Models (LLMs) stand out for their impressive performance in\nintricate language modeling tasks. However, their demanding computational and\nmemory needs pose obstacles for broad use on edge devices. Quantization is then\nintroduced to boost LLMs' on-device efficiency. Recent works show that 8-bit or\nlower weight quantization is feasible with minimal impact on end-to-end task\nperformance, while the activation is still not quantized. On the other hand,\nmainstream commodity edge devices still struggle to execute these sub-8-bit\nquantized networks effectively. In this paper, we propose Agile-Quant, an\nactivation-guided quantization framework for popular Large Language Models\n(LLMs), and implement an end-to-end accelerator on multiple edge devices for\nfaster inference. Considering the hardware profiling and activation analysis,\nwe first introduce a basic activation quantization strategy to balance the\ntrade-off of task performance and real inference speed. Then we leverage the\nactivation-aware token pruning technique to reduce the outliers and the adverse\nimpact on attentivity. Ultimately, we utilize the SIMD-based 4-bit multiplier\nand our efficient TRIP matrix multiplication to implement the accelerator for\nLLMs on the edge. We apply our framework on different scales of LLMs including\nLLaMA, OPT, and BLOOM with 4-bit or 8-bit for the activation and 4-bit for the\nweight quantization. Experiments show that Agile-Quant achieves simultaneous\nquantization of model weights and activations while maintaining task\nperformance comparable to existing weight-only quantization methods. Moreover,\nin the 8- and 4-bit scenario, Agile-Quant achieves an on-device speedup of up\nto 2.55x compared to its FP16 counterparts across multiple edge devices,\nmarking a pioneering advancement in this domain. Code:\nhttps://github.com/shawnricecake/agile-quant\n","authors":["Xuan Shen","Peiyan Dong","Lei Lu","Zhenglun Kong","Zhengang Li","Ming Lin","Chao Wu","Yanzhi Wang"],"pdf_url":"https://arxiv.org/pdf/2312.05693v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2411.12919v2","updated":"2025-04-21T01:06:56Z","published":"2024-11-19T23:17:09Z","title":"Robust multi-coil MRI reconstruction via self-supervised denoising","summary":"  To examine the effect of incorporating self-supervised denoising as a\npre-processing step for training deep learning (DL) based reconstruction\nmethods on data corrupted by Gaussian noise. K-space data employed for training\nare typically multi-coil and inherently noisy. Although DL-based reconstruction\nmethods trained on fully sampled data can enable high reconstruction quality,\nobtaining large, noise-free datasets is impractical. We leverage Generalized\nStein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based\nreconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based\nDeep Learning (MoDL). We evaluate the impact of denoising on the performance of\nthese DL-based methods in solving accelerated multi-coil magnetic resonance\nimaging (MRI) reconstruction. The experiments were carried out on T2-weighted\nbrain and fat-suppressed proton-density knee scans. We observed that\nself-supervised denoising enhances the quality and efficiency of MRI\nreconstructions across various scenarios. Specifically, employing denoised\nimages rather than noisy counterparts when training DL networks results in\nlower normalized root mean squared error (NRMSE), higher structural similarity\nindex measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR\nlevels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB,\n14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising\nis an essential pre-processing technique capable of improving the efficacy of\nDL-based MRI reconstruction methods under diverse conditions. By refining the\nquality of input data, denoising enables training more effective DL networks,\npotentially bypassing the need for noise-free reference MRI scans.\n","authors":["Asad Aali","Marius Arvinte","Sidharth Kumar","Yamin I. Arefeen","Jonathan I. Tamir"],"pdf_url":"https://arxiv.org/pdf/2411.12919v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14782v1","updated":"2025-04-21T00:46:28Z","published":"2025-04-21T00:46:28Z","title":"Novel Concept-Oriented Synthetic Data approach for Training Generative\n  AI-Driven Crystal Grain Analysis Using Diffusion Model","summary":"  The traditional techniques for extracting polycrystalline grain structures\nfrom microscopy images, such as transmission electron microscopy (TEM) and\nscanning electron microscopy (SEM), are labour-intensive, subjective, and\ntime-consuming, limiting their scalability for high-throughput analysis. In\nthis study, we present an automated methodology integrating edge detection with\ngenerative diffusion models to effectively identify grains, eliminate noise,\nand connect broken segments in alignment with predicted grain boundaries. Due\nto the limited availability of adequate images preventing the training of deep\nmachine learning models, a new seven-stage methodology is employed to generate\nsynthetic TEM images for training. This concept-oriented synthetic data\napproach can be extended to any field of interest where the scarcity of data is\na challenge. The presented model was applied to various metals with average\ngrain sizes down to the nanoscale, producing grain morphologies from\nlow-resolution TEM images that are comparable to those obtained from advanced\nand demanding experimental techniques with an average accuracy of 97.23%.\n","authors":["Ahmed Sobhi Saleh","Kristof Croes","Hajdin Ceric","Ingrid De Wolf","Houman Zahedmanesh"],"pdf_url":"https://arxiv.org/pdf/2504.14782v1.pdf","comment":"19 Pages, 5 Figures"},{"id":"http://arxiv.org/abs/2409.11686v2","updated":"2025-04-21T00:32:35Z","published":"2024-09-18T03:56:56Z","title":"Detecting underdiagnosed medical conditions with opportunistic imaging","summary":"  Abdominal computed tomography (CT) scans are frequently performed in clinical\nsettings. Opportunistic CT involves repurposing routine CT images to extract\ndiagnostic information and is an emerging tool for detecting underdiagnosed\nconditions such as sarcopenia, hepatic steatosis, and ascites. This study\nutilizes deep learning methods to promote accurate diagnosis and clinical\ndocumentation. We analyze 2,674 inpatient CT scans to identify discrepancies\nbetween imaging phenotypes (characteristics derived from opportunistic CT\nscans) and their corresponding documentation in radiology reports and ICD\ncoding. Through our analysis, we find that only 0.5%, 3.2%, and 30.7% of scans\ndiagnosed with sarcopenia, hepatic steatosis, and ascites (respectively)\nthrough either opportunistic imaging or radiology reports were ICD-coded. Our\nfindings demonstrate opportunistic CT's potential to enhance diagnostic\nprecision and accuracy of risk adjustment models, offering advancements in\nprecision medicine.\n","authors":["Asad Aali","Andrew Johnston","Louis Blankemeier","Dave Van Veen","Laura T Derry","David Svec","Jason Hom","Robert D. Boutin","Akshay S. Chaudhari"],"pdf_url":"https://arxiv.org/pdf/2409.11686v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12365v2","updated":"2025-04-21T00:23:46Z","published":"2025-01-21T18:45:09Z","title":"Efficient Algorithm for Sparse Fourier Transform of Generalized $q$-ary\n  Functions","summary":"  Computing the Fourier transform of a $q$-ary function\n$f:\\mathbb{Z}_{q}^n\\rightarrow \\mathbb{R}$, which maps $q$-ary sequences to\nreal numbers, is an important problem in mathematics with wide-ranging\napplications in biology, signal processing, and machine learning. Previous\nstudies have shown that, under the sparsity assumption, the Fourier transform\ncan be computed efficiently using fast and sample-efficient algorithms.\nHowever, in most practical settings, the function is defined over a more\ngeneral space -- the space of generalized $q$-ary sequences $\\mathbb{Z}_{q_1}\n\\times \\mathbb{Z}_{q_2} \\times \\cdots \\times \\mathbb{Z}_{q_n}$ -- where each\n$\\mathbb{Z}_{q_i}$ corresponds to integers modulo $q_i$. Herein, we develop\nGFast, a coding theoretic algorithm that computes the $S$-sparse Fourier\ntransform of $f$ with a sample complexity of $O(Sn)$, computational complexity\nof $O(Sn \\log N)$, and a failure probability that approaches zero as\n$N=\\prod_{i=1}^n q_i \\rightarrow \\infty$ with $S = N^\\delta$ for some $0 \\leq\n\\delta < 1$. We show that a noise-robust version of GFast computes the\ntransform with a sample complexity of $O(Sn^2)$ and computational complexity of\n$O(Sn^2 \\log N)$ under the same high probability guarantees. Additionally, we\ndemonstrate that GFast computes the sparse Fourier transform of generalized\n$q$-ary functions $8\\times$ faster using $16\\times$ fewer samples on synthetic\nexperiments, and enables explaining real-world heart disease diagnosis and\nprotein fitness models using up to $13\\times$ fewer samples compared to\nexisting Fourier algorithms applied to the most efficient parameterization of\nthe models as $q$-ary functions.\n","authors":["Darin Tsui","Kunal Talreja","Amirali Aghazadeh"],"pdf_url":"https://arxiv.org/pdf/2501.12365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11933v2","updated":"2025-04-21T00:07:03Z","published":"2024-10-15T17:09:34Z","title":"Beyond Sequence: Impact of Geometric Context for RNA Property Prediction","summary":"  Accurate prediction of RNA properties, such as stability and interactions, is\ncrucial for advancing our understanding of biological processes and developing\nRNA-based therapeutics. RNA structures can be represented as 1D sequences, 2D\ntopological graphs, or 3D all-atom models, each offering different insights\ninto its function. Existing works predominantly focus on 1D sequence-based\nmodels, which overlook the geometric context provided by 2D and 3D geometries.\nThis study presents the first systematic evaluation of incorporating explicit\n2D and 3D geometric information into RNA property prediction, considering not\nonly performance but also real-world challenges such as limited data\navailability, partial labeling, sequencing noise, and computational efficiency.\nTo this end, we introduce a newly curated set of RNA datasets with enhanced 2D\nand 3D structural annotations, providing a resource for model evaluation on RNA\ndata. Our findings reveal that models with explicit geometry encoding generally\noutperform sequence-based models, with an average prediction RMSE reduction of\naround 12% across all various RNA tasks and excelling in low-data and partial\nlabeling regimes, underscoring the value of explicitly incorporating geometric\ncontext. On the other hand, geometry-unaware sequence-based models are more\nrobust under sequencing noise but often require around $2-5\\times$ training\ndata to match the performance of geometry-aware models. Our study offers\nfurther insights into the trade-offs between different RNA representations in\npractical applications and addresses a significant gap in evaluating deep\nlearning models for RNA tasks.\n","authors":["Junjie Xu","Artem Moskalev","Tommaso Mansi","Mangal Prakash","Rui Liao"],"pdf_url":"https://arxiv.org/pdf/2410.11933v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06407v2","updated":"2025-04-21T00:04:26Z","published":"2024-10-08T22:28:30Z","title":"A Skewness-Based Criterion for Addressing Heteroscedastic Noise in\n  Causal Discovery","summary":"  Real-world data often violates the equal-variance assumption\n(homoscedasticity), making it essential to account for heteroscedastic noise in\ncausal discovery. In this work, we explore heteroscedastic symmetric noise\nmodels (HSNMs), where the effect $Y$ is modeled as $Y = f(X) + \\sigma(X)N$,\nwith $X$ as the cause and $N$ as independent noise following a symmetric\ndistribution. We introduce a novel criterion for identifying HSNMs based on the\nskewness of the score (i.e., the gradient of the log density) of the data\ndistribution. This criterion establishes a computationally tractable\nmeasurement that is zero in the causal direction but nonzero in the anticausal\ndirection, enabling the causal direction discovery. We extend this\nskewness-based criterion to the multivariate setting and propose SkewScore, an\nalgorithm that handles heteroscedastic noise without requiring the extraction\nof exogenous noise. We also conduct a case study on the robustness of SkewScore\nin a bivariate model with a latent confounder, providing theoretical insights\ninto its performance. Empirical studies further validate the effectiveness of\nthe proposed method.\n","authors":["Yingyu Lin","Yuxing Huang","Wenqin Liu","Haoran Deng","Ignavier Ng","Kun Zhang","Mingming Gong","Yi-An Ma","Biwei Huang"],"pdf_url":"https://arxiv.org/pdf/2410.06407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14773v1","updated":"2025-04-21T00:02:50Z","published":"2025-04-21T00:02:50Z","title":"PLANET: A Collection of Benchmarks for Evaluating LLMs' Planning\n  Capabilities","summary":"  Planning is central to agents and agentic AI. The ability to plan, e.g.,\ncreating travel itineraries within a budget, holds immense potential in both\nscientific and commercial contexts. Moreover, optimal plans tend to require\nfewer resources compared to ad-hoc methods. To date, a comprehensive\nunderstanding of existing planning benchmarks appears to be lacking. Without\nit, comparing planning algorithms' performance across domains or selecting\nsuitable algorithms for new scenarios remains challenging. In this paper, we\nexamine a range of planning benchmarks to identify commonly used testbeds for\nalgorithm development and highlight potential gaps. These benchmarks are\ncategorized into embodied environments, web navigation, scheduling, games and\npuzzles, and everyday task automation. Our study recommends the most\nappropriate benchmarks for various algorithms and offers insights to guide\nfuture benchmark development.\n","authors":["Haoming Li","Zhaoliang Chen","Jonathan Zhang","Fei Liu"],"pdf_url":"https://arxiv.org/pdf/2504.14773v1.pdf","comment":"10 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2504.15066v1","updated":"2025-04-21T12:51:54Z","published":"2025-04-21T12:51:54Z","title":"Chinese-LiPS: A Chinese audio-visual speech recognition dataset with\n  Lip-reading and Presentation Slides","summary":"  Incorporating visual modalities to assist Automatic Speech Recognition (ASR)\ntasks has led to significant improvements. However, existing Audio-Visual\nSpeech Recognition (AVSR) datasets and methods typically rely solely on\nlip-reading information or speaking contextual video, neglecting the potential\nof combining these different valuable visual cues within the speaking context.\nIn this paper, we release a multimodal Chinese AVSR dataset, Chinese-LiPS,\ncomprising 100 hours of speech, video, and corresponding manual transcription,\nwith the visual modality encompassing both lip-reading information and the\npresentation slides used by the speaker. Based on Chinese-LiPS, we develop a\nsimple yet effective pipeline, LiPS-AVSR, which leverages both lip-reading and\npresentation slide information as visual modalities for AVSR tasks. Experiments\nshow that lip-reading and presentation slide information improve ASR\nperformance by approximately 8\\% and 25\\%, respectively, with a combined\nperformance improvement of about 35\\%. The dataset is available at\nhttps://kiri0824.github.io/Chinese-LiPS/\n","authors":["Jinghua Zhao","Yuhang Jia","Shiyao Wang","Jiaming Zhou","Hui Wang","Yong Qin"],"pdf_url":"https://arxiv.org/pdf/2504.15066v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.14904v1","updated":"2025-04-21T07:20:19Z","published":"2025-04-21T07:20:19Z","title":"VLM as Policy: Common-Law Content Moderation Framework for Short Video\n  Platform","summary":"  Exponentially growing short video platforms (SVPs) face significant\nchallenges in moderating content detrimental to users' mental health,\nparticularly for minors. The dissemination of such content on SVPs can lead to\ncatastrophic societal consequences. Although substantial efforts have been\ndedicated to moderating such content, existing methods suffer from critical\nlimitations: (1) Manual review is prone to human bias and incurs high\noperational costs. (2) Automated methods, though efficient, lack nuanced\ncontent understanding, resulting in lower accuracy. (3) Industrial moderation\nregulations struggle to adapt to rapidly evolving trends due to long update\ncycles. In this paper, we annotate the first SVP content moderation benchmark\nwith authentic user/reviewer feedback to fill the absence of benchmark in this\nfield. Then we evaluate various methods on the benchmark to verify the\nexistence of the aforementioned limitations. We further propose our common-law\ncontent moderation framework named KuaiMod to address these challenges. KuaiMod\nconsists of three components: training data construction, offline adaptation,\nand online deployment & refinement. Leveraging large vision language model\n(VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video\ntoxicity based on sparse user feedback and fosters dynamic moderation policy\nwith rapid update speed and high accuracy. Offline experiments and large-scale\nonline A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the\nbest moderation performance on our benchmark. The deployment of KuaiMod reduces\nthe user reporting rate by 20% and its application in video recommendation\nincreases both Daily Active User (DAU) and APP Usage Time (AUT) on several\nKuaishou scenarios. We have open-sourced our benchmark at\nhttps://kuaimod.github.io.\n","authors":["Xingyu Lu","Tianke Zhang","Chang Meng","Xiaobei Wang","Jinpeng Wang","YiFan Zhang","Shisong Tang","Changyi Liu","Haojie Ding","Kaiyu Jiang","Kaiyu Tang","Bin Wen","Hai-Tao Zheng","Fan Yang","Tingting Gao","Di Zhang","Kun Gai"],"pdf_url":"https://arxiv.org/pdf/2504.14904v1.pdf","comment":"20 pages, 6 figures"}]},"2025-04-20T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2504.14772v1","updated":"2025-04-20T23:50:23Z","published":"2025-04-20T23:50:23Z","title":"Knowledge Distillation and Dataset Distillation of Large Language\n  Models: Emerging Trends, Challenges, and Future Directions","summary":"  The exponential growth of Large Language Models (LLMs) continues to highlight\nthe need for efficient strategies to meet ever-expanding computational and data\ndemands. This survey provides a comprehensive analysis of two complementary\nparadigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both\naimed at compressing LLMs while preserving their advanced reasoning\ncapabilities and linguistic diversity. We first examine key methodologies in\nKD, such as task-specific alignment, rationale-based training, and\nmulti-teacher frameworks, alongside DD techniques that synthesize compact,\nhigh-impact datasets through optimization-based gradient matching, latent space\nregularization, and generative synthesis. Building on these foundations, we\nexplore how integrating KD and DD can produce more effective and scalable\ncompression strategies. Together, these approaches address persistent\nchallenges in model scalability, architectural heterogeneity, and the\npreservation of emergent LLM abilities. We further highlight applications\nacross domains such as healthcare and education, where distillation enables\nefficient deployment without sacrificing performance. Despite substantial\nprogress, open challenges remain in preserving emergent reasoning and\nlinguistic diversity, enabling efficient adaptation to continually evolving\nteacher models and datasets, and establishing comprehensive evaluation\nprotocols. By synthesizing methodological innovations, theoretical foundations,\nand practical insights, our survey charts a path toward sustainable,\nresource-efficient LLMs through the tighter integration of KD and DD\nprinciples.\n","authors":["Luyang Fang","Xiaowei Yu","Jiazhang Cai","Yongkai Chen","Shushan Wu","Zhengliang Liu","Zhenyuan Yang","Haoran Lu","Xilin Gong","Yufang Liu","Terry Ma","Wei Ruan","Ali Abbasi","Jing Zhang","Tao Wang","Ehsan Latif","Wei Liu","Wei Zhang","Soheil Kolouri","Xiaoming Zhai","Dajiang Zhu","Wenxuan Zhong","Tianming Liu","Ping Ma"],"pdf_url":"https://arxiv.org/pdf/2504.14772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14766v1","updated":"2025-04-20T23:38:16Z","published":"2025-04-20T23:38:16Z","title":"Disentangling Linguistic Features with Dimension-Wise Analysis of Vector\n  Embeddings","summary":"  Understanding the inner workings of neural embeddings, particularly in models\nsuch as BERT, remains a challenge because of their high-dimensional and opaque\nnature. This paper proposes a framework for uncovering the specific dimensions\nof vector embeddings that encode distinct linguistic properties (LPs). We\nintroduce the Linguistically Distinct Sentence Pairs (LDSP-10) dataset, which\nisolates ten key linguistic features such as synonymy, negation, tense, and\nquantity. Using this dataset, we analyze BERT embeddings with various methods,\nincluding the Wilcoxon signed-rank test, mutual information, and recursive\nfeature elimination, to identify the most influential dimensions for each LP.\nWe introduce a new metric, the Embedding Dimension Impact (EDI) score, which\nquantifies the relevance of each embedding dimension to a LP. Our findings show\nthat certain properties, such as negation and polarity, are robustly encoded in\nspecific dimensions, while others, like synonymy, exhibit more complex\npatterns. This study provides insights into the interpretability of embeddings,\nwhich can guide the development of more transparent and optimized language\nmodels, with implications for model bias mitigation and the responsible\ndeployment of AI systems.\n","authors":["Saniya Karwa","Navpreet Singh"],"pdf_url":"https://arxiv.org/pdf/2504.14766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17116v2","updated":"2025-04-20T21:50:03Z","published":"2024-11-26T05:10:04Z","title":"Star Attention: Efficient LLM Inference over Long Sequences","summary":"  Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.\n","authors":["Shantanu Acharya","Fei Jia","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2411.17116v2.pdf","comment":"Code: https://github.com/NVIDIA/Star-Attention"},{"id":"http://arxiv.org/abs/2504.14738v1","updated":"2025-04-20T21:04:23Z","published":"2025-04-20T21:04:23Z","title":"PROMPTEVALS: A Dataset of Assertions and Guardrails for Custom\n  Production Large Language Model Pipelines","summary":"  Large language models (LLMs) are increasingly deployed in specialized\nproduction data processing pipelines across diverse domains -- such as finance,\nmarketing, and e-commerce. However, when running them in production across many\ninputs, they often fail to follow instructions or meet developer expectations.\nTo improve reliability in these applications, creating assertions or guardrails\nfor LLM outputs to run alongside the pipelines is essential. Yet, determining\nthe right set of assertions that capture developer requirements for a task is\nchallenging. In this paper, we introduce PROMPTEVALS, a dataset of 2087 LLM\npipeline prompts with 12623 corresponding assertion criteria, sourced from\ndevelopers using our open-source LLM pipeline tools. This dataset is 5x larger\nthan previous collections. Using a hold-out test split of PROMPTEVALS as a\nbenchmark, we evaluated closed- and open-source models in generating relevant\nassertions. Notably, our fine-tuned Mistral and Llama 3 models outperform\nGPT-4o by 20.93% on average, offering both reduced latency and improved\nperformance. We believe our dataset can spur further research in LLM\nreliability, alignment, and prompt engineering.\n","authors":["Reya Vir","Shreya Shankar","Harrison Chase","Will Fu-Hinthorn","Aditya Parameswaran"],"pdf_url":"https://arxiv.org/pdf/2504.14738v1.pdf","comment":"Accepted to NAACL 2025 Main Conference"},{"id":"http://arxiv.org/abs/2410.09344v2","updated":"2025-04-20T20:53:39Z","published":"2024-10-12T03:21:58Z","title":"DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned\n  Models","summary":"  Storing open-source fine-tuned models separately introduces redundancy and\nincreases response times in applications utilizing multiple models.\nDelta-parameter pruning (DPP), particularly the random drop and rescale (DARE)\nmethod proposed by Yu et al., addresses this by pruning the majority of delta\nparameters--the differences between fine-tuned and pre-trained model\nweights--while typically maintaining minimal performance loss. However, DARE\nfails when either the pruning rate or the magnitude of the delta parameters is\nlarge. We highlight two key reasons for this failure: (1) an excessively large\nrescaling factor as pruning rates increase, and (2) high mean and variance in\nthe delta parameters. To push DARE's limits, we introduce DAREx (DARE the\neXtreme), which features two algorithmic improvements: (1) DAREx-q, a rescaling\nfactor modification that significantly boosts performance at high pruning rates\n(e.g., >30 % on COLA and SST2 for encoder models, with even greater gains in\ndecoder models), and (2) DAREx-L2, which combines DARE with AdamR, an\nin-training method that applies appropriate delta regularization before DPP. We\nalso demonstrate that DAREx-q can be seamlessly combined with vanilla\nparameter-efficient fine-tuning techniques like LoRA and can facilitate\nstructural DPP. Additionally, we revisit the application of importance-based\npruning techniques within DPP, demonstrating that they outperform random-based\nmethods when delta parameters are large. Through this comprehensive study, we\ndevelop a pipeline for selecting the most appropriate DPP method under various\npractical scenarios.\n","authors":["Wenlong Deng","Yize Zhao","Vala Vakilian","Minghui Chen","Xiaoxiao Li","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2410.09344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17211v2","updated":"2025-04-20T19:49:45Z","published":"2025-03-21T15:20:28Z","title":"A Language Anchor-Guided Method for Robust Noisy Domain Generalization","summary":"  Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels.\n","authors":["Zilin Dai","Lehong Wang","Fangzhou Lin","Yidong Wang","Zhigang Li","Kazunori D Yamada","Ziming Zhang","Wang Lu"],"pdf_url":"https://arxiv.org/pdf/2503.17211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16358v2","updated":"2025-04-20T19:49:10Z","published":"2025-02-22T21:14:18Z","title":"Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA\n  Dataset with Answer Plausibility Scores","summary":"  Large Language Models (LLMs) are revolutionizing information retrieval, with\nchatbots becoming an important source for answering user queries. As by their\ndesign, LLMs prioritize generating correct answers, the value of highly\nplausible yet incorrect answers (candidate answers) tends to be overlooked.\nHowever, such answers can still prove useful, for example, they can play a\ncrucial role in tasks like Multiple-Choice Question Answering (MCQA) and QA\nRobustness Assessment (QARA). Existing QA datasets primarily focus on correct\nanswers without explicit consideration of the plausibility of other candidate\nanswers, limiting opportunity for more nuanced evaluations of models. To\naddress this gap, we introduce PlausibleQA, a large-scale dataset comprising\n10,000 questions and 100,000 candidate answers, each annotated with\nplausibility scores and justifications for their selection. Additionally, the\ndataset includes 900,000 justifications for pairwise comparisons between\ncandidate answers, further refining plausibility assessments. We evaluate\nPlausibleQA through human assessments and empirical experiments, demonstrating\nits utility in MCQA and QARA analysis. Our findings show that\nplausibility-aware approaches are effective for MCQA distractor generation and\nQARA. We release PlausibleQA as a resource for advancing QA research and\nenhancing LLM performance in distinguishing plausible distractors from correct\nanswers.\n","authors":["Jamshid Mozafari","Abdelrahman Abdallah","Bhawna Piryani","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.16358v2.pdf","comment":"Accepted at SIGIR 2025"},{"id":"http://arxiv.org/abs/2412.01626v3","updated":"2025-04-20T19:43:24Z","published":"2024-12-02T15:44:19Z","title":"WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation","summary":"  The use of Large Language Models (LLMs) has increased significantly with\nusers frequently asking questions to chatbots. In the time when information is\nreadily accessible, it is crucial to stimulate and preserve human cognitive\nabilities and maintain strong reasoning skills. This paper addresses such\nchallenges by promoting the use of hints as an alternative or a supplement to\ndirect answers. We first introduce a manually constructed hint dataset,\nWikiHint, which is based on Wikipedia and includes 5,000 hints created for\n1,000 questions. We then finetune open-source LLMs for hint generation in\nanswer-aware and answer-agnostic contexts. We assess the effectiveness of the\nhints with human participants who answer questions with and without the aid of\nhints. Additionally, we introduce a lightweight evaluation method, HintRank, to\nevaluate and rank hints in both answer-aware and answer-agnostic settings. Our\nfindings show that (a) the dataset helps generate more effective hints, (b)\nincluding answer information along with questions generally improves the\nquality of generated hints, and (c) encoder-based models perform better than\ndecoder-based models in hint ranking.\n","authors":["Jamshid Mozafari","Florian Gerhold","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2412.01626v3.pdf","comment":"Accepted at SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.10340v2","updated":"2025-04-20T19:03:57Z","published":"2025-04-14T15:48:56Z","title":"Forecasting from Clinical Textual Time Series: Adaptations of the\n  Encoder and Decoder Language Model Families","summary":"  Clinical case reports encode rich, temporal patient trajectories that are\noften underexploited by traditional machine learning methods relying on\nstructured data. In this work, we introduce the forecasting problem from\ntextual time series, where timestamped clinical findings -- extracted via an\nLLM-assisted annotation pipeline -- serve as the primary input for prediction.\nWe systematically evaluate a diverse suite of models, including fine-tuned\ndecoder-based large language models and encoder-based transformers, on tasks of\nevent occurrence prediction, temporal ordering, and survival analysis. Our\nexperiments reveal that encoder-based models consistently achieve higher F1\nscores and superior temporal concordance for short- and long-horizon event\nforecasting, while fine-tuned masking approaches enhance ranking performance.\nIn contrast, instruction-tuned decoder models demonstrate a relative advantage\nin survival analysis, especially in early prognosis settings. Our sensitivity\nanalyses further demonstrate the importance of time ordering, which requires\nclinical time series construction, as compared to text ordering, the format of\nthe text inputs that LLMs are classically trained on. This highlights the\nadditional benefit that can be ascertained from time-ordered corpora, with\nimplications for temporal tasks in the era of widespread LLM use.\n","authors":["Shahriar Noroozizadeh","Sayantan Kumar","Jeremy C. Weiss"],"pdf_url":"https://arxiv.org/pdf/2504.10340v2.pdf","comment":"Machine Learning for Healthcare (MLHC 2025)"},{"id":"http://arxiv.org/abs/2504.14707v1","updated":"2025-04-20T18:51:08Z","published":"2025-04-20T18:51:08Z","title":"Evaluating BERTopic on Open-Ended Data: A Case Study with Belgian Dutch\n  Daily Narratives","summary":"  This study explores BERTopic's potential for modeling open-ended Belgian\nDutch daily narratives, contrasting its performance with Latent Dirichlet\nAllocation (LDA) and KMeans. Although LDA scores well on certain automated\nmetrics, human evaluations reveal semantically irrelevant co-occurrences,\nhighlighting the limitations of purely statistic-based methods. In contrast,\nBERTopic's reliance on contextual embeddings yields culturally resonant themes,\nunderscoring the importance of hybrid evaluation frameworks that account for\nmorphologically rich languages. KMeans performed less coherently than prior\nresearch suggested, pointing to the unique challenges posed by personal\nnarratives. Our findings emphasize the need for robust generalization in NLP\nmodels, especially in underrepresented linguistic contexts.\n","authors":["Ratna Kandala","Katie Hoemann"],"pdf_url":"https://arxiv.org/pdf/2504.14707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17890v4","updated":"2025-04-20T18:07:13Z","published":"2024-05-28T07:12:06Z","title":"SLMRec: Distilling Large Language Models into Small for Sequential\n  Recommendation","summary":"  Sequential Recommendation (SR) task involves predicting the next item a user\nis likely to interact with, given their past interactions. The SR models\nexamine the sequence of a user's actions to discern more complex behavioral\npatterns and temporal dynamics. Recent research demonstrates the great impact\nof LLMs on sequential recommendation systems, either viewing sequential\nrecommendation as language modeling or serving as the backbone for user\nrepresentation. Although these methods deliver outstanding performance, there\nis scant evidence of the necessity of a large language model and how large the\nlanguage model is needed, especially in the sequential recommendation scene.\nMeanwhile, due to the huge size of LLMs, it is inefficient and impractical to\napply a LLM-based model in real-world platforms that often need to process\nbillions of traffic logs daily. In this paper, we explore the influence of\nLLMs' depth by conducting extensive experiments on large-scale industry\ndatasets. Surprisingly, our motivational experiments reveal that most\nintermediate layers of LLMs are redundant, indicating that pruning the\nremaining layers can still maintain strong performance. Motivated by this\ninsight, we empower small language models for SR, namely SLMRec, which adopt a\nsimple yet effective knowledge distillation method. Moreover, SLMRec is\northogonal to other post-training efficiency techniques, such as quantization\nand pruning, so that they can be leveraged in combination. Comprehensive\nexperimental results illustrate that the proposed SLMRec model attains the best\nperformance using only 13% of the parameters found in LLM-based recommendation\nmodels while simultaneously achieving up to 6.6x and 8.0x speedups in training\nand inference time costs, respectively. Besides, we provide a theoretical\njustification for why small language models can perform comparably to large\nlanguage models in SR.\n","authors":["Wujiang Xu","Qitian Wu","Zujie Liang","Jiaojiao Han","Xuying Ning","Yunxiao Shi","Wenfang Lin","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.17890v4.pdf","comment":"International Conference on Learning Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2504.14692v1","updated":"2025-04-20T17:53:56Z","published":"2025-04-20T17:53:56Z","title":"OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual\n  Understanding","summary":"  The practical deployment of medical vision-language models (Med-VLMs)\nnecessitates seamless integration of textual data with diverse visual\nmodalities, including 2D/3D images and videos, yet existing models typically\nemploy separate encoders for different modalities. To address this limitation,\nwe present OmniV-Med, a unified framework for multimodal medical understanding.\nOur technical contributions are threefold: First, we construct\nOmniV-Med-Instruct, a comprehensive multimodal medical dataset containing 252K\ninstructional samples spanning 14 medical image modalities and 11 clinical\ntasks. Second, we devise a rotary position-adaptive encoder that processes\nmulti-resolution 2D/3D images and videos within a unified architecture,\ndiverging from conventional modality-specific encoders. Third, we introduce a\nmedical-aware token pruning mechanism that exploits spatial-temporal redundancy\nin volumetric data (e.g., consecutive CT slices) and medical videos,\neffectively reducing 60\\% of visual tokens without performance degradation.\nEmpirical evaluations demonstrate that OmniV-Med-7B achieves state-of-the-art\nperformance on 7 benchmarks spanning 2D/3D medical imaging and video\nunderstanding tasks. Notably, our lightweight variant (OmniV-Med-1.5B) attains\ncomparable performance while requiring only 8 RTX3090 GPUs for training and\nsupporting efficient long-video inference. Data, code and model will be\nreleased.\n","authors":["Songtao Jiang","Yuan Wang","Sibo Song","Yan Zhang","Zijie Meng","Bohan Lei","Jian Wu","Jimeng Sun","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2504.14692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14690v1","updated":"2025-04-20T17:43:47Z","published":"2025-04-20T17:43:47Z","title":"FarsEval-PKBETS: A new diverse benchmark for evaluating Persian large\n  language models","summary":"  Research on evaluating and analyzing large language models (LLMs) has been\nextensive for resource-rich languages such as English, yet their performance in\nlanguages such as Persian has received considerably less attention. This paper\nintroduces FarsEval-PKBETS benchmark, a subset of FarsEval project for\nevaluating large language models in Persian. This benchmark consists of 4000\nquestions and answers in various formats, including multiple choice, short\nanswer and descriptive responses. It covers a wide range of domains and\ntasks,including medicine, law, religion, Persian language, encyclopedic\nknowledge, human preferences, social knowledge, ethics and bias, text\ngeneration, and respecting others' rights. This bechmark incorporates\nlinguistics, cultural, and local considerations relevant to the Persian\nlanguage and Iran. To ensure the questions are challenging for current LLMs,\nthree models -- Llama3-70B, PersianMind, and Dorna -- were evaluated using this\nbenchmark. Their average accuracy was below 50%, meaning they provided fully\ncorrect answers to fewer than half of the questions. These results indicate\nthat current language models are still far from being able to solve this\nbenchmark\n","authors":["Mehrnoush Shamsfard","Zahra Saaberi","Mostafa Karimi manesh","Seyed Mohammad Hossein Hashemi","Zahra Vatankhah","Motahareh Ramezani","Niki Pourazin","Tara Zare","Maryam Azimi","Sarina Chitsaz","Sama Khoraminejad","Morteza Mahdavi Mortazavi","Mohammad Mahdi Chizari","Sahar Maleki","Seyed Soroush Majd","Mostafa Masumi","Sayed Ali Musavi Khoeini","Amir Mohseni","Sogol Alipour"],"pdf_url":"https://arxiv.org/pdf/2504.14690v1.pdf","comment":"24 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2504.14669v1","updated":"2025-04-20T16:20:30Z","published":"2025-04-20T16:20:30Z","title":"Trans-Zero: Self-Play Incentivizes Large Language Models for\n  Multilingual Translation Without Parallel Data","summary":"  The rise of Large Language Models (LLMs) has reshaped machine translation\n(MT), but multilingual MT still relies heavily on parallel data for supervised\nfine-tuning (SFT), facing challenges like data scarcity for low-resource\nlanguages and catastrophic forgetting. To address these issues, we propose\nTRANS-ZERO, a self-play framework that leverages only monolingual data and the\nintrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic\nMonte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong\ntranslation performance that rivals supervised methods. Experiments demonstrate\nthat this approach not only matches the performance of models trained on\nlarge-scale parallel data but also excels in non-English translation\ndirections. Further analysis reveals that G-MCTS itself significantly enhances\ntranslation quality by exploring semantically consistent candidates through\niterative translations, providing a robust foundation for the framework's\nsuccuss.\n","authors":["Wei Zou","Sen Yang","Yu Bao","Shujian Huang","Jiajun Chen","Shanbo Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.14669v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.00872v2","updated":"2025-04-20T16:01:12Z","published":"2024-09-01T23:36:34Z","title":"Self-evolving Agents with reflective and memory-augmented abilities","summary":"  Large language models (LLMs) have made significant advances in the field of\nnatural language processing, but they still face challenges such as continuous\ndecision-making. In this research, we propose a novel framework by integrating\niterative feedback, reflective mechanisms, and a memory optimization mechanism\nbased on the Ebbinghaus forgetting curve, it significantly enhances the agents'\ncapabilities in handling multi-tasking and long-span information.\n","authors":["Xuechen Liang","Yangfan He","Yinghui Xia","Xinyuan Song","Jianhui Wang","Meiling Tao","Li Sun","Xinhang Yuan","Jiayi Su","Keqin Li","Jiaqi Chen","Jinsong Yang","Siyuan Chen","Tianyu Shi"],"pdf_url":"https://arxiv.org/pdf/2409.00872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14657v1","updated":"2025-04-20T15:37:05Z","published":"2025-04-20T15:37:05Z","title":"A Case Study Exploring the Current Landscape of Synthetic Medical Record\n  Generation with Commercial LLMs","summary":"  Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to\ncreate privacy preserving and harmonized structured data, supporting numerous\napplications in healthcare. Key benefits of synthetic data include precise\ncontrol over the data schema, improved fairness and representation of patient\npopulations, and the ability to share datasets without concerns about\ncompromising real individuals privacy. Consequently, the AI community has\nincreasingly turned to Large Language Models (LLMs) to generate synthetic data\nacross various domains. However, a significant challenge in healthcare is\nensuring that synthetic health records reliably generalize across different\nhospitals, a long standing issue in the field. In this work, we evaluate the\ncurrent state of commercial LLMs for generating synthetic data and investigate\nmultiple aspects of the generation process to identify areas where these models\nexcel and where they fall short. Our main finding from this work is that while\nLLMs can reliably generate synthetic health records for smaller subsets of\nfeatures, they struggle to preserve realistic distributions and correlations as\nthe dimensionality of the data increases, ultimately limiting their ability to\ngeneralize across diverse hospital settings.\n","authors":["Yihan Lin","Zhirong Bella Yu","Simon Lee"],"pdf_url":"https://arxiv.org/pdf/2504.14657v1.pdf","comment":"Accepted at the Conference of Health, Inference, Learning (CHIL 2025)\n  in Berkeley, CA. To appear in PMLR later in 2025"},{"id":"http://arxiv.org/abs/2504.14655v1","updated":"2025-04-20T15:28:16Z","published":"2025-04-20T15:28:16Z","title":"LeetCodeDataset: A Temporal Dataset for Robust Evaluation and Efficient\n  Training of Code LLMs","summary":"  We introduce LeetCodeDataset, a high-quality benchmark for evaluating and\ntraining code-generation models, addressing two key challenges in LLM research:\nthe lack of reasoning-focused coding benchmarks and self-contained training\ntestbeds. By curating LeetCode Python problems with rich metadata, broad\ncoverage, 100+ test cases per problem, and temporal splits (pre/post July\n2024), our dataset enables contamination-free evaluation and efficient\nsupervised fine-tuning (SFT). Experiments show reasoning models significantly\noutperform non-reasoning counterparts, while SFT with only 2.6K model-generated\nsolutions achieves performance comparable to 110K-sample counterparts. The\ndataset and evaluation framework are available on Hugging Face and Github.\n","authors":["Yunhui Xia","Wei Shen","Yan Wang","Jason Klein Liu","Huifeng Sun","Siyue Wu","Jian Hu","Xiaolong Xu"],"pdf_url":"https://arxiv.org/pdf/2504.14655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.01337v2","updated":"2025-04-20T15:13:33Z","published":"2025-04-02T03:51:59Z","title":"Advancing MoE Efficiency: A Collaboration-Constrained Routing (C2R)\n  Strategy for Better Expert Parallelism Design","summary":"  Mixture-of-Experts (MoE) has successfully scaled up models while maintaining\nnearly constant computing costs. By employing a gating network to route input\ntokens, it selectively activates a subset of expert networks to process the\ncorresponding token embeddings. However, in practice, the efficiency of MoE is\nchallenging to achieve due to two key reasons: imbalanced expert activation,\nwhich leads to substantial idle time during model or expert parallelism, and\ninsufficient capacity utilization; massive communication overhead, induced by\nnumerous expert routing combinations in expert parallelism at the system level.\nPrevious works typically formulate it as the load imbalance issue characterized\nby the gating network favoring certain experts over others or attribute it to\nstatic execution which fails to adapt to the dynamic expert workload at\nruntime. In this paper, we exploit it from a brand new perspective, a\nhigher-order view and analysis of MoE routing policies: expert collaboration\nand specialization where some experts tend to activate broadly with others\n(collaborative), while others are more likely to activate only with a specific\nsubset of experts (specialized). Our experiments reveal that most experts tend\nto be overly collaborative, leading to increased communication overhead from\nrepeatedly sending tokens to different accelerators. To this end, we propose a\nnovel collaboration-constrained routing (C2R) strategy to encourage more\nspecialized expert groups, as well as to improve expert utilization, and\npresent an efficient implementation of MoE that further leverages expert\nspecialization. We achieve an average performance improvement of 0.51% and\n0.33% on LLaMA-MoE and Qwen-MoE respectively across ten downstream NLP\nbenchmarks, and reduce the all2all communication costs between GPUs, bringing\nan extra 20%-30% total running time savings on top of the existing SoTA, i.e.\nMegaBlocks.\n","authors":["Mohan Zhang","Pingzhi Li","Jie Peng","Mufan Qiu","Tianlong Chen"],"pdf_url":"https://arxiv.org/pdf/2504.01337v2.pdf","comment":"NAACL 2025, SAC award for Low-resource Methods for NLP"},{"id":"http://arxiv.org/abs/2504.14640v1","updated":"2025-04-20T14:44:18Z","published":"2025-04-20T14:44:18Z","title":"Risk Assessment Framework for Code LLMs via Leveraging Internal States","summary":"  The pre-training paradigm plays a key role in the success of Large Language\nModels (LLMs), which have been recognized as one of the most significant\nadvancements of AI recently. Building on these breakthroughs, code LLMs with\nadvanced coding capabilities bring huge impacts on software engineering,\nshowing the tendency to become an essential part of developers' daily routines.\nHowever, the current code LLMs still face serious challenges related to\ntrustworthiness, as they can generate incorrect, insecure, or unreliable code.\nRecent exploratory studies find that it can be promising to detect such risky\noutputs by analyzing LLMs' internal states, akin to how the human brain\nunconsciously recognizes its own mistakes. Yet, most of these approaches are\nlimited to narrow sub-domains of LLM operations and fall short of achieving\nindustry-level scalability and practicability. To address these challenges, in\nthis paper, we propose PtTrust, a two-stage risk assessment framework for code\nLLM based on internal state pre-training, designed to integrate seamlessly with\nthe existing infrastructure of software companies. The core idea is that the\nrisk assessment framework could also undergo a pre-training process similar to\nLLMs. Specifically, PtTrust first performs unsupervised pre-training on\nlarge-scale unlabeled source code to learn general representations of LLM\nstates. Then, it uses a small, labeled dataset to train a risk predictor. We\ndemonstrate the effectiveness of PtTrust through fine-grained, code line-level\nrisk assessment and demonstrate that it generalizes across tasks and different\nprogramming languages. Further experiments also reveal that PtTrust provides\nhighly intuitive and interpretable features, fostering greater user trust. We\nbelieve PtTrust makes a promising step toward scalable and trustworthy\nassurance for code LLMs.\n","authors":["Yuheng Huang","Lei Ma","Keizaburo Nishikino","Takumi Akazaki"],"pdf_url":"https://arxiv.org/pdf/2504.14640v1.pdf","comment":"To appear in the 33rd ACM International Conference on the Foundations\n  of Software Engineering (FSE Companion'25 Industry Track), June 23-28, 2025,\n  Trondheim, Norway. This work was supported by Fujitsu Limited"},{"id":"http://arxiv.org/abs/2504.14633v1","updated":"2025-04-20T14:23:31Z","published":"2025-04-20T14:23:31Z","title":"Harnessing Generative LLMs for Enhanced Financial Event Entity\n  Extraction Performance","summary":"  Financial event entity extraction is a crucial task for analyzing market\ndynamics and building financial knowledge graphs, yet it presents significant\nchallenges due to the specialized language and complex structures in financial\ntexts. Traditional approaches often rely on sequence labeling models, which can\nstruggle with long-range dependencies and the inherent complexity of extracting\nmultiple, potentially overlapping entities. Motivated by the advanced language\nunderstanding and generative capabilities of Large Language Models (LLMs), we\npropose a novel method that reframes financial event entity extraction as a\ntext-to-structured-output generation task. Our approach involves fine-tuning a\npre-trained LLM using Parameter-Efficient Fine-Tuning (PEFT) to directly\ngenerate a structured representation, such as a JSON object, containing the\nextracted entities and their precise character spans from the input text. We\nevaluate our method on the challenging CCKS 2019 Financial Event Entity\nExtraction dataset, comparing its performance against strong sequence labeling\nbaselines, including SEBERTNets and sebertNets. Experimental results\ndemonstrate that our generative LLM method achieves a new state-of-the-art F1\nscore on this benchmark, significantly outperforming previous methods. Through\ndetailed quantitative analysis across event types, entity types, and instance\ncomplexity, as well as human evaluation, we show that our approach is more\neffective at handling the nuances of financial text and extracting high-quality\nentities. This work validates the potential of applying generative LLMs\ndirectly to complex, domain-specific information extraction tasks requiring\nstructured output.\n","authors":["Soo-joon Choi","Ji-jun Park"],"pdf_url":"https://arxiv.org/pdf/2504.14633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.14528v3","updated":"2025-04-20T14:22:22Z","published":"2025-01-24T14:31:30Z","title":"Idiom Detection in Sorani Kurdish Texts","summary":"  Idiom detection using Natural Language Processing (NLP) is the computerized\nprocess of recognizing figurative expressions within a text that convey\nmeanings beyond the literal interpretation of the words. While idiom detection\nhas seen significant progress across various languages, the Kurdish language\nfaces a considerable research gap in this area despite the importance of idioms\nin tasks like machine translation and sentiment analysis. This study addresses\nidiom detection in Sorani Kurdish by approaching it as a text classification\ntask using deep learning techniques. To tackle this, we developed a dataset\ncontaining 10,580 sentences embedding 101 Sorani Kurdish idioms across diverse\ncontexts. Using this dataset, we developed and evaluated three deep learning\nmodels: KuBERT-based transformer sequence classification, a Recurrent\nConvolutional Neural Network (RCNN), and a BiLSTM model with an attention\nmechanism. The evaluations revealed that the transformer model, the fine-tuned\nBERT, consistently outperformed the others, achieving nearly 99% accuracy while\nthe RCNN achieved 96.5% and the BiLSTM 80%. These results highlight the\neffectiveness of Transformer-based architectures in low-resource languages like\nKurdish. This research provides a dataset, three optimized models, and insights\ninto idiom detection, laying a foundation for advancing Kurdish NLP.\n","authors":["Skala Kamaran Omer","Hossein Hassani"],"pdf_url":"https://arxiv.org/pdf/2501.14528v3.pdf","comment":"22 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2504.14630v1","updated":"2025-04-20T14:17:17Z","published":"2025-04-20T14:17:17Z","title":"Automatic Text Summarization (ATS) for Research Documents in Sorani\n  Kurdish","summary":"  Extracting concise information from scientific documents aids learners,\nresearchers, and practitioners. Automatic Text Summarization (ATS), a key\nNatural Language Processing (NLP) application, automates this process. While\nATS methods exist for many languages, Kurdish remains underdeveloped due to\nlimited resources. This study develops a dataset and language model based on\n231 scientific papers in Sorani Kurdish, collected from four academic\ndepartments in two universities in the Kurdistan Region of Iraq (KRI),\naveraging 26 pages per document. Using Sentence Weighting and Term\nFrequency-Inverse Document Frequency (TF-IDF) algorithms, two experiments were\nconducted, differing in whether the conclusions were included. The average word\ncount was 5,492.3 in the first experiment and 5,266.96 in the second. Results\nwere evaluated manually and automatically using ROUGE-1, ROUGE-2, and ROUGE-L\nmetrics, with the best accuracy reaching 19.58%. Six experts conducted manual\nevaluations using three criteria, with results varying by document. This\nresearch provides valuable resources for Kurdish NLP researchers to advance ATS\nand related fields.\n","authors":["Rondik Hadi Abdulrahman","Hossein Hassani"],"pdf_url":"https://arxiv.org/pdf/2504.14630v1.pdf","comment":"18 pages, 11 figures, 8 tables"},{"id":"http://arxiv.org/abs/2504.14620v1","updated":"2025-04-20T13:58:20Z","published":"2025-04-20T13:58:20Z","title":"A Hierarchical Framework for Measuring Scientific Paper Innovation via\n  Large Language Models","summary":"  Measuring scientific paper innovation is both important and challenging.\nExisting content-based methods often overlook the full-paper context, fail to\ncapture the full scope of innovation, and lack generalization. We propose\nHSPIM, a hierarchical and training-free framework based on large language\nmodels (LLMs). It introduces a Paper-to-Sections-to-QAs decomposition to assess\ninnovation. We segment the text by section titles and use zero-shot LLM\nprompting to implement section classification, question-answering (QA)\naugmentation, and weighted novelty scoring. The generated QA pair focuses on\nsection-level innovation and serves as additional context to improve the LLM\nscoring. For each chunk, the LLM outputs a novelty score and a confidence\nscore. We use confidence scores as weights to aggregate novelty scores into a\npaper-level innovation score. To further improve performance, we propose a\ntwo-layer question structure consisting of common and section-specific\nquestions, and apply a genetic algorithm to optimize the question-prompt\ncombinations. Comprehensive experiments on scientific conference paper datasets\nshow that HSPIM outperforms baseline methods in effectiveness, generalization,\nand interpretability.\n","authors":["Hongming Tan","Shaoxiong Zhan","Fengwei Jia","Hai-Tao Zheng","Wai Kin Chan"],"pdf_url":"https://arxiv.org/pdf/2504.14620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14619v1","updated":"2025-04-20T13:54:28Z","published":"2025-04-20T13:54:28Z","title":"Translation Analytics for Freelancers: I. Introduction, Data\n  Preparation, Baseline Evaluations","summary":"  This is the first in a series of papers exploring the rapidly expanding new\nopportunities arising from recent progress in language technologies for\nindividual translators and language service providers with modest resources.\nThe advent of advanced neural machine translation systems, large language\nmodels, and their integration into workflows via computer-assisted translation\ntools and translation management systems have reshaped the translation\nlandscape. These advancements enable not only translation but also quality\nevaluation, error spotting, glossary generation, and adaptation to\ndomain-specific needs, creating new technical opportunities for freelancers. In\nthis series, we aim to empower translators with actionable methods to harness\nthese advancements. Our approach emphasizes Translation Analytics, a suite of\nevaluation techniques traditionally reserved for large-scale industry\napplications but now becoming increasingly available for smaller-scale users.\nThis first paper introduces a practical framework for adapting automatic\nevaluation metrics -- such as BLEU, chrF, TER, and COMET -- to freelancers'\nneeds. We illustrate the potential of these metrics using a trilingual corpus\nderived from a real-world project in the medical domain and provide statistical\nanalysis correlating human evaluations with automatic scores. Our findings\nemphasize the importance of proactive engagement with emerging technologies to\nnot only adapt but thrive in the evolving professional environment.\n","authors":["Yuri Balashov","Alex Balashov","Shiho Fukuda Koski"],"pdf_url":"https://arxiv.org/pdf/2504.14619v1.pdf","comment":"28 pages, 4 figures. Accepted at the MT Summit, University of Geneva,\n  June 2025"},{"id":"http://arxiv.org/abs/2409.05286v3","updated":"2025-04-20T13:28:25Z","published":"2024-09-09T02:41:00Z","title":"Seek and Solve Reasoning for Table Question Answering","summary":"  The complexities of table structures and question logic make table-based\nquestion answering (TQA) tasks challenging for Large Language Models (LLMs),\noften requiring task simplification before solving. This paper reveals that the\nreasoning process during task simplification may be more valuable than the\nsimplified tasks themselves and aims to improve TQA performance by leveraging\nLLMs' reasoning capabilities. We propose a Seek-and-Solve pipeline that\ninstructs the LLM to first seek relevant information and then answer questions,\nintegrating these two stages at the reasoning level into a coherent\nSeek-and-Solve Chain of Thought (SS-CoT). Additionally, we distill a\nsingle-step TQA-solving prompt from this pipeline, using demonstrations with\nSS-CoT paths to guide the LLM in solving complex TQA tasks under In-Context\nLearning settings. Our experiments show that our approaches result in improved\nperformance and reliability while being efficient. Our findings emphasize the\nimportance of eliciting LLMs' reasoning capabilities to handle complex TQA\ntasks effectively.\n","authors":["Ruya Jiang","Chun Wang","Weihong Deng"],"pdf_url":"https://arxiv.org/pdf/2409.05286v3.pdf","comment":"Published in: ICASSP 2025 - 2025 IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP)"},{"id":"http://arxiv.org/abs/2504.04994v2","updated":"2025-04-20T13:04:42Z","published":"2025-04-07T12:23:59Z","title":"Following the Whispers of Values: Unraveling Neural Mechanisms Behind\n  Value-Oriented Behaviors in LLMs","summary":"  Despite the impressive performance of large language models (LLMs), they can\npresent unintended biases and harmful behaviors driven by encoded values,\nemphasizing the urgent need to understand the value mechanisms behind them.\nHowever, current research primarily evaluates these values through external\nresponses with a focus on AI safety, lacking interpretability and failing to\nassess social values in real-world contexts. In this paper, we propose a novel\nframework called ValueExploration, which aims to explore the behavior-driven\nmechanisms of National Social Values within LLMs at the neuron level. As a case\nstudy, we focus on Chinese Social Values and first construct C-voice, a\nlarge-scale bilingual benchmark for identifying and evaluating Chinese Social\nValues in LLMs. By leveraging C-voice, we then identify and locate the neurons\nresponsible for encoding these values according to activation difference.\nFinally, by deactivating these neurons, we analyze shifts in model behavior,\nuncovering the internal mechanism by which values influence LLM\ndecision-making. Extensive experiments on four representative LLMs validate the\nefficacy of our framework. The benchmark and code will be available.\n","authors":["Ling Hu","Yuemei Xu","Xiaoyang Gu","Letao Han"],"pdf_url":"https://arxiv.org/pdf/2504.04994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14597v1","updated":"2025-04-20T12:55:59Z","published":"2025-04-20T12:55:59Z","title":"a1: Steep Test-time Scaling Law via Environment Augmented Generation","summary":"  Large Language Models (LLMs) have made remarkable breakthroughs in reasoning,\nyet continue to struggle with hallucinations, logical errors, and inability to\nself-correct during complex multi-step tasks. Current approaches like\nchain-of-thought prompting offer limited reasoning capabilities that fail when\nprecise step validation is required. We propose Environment Augmented\nGeneration (EAG), a framework that enhances LLM reasoning through: (1)\nreal-time environmental feedback validating each reasoning step, (2) dynamic\nbranch exploration for investigating alternative solution paths when faced with\nerrors, and (3) experience-based learning from successful reasoning\ntrajectories. Unlike existing methods, EAG enables deliberate backtracking and\nstrategic replanning through tight integration of execution feedback with\nbranching exploration. Our a1-32B model achieves state-of-the-art performance\namong similar-sized models across all benchmarks, matching larger models like\no1 on competition mathematics while outperforming comparable models by up to\n24.4 percentage points. Analysis reveals EAG's distinctive scaling pattern:\ninitial token investment in environment interaction yields substantial\nlong-term performance dividends, with advantages amplifying proportionally to\ntask complexity. EAG's theoretical framework demonstrates how environment\ninteractivity and systematic branch exploration together establish a new\nparadigm for reliable machine reasoning, particularly for problems requiring\nprecise multi-step calculation and logical verification.\n","authors":["Lingrui Mei","Shenghua Liu","Yiwei Wang","Baolong Bi","Yuyao Ge","Jun Wan","Yurong Wu","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.14597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14594v1","updated":"2025-04-20T12:51:16Z","published":"2025-04-20T12:51:16Z","title":"HealthGenie: Empowering Users with Healthy Dietary Guidance through\n  Knowledge Graph and Large Language Models","summary":"  Seeking dietary guidance often requires navigating complex professional\nknowledge while accommodating individual health conditions. Knowledge Graphs\n(KGs) offer structured and interpretable nutritional information, whereas Large\nLanguage Models (LLMs) naturally facilitate conversational recommendation\ndelivery. In this paper, we present HealthGenie, an interactive system that\ncombines the strengths of LLMs and KGs to provide personalized dietary\nrecommendations along with hierarchical information visualization for a quick\nand intuitive overview. Upon receiving a user query, HealthGenie performs query\nrefinement and retrieves relevant information from a pre-built KG. The system\nthen visualizes and highlights pertinent information, organized by defined\ncategories, while offering detailed, explainable recommendation rationales.\nUsers can further tailor these recommendations by adjusting preferences\ninteractively. Our evaluation, comprising a within-subject comparative\nexperiment and an open-ended discussion, demonstrates that HealthGenie\neffectively supports users in obtaining personalized dietary guidance based on\ntheir health conditions while reducing interaction effort and cognitive load.\nThese findings highlight the potential of LLM-KG integration in supporting\ndecision-making through explainable and visualized information. We examine the\nsystem's usefulness and effectiveness with an N=12 within-subject study and\nprovide design considerations for future systems that integrate conversational\nLLM and KG.\n","authors":["Fan Gao","Xinjie Zhao","Ding Xia","Zhongyi Zhou","Rui Yang","Jinghui Lu","Hang Jiang","Chanjun Park","Irene Li"],"pdf_url":"https://arxiv.org/pdf/2504.14594v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07346v2","updated":"2025-04-20T12:19:46Z","published":"2025-02-11T08:17:19Z","title":"BenchMAX: A Comprehensive Multilingual Evaluation Suite for Large\n  Language Models","summary":"  Previous multilingual benchmarks focus primarily on simple understanding\ntasks, but for large language models(LLMs), we emphasize proficiency in\ninstruction following, reasoning, long context understanding, code generation,\nand so on. However, measuring these advanced capabilities across languages is\nunderexplored. To address the disparity, we introduce BenchMAX, a multi-way\nmultilingual evaluation benchmark that allows for fair comparisons of these\nimportant abilities across languages. To maintain high quality, three distinct\nnative-speaking annotators independently annotate each sample within all tasks\nafter the data was machine-translated from English into 16 other languages.\nAdditionally, we present a novel translation challenge stemming from dataset\nconstruction. Extensive experiments on BenchMAX reveal varying effectiveness of\ncore capabilities across languages, highlighting performance gaps that cannot\nbe bridged by simply scaling up model size. BenchMAX serves as a comprehensive\nmultilingual evaluation platform, providing a promising test bed to promote the\ndevelopment of multilingual language models. The dataset and code are publicly\naccessible.\n","authors":["Xu Huang","Wenhao Zhu","Hanxu Hu","Conghui He","Lei Li","Shujian Huang","Fei Yuan"],"pdf_url":"https://arxiv.org/pdf/2502.07346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20941v4","updated":"2025-04-20T12:03:36Z","published":"2024-10-28T11:49:58Z","title":"Fine-Grained and Multi-Dimensional Metrics for Document-Level Machine\n  Translation","summary":"  Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and the outputs from GPT4-as-a-judge are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT\n","authors":["Yirong Sun","Dawei Zhu","Yanjun Chen","Erjia Xiao","Xinghao Chen","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.20941v4.pdf","comment":"Accepted at NAACL 2025 Student Research Workshop"},{"id":"http://arxiv.org/abs/2504.06868v2","updated":"2025-04-20T11:19:21Z","published":"2025-04-09T13:17:00Z","title":"Persona Dynamics: Unveiling the Impact of Personality Traits on Agents\n  in Text-Based Games","summary":"  Artificial agents are increasingly central to complex interactions and\ndecision-making tasks, yet aligning their behaviors with desired human values\nremains an open challenge. In this work, we investigate how human-like\npersonality traits influence agent behavior and performance within text-based\ninteractive environments. We introduce PANDA: Personality Adapted Neural\nDecision Agents, a novel method for projecting human personality traits onto\nagents to guide their behavior. To induce personality in a text-based game\nagent, (i) we train a personality classifier to identify what personality type\nthe agent's actions exhibit, and (ii) we integrate the personality profiles\ndirectly into the agent's policy-learning pipeline. By deploying agents\nembodying 16 distinct personality types across 25 text-based games and\nanalyzing their trajectories, we demonstrate that an agent's action decisions\ncan be guided toward specific personality profiles. Moreover, certain\npersonality types, such as those characterized by higher levels of Openness,\ndisplay marked advantages in performance. These findings underscore the promise\nof personality-adapted agents for fostering more aligned, effective, and\nhuman-centric decision-making in interactive environments.\n","authors":["Seungwon Lim","Seungbeen Lee","Dongjun Min","Youngjae Yu"],"pdf_url":"https://arxiv.org/pdf/2504.06868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14538v1","updated":"2025-04-20T08:56:27Z","published":"2025-04-20T08:56:27Z","title":"BookWorld: From Novels to Interactive Agent Societies for Creative Story\n  Generation","summary":"  Recent advances in large language models (LLMs) have enabled social\nsimulation through multi-agent systems. Prior efforts focus on agent societies\ncreated from scratch, assigning agents with newly defined personas. However,\nsimulating established fictional worlds and characters remain largely\nunderexplored, despite its significant practical value. In this paper, we\nintroduce BookWorld, a comprehensive system for constructing and simulating\nbook-based multi-agent societies. BookWorld's design covers comprehensive\nreal-world intricacies, including diverse and dynamic characters, fictional\nworldviews, geographical constraints and changes, e.t.c. BookWorld enables\ndiverse applications including story generation, interactive games and social\nsimulation, offering novel ways to extend and explore beloved fictional works.\nThrough extensive experiments, we demonstrate that BookWorld generates\ncreative, high-quality stories while maintaining fidelity to the source books,\nsurpassing previous methods with a win rate of 75.36%. The code of this paper\ncan be found at the project page: https://bookworld2025.github.io/.\n","authors":["Yiting Ran","Xintao Wang","Tian Qiu","Jiaqing Liang","Yanghua Xiao","Deqing Yang"],"pdf_url":"https://arxiv.org/pdf/2504.14538v1.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.14530v1","updated":"2025-04-20T08:11:11Z","published":"2025-04-20T08:11:11Z","title":"Causality for Natural Language Processing","summary":"  Causal reasoning is a cornerstone of human intelligence and a critical\ncapability for artificial systems aiming to achieve advanced understanding and\ndecision-making. This thesis delves into various dimensions of causal reasoning\nand understanding in large language models (LLMs). It encompasses a series of\nstudies that explore the causal inference skills of LLMs, the mechanisms behind\ntheir performance, and the implications of causal and anticausal learning for\nnatural language processing (NLP) tasks. Additionally, it investigates the\napplication of causal reasoning in text-based computational social science,\nspecifically focusing on political decision-making and the evaluation of\nscientific impact through citations. Through novel datasets, benchmark tasks,\nand methodological frameworks, this work identifies key challenges and\nopportunities to improve the causal capabilities of LLMs, providing a\ncomprehensive foundation for future research in this evolving field.\n","authors":["Zhijing Jin"],"pdf_url":"https://arxiv.org/pdf/2504.14530v1.pdf","comment":"PhD Thesis 2024"},{"id":"http://arxiv.org/abs/2504.14526v1","updated":"2025-04-20T07:50:44Z","published":"2025-04-20T07:50:44Z","title":"Are Vision LLMs Road-Ready? A Comprehensive Benchmark for\n  Safety-Critical Driving Video Understanding","summary":"  Vision Large Language Models (VLLMs) have demonstrated impressive\ncapabilities in general visual tasks such as image captioning and visual\nquestion answering. However, their effectiveness in specialized,\nsafety-critical domains like autonomous driving remains largely unexplored.\nAutonomous driving systems require sophisticated scene understanding in complex\nenvironments, yet existing multimodal benchmarks primarily focus on normal\ndriving conditions, failing to adequately assess VLLMs' performance in\nsafety-critical scenarios. To address this, we introduce DVBench, a pioneering\nbenchmark designed to evaluate the performance of VLLMs in understanding\nsafety-critical driving videos. Built around a hierarchical ability taxonomy\nthat aligns with widely adopted frameworks for describing driving scenarios\nused in assessing highly automated driving systems, DVBench features 10,000\nmultiple-choice questions with human-annotated ground-truth answers, enabling a\ncomprehensive evaluation of VLLMs' capabilities in perception and reasoning.\nExperiments on 14 SOTA VLLMs, ranging from 0.5B to 72B parameters, reveal\nsignificant performance gaps, with no model achieving over 40% accuracy,\nhighlighting critical limitations in understanding complex driving scenarios.\nTo probe adaptability, we fine-tuned selected models using domain-specific data\nfrom DVBench, achieving accuracy gains ranging from 5.24 to 10.94 percentage\npoints, with relative improvements of up to 43.59%. This improvement\nunderscores the necessity of targeted adaptation to bridge the gap between\ngeneral-purpose VLLMs and mission-critical driving applications. DVBench\nestablishes an essential evaluation framework and research roadmap for\ndeveloping VLLMs that meet the safety and robustness requirements for\nreal-world autonomous systems. We released the benchmark toolbox and the\nfine-tuned model at: https://github.com/tong-zeng/DVBench.git.\n","authors":["Tong Zeng","Longfeng Wu","Liang Shi","Dawei Zhou","Feng Guo"],"pdf_url":"https://arxiv.org/pdf/2504.14526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14520v1","updated":"2025-04-20T07:34:26Z","published":"2025-04-20T07:34:26Z","title":"Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey","summary":"  This survey explores the development of meta-thinking capabilities in Large\nLanguage Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL)\nperspective. Meta-thinking self-reflection, assessment, and control of thinking\nprocesses is an important next step in enhancing LLM reliability, flexibility,\nand performance, particularly for complex or high-stakes tasks. The survey\nbegins by analyzing current LLM limitations, such as hallucinations and the\nlack of internal self-assessment mechanisms. It then talks about newer methods,\nincluding RL from human feedback (RLHF), self-distillation, and\nchain-of-thought prompting, and each of their limitations. The crux of the\nsurvey is to talk about how multi-agent architectures, namely supervisor-agent\nhierarchies, agent debates, and theory of mind frameworks, can emulate\nhuman-like introspective behavior and enhance LLM robustness. By exploring\nreward mechanisms, self-play, and continuous learning methods in MARL, this\nsurvey gives a comprehensive roadmap to building introspective, adaptive, and\ntrustworthy LLMs. Evaluation metrics, datasets, and future research avenues,\nincluding neuroscience-inspired architectures and hybrid symbolic reasoning,\nare also discussed.\n","authors":["Ahsan Bilal","Muhammad Ahmed Mohsin","Muhammad Umer","Muhammad Awais Khan Bangash","Muhammad Ali Jamshed"],"pdf_url":"https://arxiv.org/pdf/2504.14520v1.pdf","comment":"Submitted to IEEE Transactions on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2410.04585v2","updated":"2025-04-20T05:49:38Z","published":"2024-10-06T18:46:28Z","title":"Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community\n  Retrieval","summary":"  Large language models (LLMs) have demonstrated significant potential in\nclinical decision support. Yet LLMs still suffer from hallucinations and lack\nfine-grained contextual medical knowledge, limiting their high-stake healthcare\napplications such as clinical diagnosis. Traditional retrieval-augmented\ngeneration (RAG) methods attempt to address these limitations but frequently\nretrieve sparse or irrelevant information, undermining prediction accuracy. We\nintroduce KARE, a novel framework that integrates knowledge graph (KG)\ncommunity-level retrieval with LLM reasoning to enhance healthcare predictions.\nKARE constructs a comprehensive multi-source KG by integrating biomedical\ndatabases, clinical literature, and LLM-generated insights, and organizes it\nusing hierarchical graph community detection and summarization for precise and\ncontextually relevant information retrieval. Our key innovations include: (1) a\ndense medical knowledge structuring approach enabling accurate retrieval of\nrelevant information; (2) a dynamic knowledge retrieval mechanism that enriches\npatient contexts with focused, multi-faceted medical insights; and (3) a\nreasoning-enhanced prediction framework that leverages these enriched contexts\nto produce both accurate and interpretable clinical predictions. Extensive\nexperiments demonstrate that KARE outperforms leading models by up to\n10.8-15.0% on MIMIC-III and 12.6-12.7% on MIMIC-IV for mortality and\nreadmission predictions. In addition to its impressive prediction accuracy, our\nframework leverages the reasoning capabilities of LLMs, enhancing the\ntrustworthiness of clinical predictions.\n","authors":["Pengcheng Jiang","Cao Xiao","Minhao Jiang","Parminder Bhatia","Taha Kass-Hout","Jimeng Sun","Jiawei Han"],"pdf_url":"https://arxiv.org/pdf/2410.04585v2.pdf","comment":"ICLR 2025 Camera-Ready"},{"id":"http://arxiv.org/abs/2411.05227v2","updated":"2025-04-20T05:40:15Z","published":"2024-11-07T22:37:30Z","title":"CHATTER: A Character Attribution Dataset for Narrative Understanding","summary":"  Computational narrative understanding studies the identification,\ndescription, and interaction of the elements of a narrative: characters,\nattributes, events, and relations. Narrative research has given considerable\nattention to defining and classifying character types. However, these\ncharacter-type taxonomies do not generalize well because they are small, too\nsimple, or specific to a domain. We require robust and reliable benchmarks to\ntest whether narrative models truly understand the nuances of the character's\ndevelopment in the story. Our work addresses this by curating the CHATTER\ndataset that labels whether a character portrays some attribute for 88124\ncharacter-attribute pairs, encompassing 2998 characters, 12967 attributes and\n660 movies. We validate a subset of CHATTER, called CHATTEREVAL, using human\nannotations to serve as a benchmark to evaluate the character attribution task\nin movie scripts. \\evaldataset{} also assesses narrative understanding and the\nlong-context modeling capacity of language models.\n","authors":["Sabyasachee Baruah","Shrikanth Narayanan"],"pdf_url":"https://arxiv.org/pdf/2411.05227v2.pdf","comment":"accepted to 7th Workshop on Narrative Understanding"},{"id":"http://arxiv.org/abs/2504.14496v1","updated":"2025-04-20T05:17:57Z","published":"2025-04-20T05:17:57Z","title":"Functional Abstraction of Knowledge Recall in Large Language Models","summary":"  Pre-trained transformer large language models (LLMs) demonstrate strong\nknowledge recall capabilities. This paper investigates the knowledge recall\nmechanism in LLMs by abstracting it into a functional structure. We propose\nthat during knowledge recall, the model's hidden activation space implicitly\nentails a function execution process where specific activation vectors align\nwith functional components (Input argument, Function body, and Return values).\nSpecifically, activation vectors of relation-related tokens define a mapping\nfunction from subjects to objects, with subject-related token activations\nserving as input arguments and object-related token activations as return\nvalues. For experimental verification, we first design a patching-based\nknowledge-scoring algorithm to identify knowledge-aware activation vectors as\nindependent functional components. Then, we conduct counter-knowledge testing\nto examine the independent functional effects of each component on knowledge\nrecall outcomes. From this functional perspective, we improve the contextual\nknowledge editing approach augmented by activation patching. By rewriting\nincoherent activations in context, we enable improved short-term memory\nretention for new knowledge prompting.\n","authors":["Zijian Wang","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2504.14496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14492v1","updated":"2025-04-20T04:57:00Z","published":"2025-04-20T04:57:00Z","title":"FairSteer: Inference Time Debiasing for LLMs with Dynamic Activation\n  Steering","summary":"  Large language models (LLMs) are prone to capturing biases from training\ncorpus, leading to potential negative social impacts. Existing prompt-based\ndebiasing methods exhibit instability due to their sensitivity to prompt\nchanges, while fine-tuning-based techniques incur substantial computational\noverhead and catastrophic forgetting. In this paper, we propose FairSteer, a\nnovel inference-time debiasing framework without requiring customized prompt\ndesign or model retraining. Motivated by the linear representation hypothesis,\nour preliminary investigation demonstrates that fairness-related features can\nbe encoded into separable directions in the hidden activation space. FairSteer\noperates in three steps: biased activation detection, debiasing steering vector\n(DSV) computation, and dynamic activation steering. Specifically, it first\ntrains a lightweight linear classifier to detect bias signatures in\nactivations, and then computes DSVs as intervention directions derived from\nsmall contrastive prompt pairs. Subsequently, it performs debiasing by\nadjusting activations with DSVs in the inference stage. Comprehensive\nevaluation with six LLMs demonstrates the superiority of FairSteer across\nquestion-answering, counterfactual input evaluation and open-ended text\ngeneration tasks. Code will be released.\n","authors":["Yichen Li","Zhiting Fan","Ruizhe Chen","Xiaotang Gai","Luqi Gong","Yan Zhang","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2504.14492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09107v2","updated":"2025-04-20T04:31:16Z","published":"2023-10-13T13:52:15Z","title":"GLoRE: Evaluating Logical Reasoning of Large Language Models","summary":"  Large language models (LLMs) have shown significant general language\nunderstanding abilities. However, there has been a scarcity of attempts to\nassess the logical reasoning capacities of these LLMs, an essential facet of\nnatural language understanding. To encourage further investigation in this\narea, we introduce GLoRE, a General Logical Reasoning Evaluation platform that\nnot only consolidates diverse datasets but also standardizes them into a\nunified format suitable for evaluating large language models across zero-shot\nand few-shot scenarios. Our experimental results show that compared to the\nperformance of humans and supervised fine-tuning models, the logical reasoning\ncapabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1\nand QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the\nhighest benchmark performance to date. GLoRE is designed as a living project\nthat continuously integrates new datasets and models, facilitating robust and\ncomparative assessments of model performance in both commercial and Huggingface\ncommunities.\n","authors":["Hanmeng liu","Zhiyang Teng","Ruoxi Ning","Yiran Ding","Xiulai Li","Xiaozhang Liu","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.09107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.00695v3","updated":"2025-04-20T04:18:23Z","published":"2025-04-01T12:06:42Z","title":"ToReMi: Topic-Aware Data Reweighting for Dynamic Pre-Training Data\n  Selection","summary":"  Pre-training large language models (LLMs) necessitates enormous diverse\ntextual corpora, making effective data selection a key challenge for balancing\ncomputational resources and model performance. Current methodologies primarily\nemphasize data quality metrics and mixing proportions, yet they fail to\nadequately capture the underlying semantic connections between training samples\nand quality disparities within individual domains. We introduce ToReMi\n(Topic-based Reweighting for Model improvement), a novel two-stage framework\nthat dynamically adjusts training sample weights according to their topical\nassociations and observed learning patterns. Our comprehensive experiments\nreveal that ToReMi variants consistently achieve superior performance over\nconventional pre-training approaches, demonstrating accelerated perplexity\nreduction across multiple domains and enhanced capabilities on downstream\nevaluation tasks. Code is available at https://github.com/zxx000728/ToReMi.\n","authors":["Xiaoxuan Zhu","Zhouhong Gu","Baiqian Wu","Suhang Zheng","Tao Wang","Tianyu Li","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2504.00695v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14482v1","updated":"2025-04-20T04:14:30Z","published":"2025-04-20T04:14:30Z","title":"DialogueAgents: A Hybrid Agent-Based Speech Synthesis Framework for\n  Multi-Party Dialogue","summary":"  Speech synthesis is crucial for human-computer interaction, enabling natural\nand intuitive communication. However, existing datasets involve high\nconstruction costs due to manual annotation and suffer from limited character\ndiversity, contextual scenarios, and emotional expressiveness. To address these\nissues, we propose DialogueAgents, a novel hybrid agent-based speech synthesis\nframework, which integrates three specialized agents -- a script writer, a\nspeech synthesizer, and a dialogue critic -- to collaboratively generate\ndialogues. Grounded in a diverse character pool, the framework iteratively\nrefines dialogue scripts and synthesizes speech based on speech review,\nboosting emotional expressiveness and paralinguistic features of the\nsynthesized dialogues. Using DialogueAgent, we contribute MultiTalk, a\nbilingual, multi-party, multi-turn speech dialogue dataset covering diverse\ntopics. Extensive experiments demonstrate the effectiveness of our framework\nand the high quality of the MultiTalk dataset. We release the dataset and code\nhttps://github.com/uirlx/DialogueAgents to facilitate future research on\nadvanced speech synthesis models and customized data generation.\n","authors":["Xiang Li","Duyi Pan","Hongru Xiao","Jiale Han","Jing Tang","Jiabao Ma","Wei Wang","Bo Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.14482v1.pdf","comment":"Accepted by ICME 2025. Dataset and code are publicly available:\n  [https://github.com/uirlx/DialogueAgents](https://github.com/uirlx/DialogueAgents)"},{"id":"http://arxiv.org/abs/2504.14468v1","updated":"2025-04-20T03:01:42Z","published":"2025-04-20T03:01:42Z","title":"sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning\n  Approach to Brain-Language Alignment","summary":"  Interpreting neural activity through meaningful latent representations\nremains a complex and evolving challenge at the intersection of neuroscience\nand artificial intelligence. We investigate the potential of multimodal\nfoundation models to align invasive brain recordings with natural language. We\npresent SSENSE, a contrastive learning framework that projects single-subject\nstereo-electroencephalography (sEEG) signals into the sentence embedding space\nof a frozen CLIP model, enabling sentence-level retrieval directly from brain\nactivity. SSENSE trains a neural encoder on spectral representations of sEEG\nusing InfoNCE loss, without fine-tuning the text encoder. We evaluate our\nmethod on time-aligned sEEG and spoken transcripts from a naturalistic\nmovie-watching dataset. Despite limited data, SSENSE achieves promising\nresults, demonstrating that general-purpose language representations can serve\nas effective priors for neural decoding.\n","authors":["Yijun Liu"],"pdf_url":"https://arxiv.org/pdf/2504.14468v1.pdf","comment":"Accepted for poster presentation at the CVPR 2025 Workshop on\n  Multimodal Foundation Models (MMFM3)"},{"id":"http://arxiv.org/abs/2504.14462v1","updated":"2025-04-20T02:47:18Z","published":"2025-04-20T02:47:18Z","title":"CoLoTa: A Dataset for Entity-based Commonsense Reasoning over Long-Tail\n  Knowledge","summary":"  The rise of Large Language Models (LLMs) has redefined the AI landscape,\nparticularly due to their ability to encode factual and commonsense knowledge,\nand their outstanding performance in tasks requiring reasoning. Despite these\nadvances, hallucinations and reasoning errors remain a significant barrier to\ntheir deployment in high-stakes settings. In this work, we observe that even\nthe most prominent LLMs, such as OpenAI-o1, suffer from high rates of reasoning\nerrors and hallucinations on tasks requiring commonsense reasoning over\nobscure, long-tail entities. To investigate this limitation, we present a new\ndataset for Commonsense reasoning over Long-Tail entities (CoLoTa), that\nconsists of 3,300 queries from question answering and claim verification tasks\nand covers a diverse range of commonsense reasoning skills. We remark that\nCoLoTa can also serve as a Knowledge Graph Question Answering (KGQA) dataset\nsince the support of knowledge required to answer its queries is present in the\nWikidata knowledge graph. However, as opposed to existing KGQA benchmarks that\nmerely focus on factoid questions, our CoLoTa queries also require commonsense\nreasoning. Our experiments with strong LLM-based KGQA methodologies indicate\ntheir severe inability to answer queries involving commonsense reasoning.\nHence, we propose CoLoTa as a novel benchmark for assessing both (i) LLM\ncommonsense reasoning capabilities and their robustness to hallucinations on\nlong-tail entities and (ii) the commonsense reasoning capabilities of KGQA\nmethods.\n","authors":["Armin Toroghi","Willis Guo","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2504.14462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.07532v2","updated":"2025-04-20T02:42:05Z","published":"2025-04-10T07:58:05Z","title":"AI-Slop to AI-Polish? Aligning Language Models through Edit-Based\n  Writing Rewards and Test-time Computation","summary":"  AI-generated text is proliferating across domains, from creative writing and\njournalism to marketing content and scientific articles. Models can follow\nuser-provided instructions to generate coherent and grammatically correct\noutputs but in this work, we study a more fundamental question: how do we\nevaluate and improve the writing quality of AI-generated text? Writing quality\nassessment has received less attention from the community, in part because it\nis fundamentally subjective and requires expertise. We first introduce the\nWriting Quality Benchmark (WQ) by consolidating five writing-preference\ndatasets into 4,729 writing quality judgments. Our experiments show that most\nof the competitive baselines, including state-of-the-art LLMs that excel at\nreasoning tasks, barely outperform random baselines on WQ. We then train\nspecialized Writing Quality Reward Models (WQRM) of various sizes for writing\nquality assessment that demonstrate strong generalization on four\nout-of-distribution test sets and 74% accuracy on the WQ benchmark. To further\nshow WQRM's practical benefits during inference, we leverage additional\ntest-time compute to generate and rank multiple candidate revisions, allowing\nus to select higher-quality outputs from an initial draft. Human evaluation\nwith 9 experienced writers confirm that WQRM-based selection produces writing\nsamples preferred by experts 66% overall, and 72.2% when the reward gap is\nlarger than 1 point. We release our datasets and models to encourage community\nengagement with writing quality assessment and development of AI writing\nsystems better aligned with human preferences.\n","authors":["Tuhin Chakrabarty","Philippe Laban","Chien-Sheng Wu"],"pdf_url":"https://arxiv.org/pdf/2504.07532v2.pdf","comment":"Under Submission"},{"id":"http://arxiv.org/abs/2408.06631v4","updated":"2025-04-20T02:22:15Z","published":"2024-08-13T04:36:18Z","title":"IFShip: Interpretable Fine-grained Ship Classification with Domain\n  Knowledge-Enhanced Vision-Language Models","summary":"  End-to-end interpretation currently dominates the remote sensing fine-grained\nship classification (RS-FGSC) task. However, the inference process remains\nuninterpretable, leading to criticisms of these models as \"black box\" systems.\nTo address this issue, we propose a domain knowledge-enhanced Chain-of-Thought\n(CoT) prompt generation mechanism, which is used to semi-automatically\nconstruct a task-specific instruction-following dataset, TITANIC-FGS. By\ntraining on TITANIC-FGS, we adapt general-domain vision-language models (VLMs)\nto the FGSC task, resulting in a model named IFShip. Building upon IFShip, we\ndevelop an FGSC visual chatbot that redefines the FGSC problem as a\nstep-by-step reasoning task and conveys the reasoning process in natural\nlanguage. Experimental results show that IFShip outperforms state-of-the-art\nFGSC algorithms in both interpretability and classification accuracy.\nFurthermore, compared to VLMs such as LLaVA and MiniGPT-4, IFShip demonstrates\nsuperior performance on the FGSC task. It provides an accurate chain of\nreasoning when fine-grained ship types are recognizable to the human eye and\noffers interpretable explanations when they are not. Our dataset is publicly\navailable at: https://github.com/lostwolves/IFShip.\n","authors":["Mingning Guo","Mengwei Wu","Yuxiang Shen","Haifeng Li","Chao Tao"],"pdf_url":"https://arxiv.org/pdf/2408.06631v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05714v2","updated":"2025-04-20T02:18:50Z","published":"2025-01-10T05:15:14Z","title":"How to Enable Effective Cooperation Between Humans and NLP Models: A\n  Survey of Principles, Formalizations, and Beyond","summary":"  With the advancement of large language models (LLMs), intelligent models have\nevolved from mere tools to autonomous agents with their own goals and\nstrategies for cooperating with humans. This evolution has birthed a novel\nparadigm in NLP, i.e., human-model cooperation, that has yielded remarkable\nprogress in numerous NLP tasks in recent years. In this paper, we take the\nfirst step to present a thorough review of human-model cooperation, exploring\nits principles, formalizations, and open challenges. In particular, we\nintroduce a new taxonomy that provides a unified perspective to summarize\nexisting approaches. Also, we discuss potential frontier areas and their\ncorresponding challenges. We regard our work as an entry point, paving the way\nfor more breakthrough research in this regard.\n","authors":["Chen Huang","Yang Deng","Wenqiang Lei","Jiancheng Lv","Tat-Seng Chua","Jimmy Xiangji Huang"],"pdf_url":"https://arxiv.org/pdf/2501.05714v2.pdf","comment":"V2: Only minor edits were made to the main text, and we've added more\n  supplementary materials"},{"id":"http://arxiv.org/abs/2504.14452v1","updated":"2025-04-20T01:59:46Z","published":"2025-04-20T01:59:46Z","title":"ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of\n  Pre-training Data","summary":"  Language models (LMs) can memorize and reproduce segments from their\npretraining data verbatim even in non-adversarial settings, raising concerns\nabout copyright, plagiarism, privacy, and creativity. We introduce Paraphrase\nPreference Optimization (ParaPO), a post-training method that fine-tunes LMs to\nreduce unintentional regurgitation while preserving their overall utility.\nParaPO trains LMs to prefer paraphrased versions of memorized segments over the\noriginal verbatim content from the pretraining data. To maintain the ability to\nrecall famous quotations when appropriate, we develop a variant of ParaPO that\nuses system prompts to control regurgitation behavior. In our evaluation on\nLlama3.1-8B, ParaPO consistently reduces regurgitation across all tested\ndatasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative\nwriting), whereas unlearning methods used in prior work to mitigate\nregurgitation are less effective outside their targeted unlearned domain (from\n17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO\nwith system prompting successfully preserves famous quotation recall while\nreducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when\nprompted not to regurgitate. In contrast, without ParaPO tuning, prompting the\nmodel not to regurgitate produces only a marginal reduction (8.7 to 8.4).\n","authors":["Tong Chen","Faeze Brahman","Jiacheng Liu","Niloofar Mireshghallah","Weijia Shi","Pang Wei Koh","Luke Zettlemoyer","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2504.14452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14439v1","updated":"2025-04-20T01:16:24Z","published":"2025-04-20T01:16:24Z","title":"LoRe: Personalizing LLMs via Low-Rank Reward Modeling","summary":"  Personalizing large language models (LLMs) to accommodate diverse user\npreferences is essential for enhancing alignment and user satisfaction.\nTraditional reinforcement learning from human feedback (RLHF) approaches often\nrely on monolithic value representations, limiting their ability to adapt to\nindividual preferences. We introduce a novel framework that leverages low-rank\npreference modeling to efficiently learn and generalize user-specific reward\nfunctions. By representing reward functions in a low-dimensional subspace and\nmodeling individual preferences as weighted combinations of shared basis\nfunctions, our approach avoids rigid user categorization while enabling\nscalability and few-shot adaptation. We validate our method on multiple\npreference datasets, demonstrating superior generalization to unseen users and\nimproved accuracy in preference prediction tasks.\n","authors":["Avinandan Bose","Zhihan Xiong","Yuejie Chi","Simon Shaolei Du","Lin Xiao","Maryam Fazel"],"pdf_url":"https://arxiv.org/pdf/2504.14439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21088v2","updated":"2025-04-20T00:37:24Z","published":"2025-03-27T02:03:25Z","title":"ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging","summary":"  This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.\n","authors":["Haoming Xu","Shuxun Wang","Yanqiu Zhao","Yi Zhong","Ziyan Jiang","Ningyuan Zhao","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.21088v2.pdf","comment":"SemEval@ACL 2025"},{"id":"http://arxiv.org/abs/2410.09300v3","updated":"2025-04-20T00:16:18Z","published":"2024-10-11T23:24:38Z","title":"Nudging: Inference-time Alignment of LLMs via Guided Decoding","summary":"  Large language models (LLMs) require alignment to effectively and safely\nfollow user instructions. This process necessitates training an aligned version\nfor every base model, resulting in significant computational overhead. In this\nwork, we propose nudging, a simple, plug-and-play, and training-free algorithm\nthat aligns any base model at inference time using a small aligned model.\nNudging is motivated by recent findings that alignment primarily alters the\nmodel's behavior on a small subset of stylistic tokens (e.g., discourse\nmarkers). We find that base models are significantly more uncertain when\ngenerating these tokens. Building on this insight, nudging employs a small\naligned model to generate nudging tokens to guide the base model's output\nduring decoding when the base model's uncertainty is high. We evaluate nudging\nacross 3 model families on a diverse range of open-instruction tasks. Without\nany training, nudging a large base model with a 7x-14x smaller aligned model\nachieves zero-shot performance comparable to, and sometimes surpassing, that of\nlarge aligned models. By operating at the token level, nudging enables\noff-the-shelf collaboration between model families. For instance, nudging\nGemma-2-27b with Llama-2-7b-chat outperforms Llama-2-70b-chat on various tasks.\nOverall, our work offers a modular and cost-efficient solution to LLM\nalignment. Our project website: https://fywalter.github.io/nudging/ .\n","authors":["Yu Fei","Yasaman Razeghi","Sameer Singh"],"pdf_url":"https://arxiv.org/pdf/2410.09300v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2504.14753v1","updated":"2025-04-20T22:27:24Z","published":"2025-04-20T22:27:24Z","title":"Advancing Video Anomaly Detection: A Bi-Directional Hybrid Framework for\n  Enhanced Single- and Multi-Task Approaches","summary":"  Despite the prevailing transition from single-task to multi-task approaches\nin video anomaly detection, we observe that many adopt sub-optimal frameworks\nfor individual proxy tasks. Motivated by this, we contend that optimizing\nsingle-task frameworks can advance both single- and multi-task approaches.\nAccordingly, we leverage middle-frame prediction as the primary proxy task, and\nintroduce an effective hybrid framework designed to generate accurate\npredictions for normal frames and flawed predictions for abnormal frames. This\nhybrid framework is built upon a bi-directional structure that seamlessly\nintegrates both vision transformers and ConvLSTMs. Specifically, we utilize\nthis bi-directional structure to fully analyze the temporal dimension by\npredicting frames in both forward and backward directions, significantly\nboosting the detection stability. Given the transformer's capacity to model\nlong-range contextual dependencies, we develop a convolutional temporal\ntransformer that efficiently associates feature maps from all context frames to\ngenerate attention-based predictions for target frames. Furthermore, we devise\na layer-interactive ConvLSTM bridge that facilitates the smooth flow of\nlow-level features across layers and time-steps, thereby strengthening\npredictions with fine details. Anomalies are eventually identified by\nscrutinizing the discrepancies between target frames and their corresponding\npredictions. Several experiments conducted on public benchmarks affirm the\nefficacy of our hybrid framework, whether used as a standalone single-task\napproach or integrated as a branch in a multi-task approach. These experiments\nalso underscore the advantages of merging vision transformers and ConvLSTMs for\nvideo anomaly detection.\n","authors":["Guodong Shen","Yuqi Ouyang","Junru Lu","Yixuan Yang","Victor Sanchez"],"pdf_url":"https://arxiv.org/pdf/2504.14753v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing (TIP)"},{"id":"http://arxiv.org/abs/2501.15572v3","updated":"2025-04-20T21:31:01Z","published":"2025-01-26T15:57:44Z","title":"Comparative clinical evaluation of \"memory-efficient\" synthetic 3d\n  generative adversarial networks (gan) head-to-head to state of art: results\n  on computed tomography of the chest","summary":"  Generative Adversarial Networks (GANs) are increasingly used to generate\nsynthetic medical images, addressing the critical shortage of annotated data\nfor training Artificial Intelligence systems. This study introduces CRF-GAN, a\nnovel memory-efficient GAN architecture that enhances structural consistency in\n3D medical image synthesis. Integrating Conditional Random Fields within a\ntwo-step generation process allows CRF-GAN improving spatial coherence while\nmaintaining high-resolution image quality. The model's performance is evaluated\nagainst the state-of-the-art hierarchical (HA)-GAN model. Materials and\nMethods: We evaluate the performance of CRF-GAN against the HA-GAN model. The\ncomparison between the two models was made through a quantitative evaluation,\nusing FID and MMD metrics, and a qualitative evaluation, through a\ntwo-alternative forced choice (2AFC) test completed by a pool of 12 resident\nradiologists, to assess the realism of the generated images. Results: CRF-GAN\noutperformed HA-GAN with lower FID and MMD scores, indicating better image\nfidelity. The 2AFC test showed a significant preference for images generated by\nCRF-Gan over those generated by HA-GAN. Additionally, CRF-GAN demonstrated\n9.34% lower memory usage and achieved up to 14.6% faster training speeds,\noffering substantial computational savings. Discussion: CRF-GAN model\nsuccessfully generates high-resolution 3D medical images with non-inferior\nquality to conventional models, while being more memory-efficient and faster.\nThe key objective was not only to lower the computational cost but also to\nreallocate the freed-up resources towards the creation of higher-resolution 3D\nimaging, which is still a critical factor limiting their direct clinical\napplicability. Moreover, unlike many previous studies, we combined qualitative\nand quantitative assessments to obtain a more holistic feedback on the model's\nperformance.\n","authors":["Mahshid Shiri","Chandra Bortolotto","Alessandro Bruno","Alessio Consonni","Daniela Maria Grasso","Leonardo Brizzi","Daniele Loiacono","Lorenzo Preda"],"pdf_url":"https://arxiv.org/pdf/2501.15572v3.pdf","comment":"Accpeted to Journal of Imaging Informatics in Medicine"},{"id":"http://arxiv.org/abs/2503.01019v3","updated":"2025-04-20T21:18:03Z","published":"2025-03-02T21:09:32Z","title":"MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data\n  with Vision Generation Task using Discrete Visual Representations","summary":"  Despite significant progress in Vision-Language Pre-training (VLP), current\napproaches predominantly emphasize feature extraction and cross-modal\ncomprehension, with limited attention to generating or transforming visual\ncontent. This gap hinders the model's ability to synthesize coherent and novel\nvisual representations from textual prompts, thereby reducing the effectiveness\nof multi-modal learning. In this work, we propose MedUnifier, a unified VLP\nframework tailored for medical data. MedUnifier seamlessly integrates\ntext-grounded image generation capabilities with multi-modal learning\nstrategies, including image-text contrastive alignment, image-text matching and\nimage-grounded text generation. Unlike traditional methods that reply on\ncontinuous visual representations, our approach employs visual vector\nquantization, which not only facilitates a more cohesive learning strategy for\ncross-modal understanding but also enhances multi-modal generation quality by\neffectively leveraging discrete representations. Our framework's effectiveness\nis evidenced by the experiments on established benchmarks, including uni-modal\ntasks (supervised fine-tuning), cross-modal tasks (image-text retrieval and\nzero-shot image classification), and multi-modal tasks (medical report\ngeneration, image synthesis), where it achieves state-of-the-art performance\nacross various tasks. MedUnifier also offers a highly adaptable tool for a wide\nrange of language and vision tasks in healthcare, marking advancement toward\nthe development of a generalizable AI model for medical applications.\n","authors":["Ziyang Zhang","Yang Yu","Yucheng Chen","Xulei Yang","Si Yong Yeo"],"pdf_url":"https://arxiv.org/pdf/2503.01019v3.pdf","comment":"To be pubilshed in CVPR 2025"},{"id":"http://arxiv.org/abs/2412.14462v2","updated":"2025-04-20T20:58:33Z","published":"2024-12-19T02:23:13Z","title":"Affordance-Aware Object Insertion via Mask-Aware Dual Diffusion","summary":"  As a common image editing operation, image composition involves integrating\nforeground objects into background scenes. In this paper, we expand the\napplication of the concept of Affordance from human-centered image composition\ntasks to a more general object-scene composition framework, addressing the\ncomplex interplay between foreground objects and background scenes. Following\nthe principle of Affordance, we define the affordance-aware object insertion\ntask, which aims to seamlessly insert any object into any scene with various\nposition prompts. To address the limited data issue and incorporate this task,\nwe constructed the SAM-FB dataset, which contains over 3 million examples\nacross more than 3,000 object categories. Furthermore, we propose the\nMask-Aware Dual Diffusion (MADD) model, which utilizes a dual-stream\narchitecture to simultaneously denoise the RGB image and the insertion mask. By\nexplicitly modeling the insertion mask in the diffusion process, MADD\neffectively facilitates the notion of affordance. Extensive experimental\nresults show that our method outperforms the state-of-the-art methods and\nexhibits strong generalization performance on in-the-wild images. Please refer\nto our code on https://github.com/KaKituken/affordance-aware-any.\n","authors":["Jixuan He","Wanhua Li","Ye Liu","Junsik Kim","Donglai Wei","Hanspeter Pfister"],"pdf_url":"https://arxiv.org/pdf/2412.14462v2.pdf","comment":"Code is available at:\n  https://github.com/KaKituken/affordance-aware-any. Project page at:\n  https://kakituken.github.io/affordance-any.github.io/"},{"id":"http://arxiv.org/abs/2504.14737v1","updated":"2025-04-20T20:57:03Z","published":"2025-04-20T20:57:03Z","title":"SuperCL: Superpixel Guided Contrastive Learning for Medical Image\n  Segmentation Pre-training","summary":"  Medical image segmentation is a critical yet challenging task, primarily due\nto the difficulty of obtaining extensive datasets of high-quality,\nexpert-annotated images. Contrastive learning presents a potential but still\nproblematic solution to this issue. Because most existing methods focus on\nextracting instance-level or pixel-to-pixel representation, which ignores the\ncharacteristics between intra-image similar pixel groups. Moreover, when\nconsidering contrastive pairs generation, most SOTA methods mainly rely on\nmanually setting thresholds, which requires a large number of gradient\nexperiments and lacks efficiency and generalization. To address these issues,\nwe propose a novel contrastive learning approach named SuperCL for medical\nimage segmentation pre-training. Specifically, our SuperCL exploits the\nstructural prior and pixel correlation of images by introducing two novel\ncontrastive pairs generation strategies: Intra-image Local Contrastive Pairs\n(ILCP) Generation and Inter-image Global Contrastive Pairs (IGCP) Generation.\nConsidering superpixel cluster aligns well with the concept of contrastive\npairs generation, we utilize the superpixel map to generate pseudo masks for\nboth ILCP and IGCP to guide supervised contrastive learning. Moreover, we also\npropose two modules named Average SuperPixel Feature Map Generation (ASP) and\nConnected Components Label Generation (CCL) to better exploit the prior\nstructural information for IGCP. Finally, experiments on 8 medical image\ndatasets indicate our SuperCL outperforms existing 12 methods. i.e. Our SuperCL\nachieves a superior performance with more precise predictions from\nvisualization figures and 3.15%, 5.44%, 7.89% DSC higher than the previous best\nresults on MMWHS, CHAOS, Spleen with 10% annotations. Our code will be released\nafter acceptance.\n","authors":["Shuang Zeng","Lei Zhu","Xinliang Zhang","Hangzhou He","Yanye Lu"],"pdf_url":"https://arxiv.org/pdf/2504.14737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14736v1","updated":"2025-04-20T20:56:25Z","published":"2025-04-20T20:56:25Z","title":"ChronoRoot 2.0: An Open AI-Powered Platform for 2D Temporal Plant\n  Phenotyping","summary":"  The analysis of plant developmental plasticity, including root system\narchitecture, is fundamental to understanding plant adaptability and\ndevelopment, particularly in the context of climate change and agricultural\nsustainability. While significant advances have been made in plant phenotyping\ntechnologies, comprehensive temporal analysis of root development remains\nchallenging, with most existing solutions providing either limited throughput\nor restricted structural analysis capabilities. Here, we present ChronoRoot\n2.0, an integrated open-source platform that combines affordable hardware with\nadvanced artificial intelligence to enable sophisticated temporal plant\nphenotyping. The system introduces several major advances, offering an integral\nperspective of seedling development: (i) simultaneous multi-organ tracking of\nsix distinct plant structures, (ii) quality control through real-time\nvalidation, (iii) comprehensive architectural measurements including novel\ngravitropic response parameters, and (iv) dual specialized user interfaces for\nboth architectural analysis and high-throughput screening. We demonstrate the\nsystem's capabilities through three use cases for Arabidopsis thaliana:\ncharacterization of circadian growth patterns under different light conditions,\ndetailed analysis of gravitropic responses in transgenic plants, and\nhigh-throughput screening of etiolation responses across multiple genotypes.\nChronoRoot 2.0 maintains its predecessor's advantages of low cost and\nmodularity while significantly expanding its capabilities, making sophisticated\ntemporal phenotyping more accessible to the broader plant science community.\nThe system's open-source nature, combined with extensive documentation and\ncontainerized deployment options, ensures reproducibility and enables\ncommunity-driven development of new analytical capabilities.\n","authors":["Nicolás Gaggion","Rodrigo Bonazzola","María Florencia Legascue","María Florencia Mammarella","Florencia Sol Rodriguez","Federico Emanuel Aballay","Florencia Belén Catulo","Andana Barrios","Franco Accavallo","Santiago Nahuel Villarreal","Martin Crespi","Martiniano María Ricardi","Ezequiel Petrillo","Thomas Blein","Federico Ariel","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2504.14736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14727v1","updated":"2025-04-20T19:53:13Z","published":"2025-04-20T19:53:13Z","title":"Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual\n  Learning","summary":"  Humans and most animals inherently possess a distinctive capacity to\ncontinually acquire novel experiences and accumulate worldly knowledge over\ntime. This ability, termed continual learning, is also critical for deep neural\nnetworks (DNNs) to adapt to the dynamically evolving world in open\nenvironments. However, DNNs notoriously suffer from catastrophic forgetting of\npreviously learned knowledge when trained on sequential tasks. In this work,\ninspired by the interactive human memory and learning system, we propose a\nnovel biomimetic continual learning framework that integrates semi-parametric\nmemory and the wake-sleep consolidation mechanism. For the first time, our\nmethod enables deep neural networks to retain high performance on novel tasks\nwhile maintaining prior knowledge in real-world challenging continual learning\nscenarios, e.g., class-incremental learning on ImageNet. This study\ndemonstrates that emulating biological intelligence provides a promising path\nto enable deep neural networks with continual learning capabilities.\n","authors":["Geng Liu","Fei Zhu","Rong Feng","Zhiqiang Yi","Shiqi Wang","Gaofeng Meng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.14727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17211v2","updated":"2025-04-20T19:49:45Z","published":"2025-03-21T15:20:28Z","title":"A Language Anchor-Guided Method for Robust Noisy Domain Generalization","summary":"  Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels.\n","authors":["Zilin Dai","Lehong Wang","Fangzhou Lin","Yidong Wang","Zhigang Li","Kazunori D Yamada","Ziming Zhang","Wang Lu"],"pdf_url":"https://arxiv.org/pdf/2503.17211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05769v3","updated":"2025-04-20T19:36:15Z","published":"2025-02-09T04:06:07Z","title":"Digital Twin Buildings: 3D Modeling, GIS Integration, and Visual\n  Descriptions Using Gaussian Splatting, ChatGPT/Deepseek, and Google Maps\n  Platform","summary":"  Urban digital twins are virtual replicas of cities that use multi-source data\nand data analytics to optimize urban planning, infrastructure management, and\ndecision-making. Towards this, we propose a framework focused on the\nsingle-building scale. By connecting to cloud mapping platforms such as Google\nMap Platforms APIs, by leveraging state-of-the-art multi-agent Large Language\nModels data analysis using ChatGPT(4o) and Deepseek-V3/R1, and by using our\nGaussian Splatting-based mesh extraction pipeline, our Digital Twin Buildings\nframework can retrieve a building's 3D model, visual descriptions, and achieve\ncloud-based mapping integration with large language model-based data analytics\nusing a building's address, postal code, or geographic coordinates.\n","authors":["Kyle Gao","Dening Lu","Liangzhi Li","Nan Chen","Hongjie He","Linlin Xu","Jonathan Li"],"pdf_url":"https://arxiv.org/pdf/2502.05769v3.pdf","comment":"-Fixed minor typo"},{"id":"http://arxiv.org/abs/2504.14717v1","updated":"2025-04-20T19:09:43Z","published":"2025-04-20T19:09:43Z","title":"TAPIP3D: Tracking Any Point in Persistent 3D Geometry","summary":"  We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io\n","authors":["Bowei Zhang","Lei Ke","Adam W. Harley","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2504.14717v1.pdf","comment":"Long-term feed-forward 3D point tracking in persistent 3D point maps.\n  Code:https://github.com/zbw001/TAPIP3D"},{"id":"http://arxiv.org/abs/2504.14715v1","updated":"2025-04-20T19:04:43Z","published":"2025-04-20T19:04:43Z","title":"Med-2D SegNet: A Light Weight Deep Neural Network for Medical 2D Image\n  Segmentation","summary":"  Accurate and efficient medical image segmentation is crucial for advancing\nclinical diagnostics and surgical planning, yet remains a complex challenge due\nto the variability in anatomical structures and the demand for low-complexity\nmodels. In this paper, we introduced Med-2D SegNet, a novel and highly\nefficient segmentation architecture that delivers outstanding accuracy while\nmaintaining a minimal computational footprint. Med-2D SegNet achieves\nstate-of-the-art performance across multiple benchmark datasets, including\nKVASIR-SEG, PH2, EndoVis, and GLAS, with an average Dice similarity coefficient\n(DSC) of 89.77% across 20 diverse datasets. Central to its success is the\ncompact Med Block, a specialized encoder design that incorporates dimension\nexpansion and parameter reduction, enabling precise feature extraction while\nkeeping model parameters to a low count of just 2.07 million. Med-2D SegNet\nexcels in cross-dataset generalization, particularly in polyp segmentation,\nwhere it was trained on KVASIR-SEG and showed strong performance on unseen\ndatasets, demonstrating its robustness in zero-shot learning scenarios, even\nthough we acknowledge that further improvements are possible. With top-tier\nperformance in both binary and multi-class segmentation, Med-2D SegNet\nredefines the balance between accuracy and efficiency, setting a new benchmark\nfor medical image analysis. This work paves the way for developing accessible,\nhigh-performance diagnostic tools suitable for clinical environments and\nresource-constrained settings, making it a step forward in the democratization\nof advanced medical technology.\n","authors":["Md. Sanaullah Chowdhury","Salauddin Tapu","Noyon Kumar Sarkar","Ferdous Bin Ali","Lameya Sabrin"],"pdf_url":"https://arxiv.org/pdf/2504.14715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14709v1","updated":"2025-04-20T18:51:26Z","published":"2025-04-20T18:51:26Z","title":"Exposing the Copycat Problem of Imitation-based Planner: A Novel\n  Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline","summary":"  Machine learning (ML)-based planners have recently gained significant\nattention. They offer advantages over traditional optimization-based planning\nalgorithms. These advantages include fewer manually selected parameters and\nfaster development. Within ML-based planning, imitation learning (IL) is a\ncommon algorithm. It primarily learns driving policies directly from supervised\ntrajectory data. While IL has demonstrated strong performance on many open-loop\nbenchmarks, it remains challenging to determine if the learned policy truly\nunderstands fundamental driving principles, rather than simply extrapolating\nfrom the ego-vehicle's initial state. Several studies have identified this\nlimitation and proposed algorithms to address it. However, these methods often\nuse original datasets for evaluation. In these datasets, future trajectories\nare heavily dependent on initial conditions. Furthermore, IL often overfits to\nthe most common scenarios. It struggles to generalize to rare or unseen\nsituations.\n  To address these challenges, this work proposes: 1) a novel closed-loop\nsimulator supporting both imitation and reinforcement learning, 2) a causal\nbenchmark derived from the Waymo Open Dataset to rigorously assess the impact\nof the copycat problem, and 3) a novel framework integrating imitation learning\nand reinforcement learning to overcome the limitations of purely imitative\napproaches. The code for this work will be released soon.\n","authors":["Hui Zhou","Shaoshuai Shi","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2504.14709v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14708v1","updated":"2025-04-20T18:51:10Z","published":"2025-04-20T18:51:10Z","title":"Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine\n  grained Features","summary":"  Electromyography (EMG) based hand gesture recognition converts forearm muscle\nactivity into control commands for prosthetics, rehabilitation, and human\ncomputer interaction. This paper proposes a novel approach to EMG-based hand\ngesture recognition that uses fine-grained classification and presents XMANet,\nwhich unifies low-level local and high level semantic cues through cross layer\nmutual attention among shallow to deep CNN experts. Using stacked spectrograms\nand scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet\nTransform (WT), we benchmark XMANet against ResNet50, DenseNet-121,\nMobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset\nindicate that, using STFT, the proposed XMANet model outperforms the baseline\nResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement\nof approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing\nthe WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are\nobserved over the same baselines. Similarly, on the FORS EMG dataset, the\nXMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the\nbaseline ResNet50. In comparison, the XMANet(DenseNet121) and\nXMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,\nrespectively. Moreover, when using WT, the proposed XMANet achieves gains of\naround 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,\nMobileNetV3, and EfficientNetB0 models, respectively. These results confirm\nthat XMANet consistently improves performance across various architectures and\nsignal processing techniques, demonstrating the strong potential of fine\ngrained features for accurate and robust EMG classification.\n","authors":["Parshuram N. Aarotale","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2504.14708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09542v2","updated":"2025-04-20T18:49:55Z","published":"2024-09-14T21:42:38Z","title":"MANGO: Learning Disentangled Image Transformation Manifolds with Grouped\n  Operators","summary":"  Learning semantically meaningful image transformations (i.e. rotation,\nthickness, blur) directly from examples can be a challenging task. Recently,\nthe Manifold Autoencoder (MAE) proposed using a set of Lie group operators to\nlearn image transformations directly from examples. However, this approach has\nlimitations, as the learned operators are not guaranteed to be disentangled and\nthe training routine is prohibitively expensive when scaling up the model. To\naddress these limitations, we propose MANGO (transformation Manifolds with\nGrouped Operators) for learning disentangled operators that describe image\ntransformations in distinct latent subspaces. Moreover, our approach allows\npractitioners the ability to define which transformations they aim to model,\nthus improving the semantic meaning of the learned operators. Through our\nexperiments, we demonstrate that MANGO enables composition of image\ntransformations and introduces a one-phase training routine that leads to a\n100x speedup over prior works.\n","authors":["Brighton Ancelin","Yenho Chen","Peimeng Guan","Chiraag Kaushik","Belen Martin-Urcelay","Alex Saad-Falcon","Nakul Singh"],"pdf_url":"https://arxiv.org/pdf/2409.09542v2.pdf","comment":"Submitted to SampTA 2025. This work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2504.14699v1","updated":"2025-04-20T18:28:13Z","published":"2025-04-20T18:28:13Z","title":"IXGS-Intraoperative 3D Reconstruction from Sparse, Arbitrarily Posed\n  Real X-rays","summary":"  Spine surgery is a high-risk intervention demanding precise execution, often\nsupported by image-based navigation systems. Recently, supervised learning\napproaches have gained attention for reconstructing 3D spinal anatomy from\nsparse fluoroscopic data, significantly reducing reliance on\nradiation-intensive 3D imaging systems. However, these methods typically\nrequire large amounts of annotated training data and may struggle to generalize\nacross varying patient anatomies or imaging conditions. Instance-learning\napproaches like Gaussian splatting could offer an alternative by avoiding\nextensive annotation requirements. While Gaussian splatting has shown promise\nfor novel view synthesis, its application to sparse, arbitrarily posed real\nintraoperative X-rays has remained largely unexplored. This work addresses this\nlimitation by extending the $R^2$-Gaussian splatting framework to reconstruct\nanatomically consistent 3D volumes under these challenging conditions. We\nintroduce an anatomy-guided radiographic standardization step using style\ntransfer, improving visual consistency across views, and enhancing\nreconstruction quality. Notably, our framework requires no pretraining, making\nit inherently adaptable to new patients and anatomies. We evaluated our\napproach using an ex-vivo dataset. Expert surgical evaluation confirmed the\nclinical utility of the 3D reconstructions for navigation, especially when\nusing 20 to 30 views, and highlighted the standardization's benefit for\nanatomical clarity. Benchmarking via quantitative 2D metrics (PSNR/SSIM)\nconfirmed performance trade-offs compared to idealized settings, but also\nvalidated the improvement gained from standardization over raw inputs. This\nwork demonstrates the feasibility of instance-based volumetric reconstruction\nfrom arbitrary sparse-view X-rays, advancing intraoperative 3D imaging for\nsurgical navigation.\n","authors":["Sascha Jecklin","Aidana Massalimova","Ruyi Zha","Lilian Calvet","Christoph J. Laux","Mazda Farshad","Philipp Fürnstahl"],"pdf_url":"https://arxiv.org/pdf/2504.14699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14693v1","updated":"2025-04-20T17:58:46Z","published":"2025-04-20T17:58:46Z","title":"Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark","summary":"  Recent advancements in language multimodal models (LMMs) for video have\ndemonstrated their potential for understanding video content, yet the task of\ncomprehending multi-discipline lectures remains largely unexplored. We\nintroduce Video-MMLU, a massive benchmark designed to evaluate the capabilities\nof LMMs in understanding Multi-Discipline Lectures. We evaluate over 90\nopen-source and proprietary models, ranging from 0.5B to 40B parameters. Our\nresults highlight the limitations of current models in addressing the cognitive\nchallenges presented by these lectures, especially in tasks requiring both\nperception and reasoning. Additionally, we explore how the number of visual\ntokens and the large language models influence performance, offering insights\ninto the interplay between multimodal perception and reasoning in lecture\ncomprehension.\n","authors":["Enxin Song","Wenhao Chai","Weili Xu","Jianwen Xie","Yuxuan Liu","Gaoang Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14693v1.pdf","comment":"Code, docs, and benchmark are all avaliable at\n  https://enxinsong.com/Video-MMLU-web/"},{"id":"http://arxiv.org/abs/2210.13167v2","updated":"2025-04-20T17:55:15Z","published":"2022-10-24T12:36:40Z","title":"Exploring Self-Attention for Crop-type Classification Explainability","summary":"  Transformer models have become a promising approach for crop-type\nclassification. Although their attention weights can be used to understand the\nrelevant time points for crop disambiguation, the validity of these insights\ndepends on how closely the attention weights approximate the actual workings of\nthese black-box models, which is not always clear. In this paper, we introduce\na novel explainability framework that systematically evaluates the explanatory\npower of the attention weights of a standard transformer encoder for crop-type\nclassification. Our results show that attention patterns strongly relate to key\ndates, which are often associated with critical phenological events for\ncrop-type classification. Further, the sensitivity analysis reveals the limited\ncapability of the attention weights to characterize crop phenology as the\nidentified phenological events depend on the other crops considered during\ntraining. This limitation highlights the relevance of future work towards the\ndevelopment of deep learning approaches capable of automatically learning the\ntemporal vegetation dynamics for accurate crop disambiguation\n","authors":["Ivica Obadic","Ribana Roscher","Dario Augusto Borges Oliveira","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2210.13167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14687v1","updated":"2025-04-20T17:37:02Z","published":"2025-04-20T17:37:02Z","title":"Seurat: From Moving Points to Depth","summary":"  Accurate depth estimation from monocular videos remains challenging due to\nambiguities inherent in single-view geometry, as crucial depth cues like\nstereopsis are absent. However, humans often perceive relative depth\nintuitively by observing variations in the size and spacing of objects as they\nmove. Inspired by this, we propose a novel method that infers relative depth by\nexamining the spatial relationships and temporal evolution of a set of tracked\n2D trajectories. Specifically, we use off-the-shelf point tracking models to\ncapture 2D trajectories. Then, our approach employs spatial and temporal\ntransformers to process these trajectories and directly infer depth changes\nover time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust\nzero-shot performance, generalizing effectively from synthetic to real-world\ndatasets. Results indicate that our approach achieves temporally smooth,\nhigh-accuracy depth predictions across diverse domains.\n","authors":["Seokju Cho","Jiahui Huang","Seungryong Kim","Joon-Young Lee"],"pdf_url":"https://arxiv.org/pdf/2504.14687v1.pdf","comment":"CVPR 2025 Highlight. Project page: https://seurat-cvpr.github.io"},{"id":"http://arxiv.org/abs/2403.13319v3","updated":"2025-04-20T16:17:38Z","published":"2024-03-20T05:50:04Z","title":"HyperFusion: A Hypernetwork Approach to Multimodal Integration of\n  Tabular and Medical Imaging Data for Predictive Modeling","summary":"  The integration of diverse clinical modalities such as medical imaging and\nthe tabular data extracted from patients' Electronic Health Records (EHRs) is a\ncrucial aspect of modern healthcare. Integrative analysis of multiple sources\ncan provide a comprehensive understanding of the clinical condition of a\npatient, improving diagnosis and treatment decision. Deep Neural Networks\n(DNNs) consistently demonstrate outstanding performance in a wide range of\nmultimodal tasks in the medical domain. However, the complex endeavor of\neffectively merging medical imaging with clinical, demographic and genetic\ninformation represented as numerical tabular data remains a highly active and\nongoing research pursuit.\n  We present a novel framework based on hypernetworks to fuse clinical imaging\nand tabular data by conditioning the image processing on the EHR's values and\nmeasurements. This approach aims to leverage the complementary information\npresent in these modalities to enhance the accuracy of various medical\napplications. We demonstrate the strength and generality of our method on two\ndifferent brain Magnetic Resonance Imaging (MRI) analysis tasks, namely, brain\nage prediction conditioned by subject's sex and multi-class Alzheimer's Disease\n(AD) classification conditioned by tabular data. We show that our framework\noutperforms both single-modality models and state-of-the-art MRI tabular data\nfusion methods. A link to our code can be found at\nhttps://github.com/daniel4725/HyperFusion\n","authors":["Daniel Duenias","Brennan Nichyporuk","Tal Arbel","Tammy Riklin Raviv"],"pdf_url":"https://arxiv.org/pdf/2403.13319v3.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2504.14666v1","updated":"2025-04-20T16:14:28Z","published":"2025-04-20T16:14:28Z","title":"Generative Multimodal Pretraining with Discrete Diffusion Timestep\n  Tokens","summary":"  Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify\nvisual comprehension and generation by combining LLM and diffusion models, the\nstate-of-the-art in each task, respectively. Existing approaches rely on\nspatial visual tokens, where image patches are encoded and arranged according\nto a spatial order (e.g., raster scan). However, we show that spatial tokens\nlack the recursive structure inherent to languages, hence form an impossible\nlanguage for LLM to master. In this paper, we build a proper visual language by\nleveraging diffusion timesteps to learn discrete, recursive visual tokens. Our\nproposed tokens recursively compensate for the progressive attribute loss in\nnoisy images as timesteps increase, enabling the diffusion model to reconstruct\nthe original image at any timestep. This approach allows us to effectively\nintegrate the strengths of LLMs in autoregressive reasoning and diffusion\nmodels in precise image generation, achieving seamless multimodal comprehension\nand generation within a unified framework. Extensive experiments show that we\nachieve superior performance for multimodal comprehension and generation\nsimultaneously compared with other MLLMs. Project Page:\nhttps://DDT-LLaMA.github.io/.\n","authors":["Kaihang Pan","Wang Lin","Zhongqi Yue","Tenglong Ao","Liyu Jia","Wei Zhao","Juncheng Li","Siliang Tang","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.14666v1.pdf","comment":"Accepted by CVPR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2504.14665v1","updated":"2025-04-20T16:14:07Z","published":"2025-04-20T16:14:07Z","title":"DMPCN: Dynamic Modulated Predictive Coding Network with Hybrid Feedback\n  Representations","summary":"  Traditional predictive coding networks, inspired by theories of brain\nfunction, consistently achieve promising results across various domains,\nextending their influence into the field of computer vision. However, the\nperformance of the predictive coding networks is limited by their error\nfeedback mechanism, which traditionally employs either local or global\nrecurrent updates, leading to suboptimal performance in processing both local\nand broader details simultaneously. In addition, traditional predictive coding\nnetworks face difficulties in dynamically adjusting to the complexity and\ncontext of varying input data, which is crucial for achieving high levels of\nperformance in diverse scenarios. Furthermore, there is a gap in the\ndevelopment and application of specific loss functions that could more\neffectively guide the model towards optimal performance. To deal with these\nissues, this paper introduces a hybrid prediction error feedback mechanism with\ndynamic modulation for deep predictive coding networks by effectively combining\nglobal contexts and local details while adjusting feedback based on input\ncomplexity. Additionally, we present a loss function tailored to this framework\nto improve accuracy by focusing on precise prediction error minimization.\nExperimental results demonstrate the superiority of our model over other\napproaches, showcasing faster convergence and higher predictive accuracy in\nCIFAR-10, CIFAR-100, MNIST, and FashionMNIST datasets.\n","authors":["A S M Sharifuzzaman Sagar","Yu Chen","Jun Hoong Chan"],"pdf_url":"https://arxiv.org/pdf/2504.14665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14664v1","updated":"2025-04-20T16:00:38Z","published":"2025-04-20T16:00:38Z","title":"Frequency-domain Learning with Kernel Prior for Blind Image Deblurring","summary":"  While achieving excellent results on various datasets, many deep learning\nmethods for image deblurring suffer from limited generalization capabilities\nwith out-of-domain data. This limitation is likely caused by their dependence\non certain domain-specific datasets. To address this challenge, we argue that\nit is necessary to introduce the kernel prior into deep learning methods, as\nthe kernel prior remains independent of the image context. For effective fusion\nof kernel prior information, we adopt a rational implementation method inspired\nby traditional deblurring algorithms that perform deconvolution in the\nfrequency domain. We propose a module called Frequency Integration Module (FIM)\nfor fusing the kernel prior and combine it with a frequency-based deblurring\nTransfomer network. Experimental results demonstrate that our method\noutperforms state-of-the-art methods on multiple blind image deblurring tasks,\nshowcasing robust generalization abilities. Source code will be available soon.\n","authors":["Jixiang Sun","Fei Lei","Jiawei Zhang","Wenxiu Sun","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2504.14664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14662v1","updated":"2025-04-20T15:57:12Z","published":"2025-04-20T15:57:12Z","title":"Mitigating Parameter Interference in Model Merging via Sharpness-Aware\n  Fine-Tuning","summary":"  Large-scale deep learning models with a pretraining-finetuning paradigm have\nled to a surge of numerous task-specific models fine-tuned from a common\npre-trained model. Recently, several research efforts have been made on merging\nthese large models into a single multi-task model, particularly with simple\narithmetic on parameters. Such merging methodology faces a central challenge:\ninterference between model parameters fine-tuned on different tasks. Few recent\nworks have focused on designing a new fine-tuning scheme that can lead to small\nparameter interference, however at the cost of the performance of each\ntask-specific fine-tuned model and thereby limiting that of a merged model. To\nimprove the performance of a merged model, we note that a fine-tuning scheme\nshould aim for (1) smaller parameter interference and (2) better performance of\neach fine-tuned model on the corresponding task. In this work, we aim to design\na new fine-tuning objective function to work towards these two goals. In the\ncourse of this process, we find such objective function to be strikingly\nsimilar to sharpness-aware minimization (SAM) objective function, which aims to\nachieve generalization by finding flat minima. Drawing upon our observation, we\npropose to fine-tune pre-trained models via sharpness-aware minimization. The\nexperimental and theoretical results showcase the effectiveness and\northogonality of our proposed approach, improving performance upon various\nmerging and fine-tuning methods. Our code is available at\nhttps://github.com/baiklab/SAFT-Merge.\n","authors":["Yeoreum Lee","Jinwook Jung","Sungyong Baik"],"pdf_url":"https://arxiv.org/pdf/2504.14662v1.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2504.04323v2","updated":"2025-04-20T15:56:56Z","published":"2025-04-06T01:44:46Z","title":"MedM-VL: What Makes a Good Medical LVLM?","summary":"  Medical image analysis is essential in modern healthcare. Deep learning has\nredirected research focus toward complex medical multimodal tasks, including\nreport generation and visual question answering. Traditional task-specific\nmodels often fall short in handling these challenges. Large vision-language\nmodels (LVLMs) offer new solutions for solving such tasks. In this study, we\nbuild on the popular LLaVA framework to systematically explore model\narchitectures and training strategies for both 2D and 3D medical LVLMs. We\npresent extensive empirical findings and practical guidance. To support\nreproducibility and future research, we release a modular codebase, MedM-VL,\nand two pre-trained models: MedM-VL-2D for 2D medical image analysis and\nMedM-VL-CT-Chest for 3D CT-based applications. The code and models are\navailable at: https://github.com/MSIIP/MedM-VL\n","authors":["Yiming Shi","Shaoshuai Yang","Xun Zhu","Haoyu Wang","Miao Li","Ji Wu"],"pdf_url":"https://arxiv.org/pdf/2504.04323v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14658v1","updated":"2025-04-20T15:40:00Z","published":"2025-04-20T15:40:00Z","title":"EmoSEM: Segment and Explain Emotion Stimuli in Visual Art","summary":"  This paper focuses on a key challenge in visual art understanding: given an\nart image, the model pinpoints pixel regions that trigger a specific human\nemotion, and generates linguistic explanations for the emotional arousal.\nDespite recent advances in art understanding, pixel-level emotion understanding\nstill faces a dual challenge: first, the subjectivity of emotion makes it\ndifficult for general segmentation models like SAM to adapt to emotion-oriented\nsegmentation tasks; and second, the abstract nature of art expression makes it\ndifficult for captioning models to balance pixel-level semantic understanding\nand emotion reasoning. To solve the above problems, this paper proposes the\nEmotion stimuli Segmentation and Explanation Model (EmoSEM) to endow the\nsegmentation model SAM with emotion comprehension capability. First, to enable\nthe model to perform segmentation under the guidance of emotional intent well,\nwe introduce an emotional prompt with a learnable mask token as the conditional\ninput for segmentation decoding. Then, we design an emotion projector to\nestablish the association between emotion and visual features. Next, more\nimportantly, to address emotion-visual stimuli alignment, we develop a\nlightweight prefix projector, a module that fuses the learned emotional mask\nwith the corresponding emotion into a unified representation compatible with\nthe language model.Finally, we input the joint visual, mask, and emotional\ntokens into the language model and output the emotional explanations. It\nensures that the generated interpretations remain semantically and emotionally\ncoherent with the visual stimuli. The method innovatively realizes end-to-end\nmodeling from low-level pixel features to high-level emotion interpretation,\nproviding the first interpretable fine-grained analysis framework for artistic\nemotion computing. Extensive experiments validate the effectiveness of our\nmodel.\n","authors":["Jing Zhang","Dan Guo","Zhangbin Li","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06613v2","updated":"2025-04-20T15:35:20Z","published":"2024-07-09T07:36:54Z","title":"Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View","summary":"  Recent studies construct deblurred neural radiance fields~(DeRF) using dozens\nof blurry images, which are not practical scenarios if only a limited number of\nblurry images are available. This paper focuses on constructing DeRF from\nsparse-view for more pragmatic real-world scenarios. As observed in our\nexperiments, establishing DeRF from sparse views proves to be a more\nchallenging problem due to the inherent complexity arising from the\nsimultaneous optimization of blur kernels and NeRF from sparse view.\nSparse-DeRF successfully regularizes the complicated joint optimization,\npresenting alleviated overfitting artifacts and enhanced quality on radiance\nfields. The regularization consists of three key components: Surface\nsmoothness, helps the model accurately predict the scene structure utilizing\nunseen and additional hidden rays derived from the blur kernel based on\nstatistical tendencies of real-world; Modulated gradient scaling, helps the\nmodel adjust the amount of the backpropagated gradient according to the\narrangements of scene objects; Perceptual distillation improves the perceptual\nquality by overcoming the ill-posed multi-view inconsistency of image\ndeblurring and distilling the pre-deblurred information, compensating for the\nlack of clean information in blurry images. We demonstrate the effectiveness of\nthe Sparse-DeRF with extensive quantitative and qualitative experimental\nresults by training DeRF from 2-view, 4-view, and 6-view blurry images.\n","authors":["Dogyoon Lee","Donghyeong Kim","Jungho Lee","Minhyeok Lee","Seunghoon Lee","Sangyoun Lee"],"pdf_url":"https://arxiv.org/pdf/2407.06613v2.pdf","comment":"Accepted and to appear in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI). Project page:\n  https://dogyoonlee.github.io/sparsederf/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.16358v2","updated":"2025-04-20T19:49:10Z","published":"2025-02-22T21:14:18Z","title":"Wrong Answers Can Also Be Useful: PlausibleQA -- A Large-Scale QA\n  Dataset with Answer Plausibility Scores","summary":"  Large Language Models (LLMs) are revolutionizing information retrieval, with\nchatbots becoming an important source for answering user queries. As by their\ndesign, LLMs prioritize generating correct answers, the value of highly\nplausible yet incorrect answers (candidate answers) tends to be overlooked.\nHowever, such answers can still prove useful, for example, they can play a\ncrucial role in tasks like Multiple-Choice Question Answering (MCQA) and QA\nRobustness Assessment (QARA). Existing QA datasets primarily focus on correct\nanswers without explicit consideration of the plausibility of other candidate\nanswers, limiting opportunity for more nuanced evaluations of models. To\naddress this gap, we introduce PlausibleQA, a large-scale dataset comprising\n10,000 questions and 100,000 candidate answers, each annotated with\nplausibility scores and justifications for their selection. Additionally, the\ndataset includes 900,000 justifications for pairwise comparisons between\ncandidate answers, further refining plausibility assessments. We evaluate\nPlausibleQA through human assessments and empirical experiments, demonstrating\nits utility in MCQA and QARA analysis. Our findings show that\nplausibility-aware approaches are effective for MCQA distractor generation and\nQARA. We release PlausibleQA as a resource for advancing QA research and\nenhancing LLM performance in distinguishing plausible distractors from correct\nanswers.\n","authors":["Jamshid Mozafari","Abdelrahman Abdallah","Bhawna Piryani","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.16358v2.pdf","comment":"Accepted at SIGIR 2025"},{"id":"http://arxiv.org/abs/2412.01626v3","updated":"2025-04-20T19:43:24Z","published":"2024-12-02T15:44:19Z","title":"WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation","summary":"  The use of Large Language Models (LLMs) has increased significantly with\nusers frequently asking questions to chatbots. In the time when information is\nreadily accessible, it is crucial to stimulate and preserve human cognitive\nabilities and maintain strong reasoning skills. This paper addresses such\nchallenges by promoting the use of hints as an alternative or a supplement to\ndirect answers. We first introduce a manually constructed hint dataset,\nWikiHint, which is based on Wikipedia and includes 5,000 hints created for\n1,000 questions. We then finetune open-source LLMs for hint generation in\nanswer-aware and answer-agnostic contexts. We assess the effectiveness of the\nhints with human participants who answer questions with and without the aid of\nhints. Additionally, we introduce a lightweight evaluation method, HintRank, to\nevaluate and rank hints in both answer-aware and answer-agnostic settings. Our\nfindings show that (a) the dataset helps generate more effective hints, (b)\nincluding answer information along with questions generally improves the\nquality of generated hints, and (c) encoder-based models perform better than\ndecoder-based models in hint ranking.\n","authors":["Jamshid Mozafari","Florian Gerhold","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2412.01626v3.pdf","comment":"Accepted at SIGIR 2025"},{"id":"http://arxiv.org/abs/2405.17890v4","updated":"2025-04-20T18:07:13Z","published":"2024-05-28T07:12:06Z","title":"SLMRec: Distilling Large Language Models into Small for Sequential\n  Recommendation","summary":"  Sequential Recommendation (SR) task involves predicting the next item a user\nis likely to interact with, given their past interactions. The SR models\nexamine the sequence of a user's actions to discern more complex behavioral\npatterns and temporal dynamics. Recent research demonstrates the great impact\nof LLMs on sequential recommendation systems, either viewing sequential\nrecommendation as language modeling or serving as the backbone for user\nrepresentation. Although these methods deliver outstanding performance, there\nis scant evidence of the necessity of a large language model and how large the\nlanguage model is needed, especially in the sequential recommendation scene.\nMeanwhile, due to the huge size of LLMs, it is inefficient and impractical to\napply a LLM-based model in real-world platforms that often need to process\nbillions of traffic logs daily. In this paper, we explore the influence of\nLLMs' depth by conducting extensive experiments on large-scale industry\ndatasets. Surprisingly, our motivational experiments reveal that most\nintermediate layers of LLMs are redundant, indicating that pruning the\nremaining layers can still maintain strong performance. Motivated by this\ninsight, we empower small language models for SR, namely SLMRec, which adopt a\nsimple yet effective knowledge distillation method. Moreover, SLMRec is\northogonal to other post-training efficiency techniques, such as quantization\nand pruning, so that they can be leveraged in combination. Comprehensive\nexperimental results illustrate that the proposed SLMRec model attains the best\nperformance using only 13% of the parameters found in LLM-based recommendation\nmodels while simultaneously achieving up to 6.6x and 8.0x speedups in training\nand inference time costs, respectively. Besides, we provide a theoretical\njustification for why small language models can perform comparably to large\nlanguage models in SR.\n","authors":["Wujiang Xu","Qitian Wu","Zujie Liang","Jiaojiao Han","Xuying Ning","Yunxiao Shi","Wenfang Lin","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.17890v4.pdf","comment":"International Conference on Learning Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2504.14587v1","updated":"2025-04-20T12:28:49Z","published":"2025-04-20T12:28:49Z","title":"Generative Auto-Bidding with Value-Guided Explorations","summary":"  Auto-bidding, with its strong capability to optimize bidding decisions within\ndynamic and competitive online environments, has become a pivotal strategy for\nadvertising platforms. Existing approaches typically employ rule-based\nstrategies or Reinforcement Learning (RL) techniques. However, rule-based\nstrategies lack the flexibility to adapt to time-varying market conditions, and\nRL-based methods struggle to capture essential historical dependencies and\nobservations within Markov Decision Process (MDP) frameworks. Furthermore,\nthese approaches often face challenges in ensuring strategy adaptability across\ndiverse advertising objectives. Additionally, as offline training methods are\nincreasingly adopted to facilitate the deployment and maintenance of stable\nonline strategies, the issues of documented behavioral patterns and behavioral\ncollapse resulting from training on fixed offline datasets become increasingly\nsignificant. To address these limitations, this paper introduces a novel\noffline Generative Auto-bidding framework with Value-Guided Explorations\n(GAVE). GAVE accommodates various advertising objectives through a score-based\nReturn-To-Go (RTG) module. Moreover, GAVE integrates an action exploration\nmechanism with an RTG-based evaluation method to explore novel actions while\nensuring stability-preserving updates. A learnable value function is also\ndesigned to guide the direction of action exploration and mitigate\nOut-of-Distribution (OOD) problems. Experimental results on two offline\ndatasets and real-world deployments demonstrate that GAVE outperforms\nstate-of-the-art baselines in both offline evaluations and online A/B tests.\nThe implementation code is publicly available to facilitate reproducibility and\nfurther research.\n","authors":["Jingtong Gao","Yewen Li","Shuai Mao","Peng Jiang","Nan Jiang","Yejing Wang","Qingpeng Cai","Fei Pan","Peng Jiang","Kun Gai","Bo An","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.14587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14565v1","updated":"2025-04-20T10:47:21Z","published":"2025-04-20T10:47:21Z","title":"Matrix Factorization with Dynamic Multi-view Clustering for Recommender\n  System","summary":"  Matrix factorization (MF), a cornerstone of recommender systems, decomposes\nuser-item interaction matrices into latent representations. Traditional MF\napproaches, however, employ a two-stage, non-end-to-end paradigm, sequentially\nperforming recommendation and clustering, resulting in prohibitive\ncomputational costs for large-scale applications like e-commerce and IoT, where\nbillions of users interact with trillions of items. To address this, we propose\nMatrix Factorization with Dynamic Multi-view Clustering (MFDMC), a unified\nframework that balances efficient end-to-end training with comprehensive\nutilization of web-scale data and enhances interpretability. MFDMC leverages\ndynamic multi-view clustering to learn user and item representations,\nadaptively pruning poorly formed clusters. Each entity's representation is\nmodeled as a weighted projection of robust clusters, capturing its diverse\nroles across views. This design maximizes representation space utilization,\nimproves interpretability, and ensures resilience for downstream tasks.\nExtensive experiments demonstrate MFDMC's superior performance in recommender\nsystems and other representation learning domains, such as computer vision,\nhighlighting its scalability and versatility.\n","authors":["Shangde Gao","Ke Liu","Yichao Fu","Hongxia Xu","Jian Wu"],"pdf_url":"https://arxiv.org/pdf/2504.14565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13117v2","updated":"2025-04-20T10:42:19Z","published":"2024-10-17T01:02:04Z","title":"Preference Diffusion for Recommendation","summary":"  Recommender systems predict personalized item rankings based on user\npreference distributions derived from historical behavior data. Recently,\ndiffusion models (DMs) have gained attention in recommendation for their\nability to model complex distributions, yet current DM-based recommenders often\nrely on traditional objectives like mean squared error (MSE) or recommendation\nobjectives, which are not optimized for personalized ranking tasks or fail to\nfully leverage DM's generative potential. To address this, we propose\nPreferDiff, a tailored optimization objective for DM-based recommenders.\nPreferDiff transforms BPR into a log-likelihood ranking objective and\nintegrates multiple negative samples to better capture user preferences.\nSpecifically, we employ variational inference to handle the intractability\nthrough minimizing the variational upper bound and replaces MSE with cosine\nerror to improve alignment with recommendation tasks. Finally, we balance\nlearning generation and preference to enhance the training stability of DMs.\nPreferDiff offers three key benefits: it is the first personalized ranking loss\ndesigned specifically for DM-based recommenders and it improves ranking and\nfaster convergence by addressing hard negatives. We also prove that it is\ntheoretically connected to Direct Preference Optimization which indicates that\nit has the potential to align user preferences in DM-based recommenders via\ngenerative modeling. Extensive experiments across three benchmarks validate its\nsuperior recommendation performance and commendable general sequential\nrecommendation capabilities. Our codes are available at\nhttps://github.com/lswhim/PreferDiff.\n","authors":["Shuo Liu","An Zhang","Guoqing Hu","Hong Qian","Tat-seng Chua"],"pdf_url":"https://arxiv.org/pdf/2410.13117v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2504.14550v1","updated":"2025-04-20T09:43:23Z","published":"2025-04-20T09:43:23Z","title":"Regret-aware Re-ranking for Guaranteeing Two-sided Fairness and Accuracy\n  in Recommender Systems","summary":"  In multi-stakeholder recommender systems (RS), users and providers operate as\ntwo crucial and interdependent roles, whose interests must be well-balanced.\nPrior research, including our work BankFair, has demonstrated the importance of\nguaranteeing both provider fairness and user accuracy to meet their interests.\nHowever, when they balance the two objectives, another critical factor emerges\nin RS: individual fairness, which manifests as a significant disparity in\nindividual recommendation accuracy, with some users receiving high accuracy\nwhile others are left with notably low accuracy. This oversight severely harms\nthe interests of users and exacerbates social polarization. How to guarantee\nindividual fairness while ensuring user accuracy and provider fairness remains\nan unsolved problem. To bridge this gap, in this paper, we propose our method\nBankFair+. Specifically, BankFair+ extends BankFair with two steps: (1)\nintroducing a non-linear function from regret theory to ensure individual\nfairness while enhancing user accuracy; (2) formulating the re-ranking process\nas a regret-aware fuzzy programming problem to meet the interests of both\nindividual user and provider, therefore balancing the trade-off between\nindividual fairness and provider fairness. Experiments on two real-world\nrecommendation datasets demonstrate that BankFair+ outperforms all baselines\nregarding individual fairness, user accuracy, and provider fairness.\n","authors":["Xiaopeng Ye","Chen Xu","Jun Xu","Xuyang Xie","Gang Wang","Zhenhua Dong"],"pdf_url":"https://arxiv.org/pdf/2504.14550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14493v1","updated":"2025-04-20T04:58:14Z","published":"2025-04-20T04:58:14Z","title":"FinSage: A Multi-aspect RAG System for Financial Filings Question\n  Answering","summary":"  Leveraging large language models in real-world settings often entails a need\nto utilize domain-specific data and tools in order to follow the complex\nregulations that need to be followed for acceptable use. Within financial\nsectors, modern enterprises increasingly rely on Retrieval-Augmented Generation\n(RAG) systems to address complex compliance requirements in financial document\nworkflows. However, existing solutions struggle to account for the inherent\nheterogeneity of data (e.g., text, tables, diagrams) and evolving nature of\nregulatory standards used in financial filings, leading to compromised accuracy\nin critical information extraction. We propose the FinSage framework as a\nsolution, utilizing a multi-aspect RAG framework tailored for regulatory\ncompliance analysis in multi-modal financial documents. FinSage introduces\nthree innovative components: (1) a multi-modal pre-processing pipeline that\nunifies diverse data formats and generates chunk-level metadata summaries, (2)\na multi-path sparse-dense retrieval system augmented with query expansion\n(HyDE) and metadata-aware semantic search, and (3) a domain-specialized\nre-ranking module fine-tuned via Direct Preference Optimization (DPO) to\nprioritize compliance-critical content. Extensive experiments demonstrate that\nFinSage achieves an impressive recall of 92.51% on 75 expert-curated questions\nderived from surpasses the best baseline method on the FinanceBench question\nanswering datasets by 24.06% in accuracy. Moreover, FinSage has been\nsuccessfully deployed as financial question-answering agent in online meetings,\nwhere it has already served more than 1,200 people.\n","authors":["Xinyu Wang","Jijun Chi","Zhenghan Tai","Tung Sum Thomas Kwok","Muzhi Li","Zhuhong Li","Hailin He","Yuchen Hua","Peng Lu","Suyuchen Wang","Yihong Wu","Jerry Huang","Ling Zhou"],"pdf_url":"https://arxiv.org/pdf/2504.14493v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2504.14772v1","updated":"2025-04-20T23:50:23Z","published":"2025-04-20T23:50:23Z","title":"Knowledge Distillation and Dataset Distillation of Large Language\n  Models: Emerging Trends, Challenges, and Future Directions","summary":"  The exponential growth of Large Language Models (LLMs) continues to highlight\nthe need for efficient strategies to meet ever-expanding computational and data\ndemands. This survey provides a comprehensive analysis of two complementary\nparadigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both\naimed at compressing LLMs while preserving their advanced reasoning\ncapabilities and linguistic diversity. We first examine key methodologies in\nKD, such as task-specific alignment, rationale-based training, and\nmulti-teacher frameworks, alongside DD techniques that synthesize compact,\nhigh-impact datasets through optimization-based gradient matching, latent space\nregularization, and generative synthesis. Building on these foundations, we\nexplore how integrating KD and DD can produce more effective and scalable\ncompression strategies. Together, these approaches address persistent\nchallenges in model scalability, architectural heterogeneity, and the\npreservation of emergent LLM abilities. We further highlight applications\nacross domains such as healthcare and education, where distillation enables\nefficient deployment without sacrificing performance. Despite substantial\nprogress, open challenges remain in preserving emergent reasoning and\nlinguistic diversity, enabling efficient adaptation to continually evolving\nteacher models and datasets, and establishing comprehensive evaluation\nprotocols. By synthesizing methodological innovations, theoretical foundations,\nand practical insights, our survey charts a path toward sustainable,\nresource-efficient LLMs through the tighter integration of KD and DD\nprinciples.\n","authors":["Luyang Fang","Xiaowei Yu","Jiazhang Cai","Yongkai Chen","Shushan Wu","Zhengliang Liu","Zhenyuan Yang","Haoran Lu","Xilin Gong","Yufang Liu","Terry Ma","Wei Ruan","Ali Abbasi","Jing Zhang","Tao Wang","Ehsan Latif","Wei Liu","Wei Zhang","Soheil Kolouri","Xiaoming Zhai","Dajiang Zhu","Wenxuan Zhong","Tianming Liu","Ping Ma"],"pdf_url":"https://arxiv.org/pdf/2504.14772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14762v1","updated":"2025-04-20T23:09:20Z","published":"2025-04-20T23:09:20Z","title":"A Combinatorial Theory of Dropout: Subnetworks, Graph Geometry, and\n  Generalization","summary":"  We propose a combinatorial and graph-theoretic theory of dropout by modeling\ntraining as a random walk over a high-dimensional graph of binary subnetworks.\nEach node represents a masked version of the network, and dropout induces\nstochastic traversal across this space. We define a subnetwork contribution\nscore that quantifies generalization and show that it varies smoothly over the\ngraph. Using tools from spectral graph theory, PAC-Bayes analysis, and\ncombinatorics, we prove that generalizing subnetworks form large, connected,\nlow-resistance clusters, and that their number grows exponentially with network\nwidth. This reveals dropout as a mechanism for sampling from a robust,\nstructured ensemble of well-generalizing subnetworks with built-in redundancy.\nExtensive experiments validate every theoretical claim across diverse\narchitectures. Together, our results offer a unified foundation for\nunderstanding dropout and suggest new directions for mask-guided regularization\nand subnetwork optimization.\n","authors":["Sahil Rajesh Dhayalkar"],"pdf_url":"https://arxiv.org/pdf/2504.14762v1.pdf","comment":"17 pages (9 pages main content and remaining pages are references,\n  appendix which includes 7 figures, proofs and derivations)"},{"id":"http://arxiv.org/abs/2504.14751v1","updated":"2025-04-20T22:22:00Z","published":"2025-04-20T22:22:00Z","title":"AI for the Open-World: the Learning Principles","summary":"  During the past decades, numerous successes of AI has been made on \"specific\ncapabilities\", named closed-world, such as artificial environments or specific\nreal-world tasks. This well-defined narrow capability brings two nice benefits,\na clear criterion of success and the opportunity to collect a lot of examples.\nThe criteria not only reveal whether a machine has achieved a goal, but reveal\nhow the machine falls short of the goal. As a result, human designers can fix\nthe problems one after the other until the machine is deemed good enough for\nthe task. Furthermore, the large set of collected examples reduces the\ndifficulty of this problem-fixing process (by the central limit theorem).\n  Do the success in closed-world translate into broad open-world, where a\nmachine is required to perform any task that a human could possibly undertake\nwith fewer examples and less priori knowledge from human designers? No. Because\ncompetence in a specific task provides little insight in handling other tasks,\nthe valuable criteria for specific tasks become helpless when handling broader\nunseen tasks. Furthermore, due to the shortage of examples in unseen tasks,\ncentral limit theorem does not stand on our side. At the end, human designers\nlose the oscilloscope to \"hack\" an AI system for the open-world.\n  Achieving AI for the open-world requires unique learning principles and\ninnovated techniques, which are different from the ones in building AI for the\nclosed-world. This thesis explores necessary learning principles required to\nconstruct AI for the open-world, including rich features (analogy a large tool\nbox), disentangled representation (an organized tool box), and inference-time\nlearning (a tool-savvy hand). Driven by the learning principles, this thesis\nfurther proposes techniques to use the learning principles, conducts enormous\nlarge-scale experiments to verify the learning principles.\n","authors":["Jianyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.14751v1.pdf","comment":"PhD thesis. This is not a compilation of published papers, but a new\n  one"},{"id":"http://arxiv.org/abs/2411.17116v2","updated":"2025-04-20T21:50:03Z","published":"2024-11-26T05:10:04Z","title":"Star Attention: Efficient LLM Inference over Long Sequences","summary":"  Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.\n","authors":["Shantanu Acharya","Fei Jia","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2411.17116v2.pdf","comment":"Code: https://github.com/NVIDIA/Star-Attention"},{"id":"http://arxiv.org/abs/2410.05298v2","updated":"2025-04-20T21:31:45Z","published":"2024-10-04T04:48:33Z","title":"How Do Large Language Models Understand Graph Patterns? A Benchmark for\n  Graph Pattern Comprehension","summary":"  Benchmarking the capabilities and limitations of large language models (LLMs)\nin graph-related tasks is becoming an increasingly popular and crucial area of\nresearch. Recent studies have shown that LLMs exhibit a preliminary ability to\nunderstand graph structures and node features. However, the potential of LLMs\nin graph pattern mining remains largely unexplored. This is a key component in\nfields such as computational chemistry, biology, and social network analysis.\nTo bridge this gap, this work introduces a comprehensive benchmark to assess\nLLMs' capabilities in graph pattern tasks. We have developed a benchmark that\nevaluates whether LLMs can understand graph patterns based on either\nterminological or topological descriptions. Additionally, our benchmark tests\nthe LLMs' capacity to autonomously discover graph patterns from data. The\nbenchmark encompasses both synthetic and real datasets, and a variety of\nmodels, with a total of 11 tasks and 7 models. Our experimental framework is\ndesigned for easy expansion to accommodate new models and datasets. Our\nfindings reveal that: (1) LLMs have preliminary abilities to understand graph\npatterns, with O1-mini outperforming in the majority of tasks; (2) Formatting\ninput data to align with the knowledge acquired during pretraining can enhance\nperformance; (3) The strategies employed by LLMs may differ from those used in\nconventional algorithms.\n","authors":["Xinnan Dai","Haohao Qu","Yifen Shen","Bohang Zhang","Qihao Wen","Wenqi Fan","Dongsheng Li","Jiliang Tang","Caihua Shan"],"pdf_url":"https://arxiv.org/pdf/2410.05298v2.pdf","comment":"The paper is published in ICLR 2025"},{"id":"http://arxiv.org/abs/2504.14744v1","updated":"2025-04-20T21:27:23Z","published":"2025-04-20T21:27:23Z","title":"On the Tunability of Random Survival Forests Model for Predictive\n  Maintenance","summary":"  This paper investigates the tunability of the Random Survival Forest (RSF)\nmodel in predictive maintenance, where accurate time-to-failure estimation is\ncrucial. Although RSF is widely used due to its flexibility and ability to\nhandle censored data, its performance is sensitive to hyperparameter\nconfigurations. However, systematic evaluations of RSF tunability remain\nlimited, especially in predictive maintenance contexts. We introduce a\nthree-level framework to quantify tunability: (1) a model-level metric\nmeasuring overall performance gain from tuning, (2) a hyperparameter-level\nmetric assessing individual contributions, and (3) identification of optimal\ntuning ranges. These metrics are evaluated across multiple datasets using\nsurvival-specific criteria: the C-index for discrimination and the Brier score\nfor calibration. Experiments on four CMAPSS dataset subsets, simulating\naircraft engine degradation, reveal that hyperparameter tuning consistently\nimproves model performance. On average, the C-index increased by 0.0547, while\nthe Brier score decreased by 0.0199. These gains were consistent across all\nsubsets. Moreover, ntree and mtry showed the highest average tunability, while\nnodesize offered stable improvements within the range of 10 to 30. In contrast,\nsplitrule demonstrated negative tunability on average, indicating that improper\ntuning may reduce model performance. Our findings emphasize the practical\nimportance of hyperparameter tuning in survival models and provide actionable\ninsights for optimizing RSF in real-world predictive maintenance applications.\n","authors":["Yigitcan Yardımcı","Mustafa Cavus"],"pdf_url":"https://arxiv.org/pdf/2504.14744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14741v1","updated":"2025-04-20T21:07:59Z","published":"2025-04-20T21:07:59Z","title":"AltGDmin: Alternating GD and Minimization for Partly-Decoupled\n  (Federated) Optimization","summary":"  This article describes a novel optimization solution framework, called\nalternating gradient descent (GD) and minimization (AltGDmin), that is useful\nfor many problems for which alternating minimization (AltMin) is a popular\nsolution. AltMin is a special case of the block coordinate descent algorithm\nthat is useful for problems in which minimization w.r.t one subset of variables\nkeeping the other fixed is closed form or otherwise reliably solved. Denote the\ntwo blocks/subsets of the optimization variables Z by Za, Zb, i.e., Z = {Za,\nZb}. AltGDmin is often a faster solution than AltMin for any problem for which\n(i) the minimization over one set of variables, Zb, is much quicker than that\nover the other set, Za; and (ii) the cost function is differentiable w.r.t. Za.\nOften, the reason for one minimization to be quicker is that the problem is\n``decoupled\" for Zb and each of the decoupled problems is quick to solve. This\ndecoupling is also what makes AltGDmin communication-efficient for federated\nsettings.\n  Important examples where this assumption holds include (a) low rank\ncolumn-wise compressive sensing (LRCS), low rank matrix completion (LRMC), (b)\ntheir outlier-corrupted extensions such as robust PCA, robust LRCS and robust\nLRMC; (c) phase retrieval and its sparse and low-rank model based extensions;\n(d) tensor extensions of many of these problems such as tensor LRCS and tensor\ncompletion; and (e) many partly discrete problems where GD does not apply --\nsuch as clustering, unlabeled sensing, and mixed linear regression. LRCS finds\nimportant applications in multi-task representation learning and few shot\nlearning, federated sketching, and accelerated dynamic MRI. LRMC and robust PCA\nfind important applications in recommender systems, computer vision and video\nanalytics.\n","authors":["Namrata Vaswani"],"pdf_url":"https://arxiv.org/pdf/2504.14741v1.pdf","comment":"To appear in Foundations and Trends in Optimization (NOW publishers)"},{"id":"http://arxiv.org/abs/2411.14390v8","updated":"2025-04-20T20:55:16Z","published":"2024-11-21T18:24:06Z","title":"Persistent Homology for Structural Characterization in Disordered\n  Systems","summary":"  We propose a unified framework based on persistent homology (PH) to\ncharacterize both local and global structures in disordered systems. It can\nsimultaneously generate local and global descriptors using the same algorithm\nand data structure, and has shown to be highly effective and interpretable in\npredicting particle rearrangements and classifying global phases. We also\ndemonstrated that using a single variable enables a linear SVM to achieve\nnearly perfect three-phase classification. Inspired by this discovery, we\ndefine a non-parametric metric, the Separation Index (SI), which not only\nachieves this classification without sacrificing significant performance but\nalso establishes a connection between particle environments and the global\nphase structure. Our methods provide an effective framework for understanding\nand analyzing the properties of disordered materials, with broad potential\napplications in materials science and even wider studies of complex systems.\n","authors":["An Wang","Li Zou"],"pdf_url":"https://arxiv.org/pdf/2411.14390v8.pdf","comment":"24 pages, 19 figures"},{"id":"http://arxiv.org/abs/2410.09344v2","updated":"2025-04-20T20:53:39Z","published":"2024-10-12T03:21:58Z","title":"DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned\n  Models","summary":"  Storing open-source fine-tuned models separately introduces redundancy and\nincreases response times in applications utilizing multiple models.\nDelta-parameter pruning (DPP), particularly the random drop and rescale (DARE)\nmethod proposed by Yu et al., addresses this by pruning the majority of delta\nparameters--the differences between fine-tuned and pre-trained model\nweights--while typically maintaining minimal performance loss. However, DARE\nfails when either the pruning rate or the magnitude of the delta parameters is\nlarge. We highlight two key reasons for this failure: (1) an excessively large\nrescaling factor as pruning rates increase, and (2) high mean and variance in\nthe delta parameters. To push DARE's limits, we introduce DAREx (DARE the\neXtreme), which features two algorithmic improvements: (1) DAREx-q, a rescaling\nfactor modification that significantly boosts performance at high pruning rates\n(e.g., >30 % on COLA and SST2 for encoder models, with even greater gains in\ndecoder models), and (2) DAREx-L2, which combines DARE with AdamR, an\nin-training method that applies appropriate delta regularization before DPP. We\nalso demonstrate that DAREx-q can be seamlessly combined with vanilla\nparameter-efficient fine-tuning techniques like LoRA and can facilitate\nstructural DPP. Additionally, we revisit the application of importance-based\npruning techniques within DPP, demonstrating that they outperform random-based\nmethods when delta parameters are large. Through this comprehensive study, we\ndevelop a pipeline for selecting the most appropriate DPP method under various\npractical scenarios.\n","authors":["Wenlong Deng","Yize Zhao","Vala Vakilian","Minghui Chen","Xiaoxiao Li","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2410.09344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08201v3","updated":"2025-04-20T20:44:18Z","published":"2025-04-11T02:06:20Z","title":"Neural Encoding and Decoding at Scale","summary":"  Recent work has demonstrated that large-scale, multi-animal models are\npowerful tools for characterizing the relationship between neural activity and\nbehavior. Current large-scale approaches, however, focus exclusively on either\npredicting neural activity from behavior (encoding) or predicting behavior from\nneural activity (decoding), limiting their ability to capture the bidirectional\nrelationship between neural activity and behavior. To bridge this gap, we\nintroduce a multimodal, multi-task model that enables simultaneous Neural\nEncoding and Decoding at Scale (NEDS). Central to our approach is a novel\nmulti-task-masking strategy, which alternates between neural, behavioral,\nwithin-modality, and cross-modality masking. We pretrain our method on the\nInternational Brain Laboratory (IBL) repeated site dataset, which includes\nrecordings from 83 animals performing the same visual decision-making task. In\ncomparison to other large-scale models, we demonstrate that NEDS achieves\nstate-of-the-art performance for both encoding and decoding when pretrained on\nmulti-animal data and then fine-tuned on new animals. Surprisingly, NEDS's\nlearned embeddings exhibit emergent properties: even without explicit training,\nthey are highly predictive of the brain regions in each recording. Altogether,\nour approach is a step towards a foundation model of the brain that enables\nseamless translation between neural activity and behavior.\n","authors":["Yizi Zhang","Yanchen Wang","Mehdi Azabou","Alexandre Andre","Zixuan Wang","Hanrui Lyu","The International Brain Laboratory","Eva Dyer","Liam Paninski","Cole Hurwitz"],"pdf_url":"https://arxiv.org/pdf/2504.08201v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19625v2","updated":"2025-04-20T20:25:08Z","published":"2025-02-26T23:30:55Z","title":"Revealing Treatment Non-Adherence Bias in Clinical Machine Learning\n  Using Large Language Models","summary":"  Machine learning systems trained on electronic health records (EHRs)\nincreasingly guide treatment decisions, but their reliability depends on the\ncritical assumption that patients follow the prescribed treatments recorded in\nEHRs. Using EHR data from 3,623 hypertension patients, we investigate how\ntreatment non-adherence introduces implicit bias that can fundamentally distort\nboth causal inference and predictive modeling. By extracting patient adherence\ninformation from clinical notes using a large language model (LLM), we identify\n786 patients (21.7%) with medication non-adherence. We further uncover key\ndemographic and clinical factors associated with non-adherence, as well as\npatient-reported reasons including side effects and difficulties obtaining\nrefills. Our findings demonstrate that this implicit bias can not only reverse\nestimated treatment effects, but also degrade model performance by up to 5%\nwhile disproportionately affecting vulnerable populations by exacerbating\ndisparities in decision outcomes and model error rates. This highlights the\nimportance of accounting for treatment non-adherence in developing responsible\nand equitable clinical machine learning systems.\n","authors":["Zhongyuan Liang","Arvind Suresh","Irene Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2502.19625v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11566v3","updated":"2025-04-20T20:17:39Z","published":"2024-05-19T14:30:57Z","title":"Uncertainty-Aware PPG-2-ECG for Enhanced Cardiovascular Diagnosis using\n  Diffusion Models","summary":"  Analyzing the cardiovascular system condition via Electrocardiography (ECG)\nis a common and highly effective approach, and it has been practiced and\nperfected over many decades. ECG sensing is non-invasive and relatively easy to\nacquire, and yet it is still cumbersome for holter monitoring tests that may\nspan over hours and even days. A possible alternative in this context is\nPhotoplethysmography (PPG): An optically-based signal that measures blood\nvolume fluctuations, as typically sensed by conventional ``wearable devices''.\nWhile PPG presents clear advantages in acquisition, convenience, and\ncost-effectiveness, ECG provides more comprehensive information, allowing for a\nmore precise detection of heart conditions. This implies that a conversion from\nPPG to ECG, as recently discussed in the literature, inherently involves an\nunavoidable level of uncertainty. In this paper we introduce a novel\nmethodology for addressing the PPG-2-ECG conversion, and offer an enhanced\nclassification of cardiovascular conditions using the given PPG, all while\ntaking into account the uncertainties arising from the conversion process. We\nprovide a mathematical justification for our proposed computational approach,\nand present empirical studies demonstrating its superior performance compared\nto state-of-the-art baseline methods.\n","authors":["Omer Belhasin","Idan Kligvasser","George Leifman","Regev Cohen","Erin Rainaldi","Li-Fang Cheng","Nishant Verma","Paul Varghese","Ehud Rivlin","Michael Elad"],"pdf_url":"https://arxiv.org/pdf/2405.11566v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12714v2","updated":"2025-04-20T20:10:41Z","published":"2025-04-17T07:41:25Z","title":"Cross-environment Cooperation Enables Zero-shot Multi-agent Coordination","summary":"  Zero-shot coordination (ZSC), the ability to adapt to a new partner in a\ncooperative task, is a critical component of human-compatible AI. While prior\nwork has focused on training agents to cooperate on a single task, these\nspecialized models do not generalize to new tasks, even if they are highly\nsimilar. Here, we study how reinforcement learning on a distribution of\nenvironments with a single partner enables learning general cooperative skills\nthat support ZSC with many new partners on many new problems. We introduce two\nJax-based, procedural generators that create billions of solvable coordination\nchallenges. We develop a new paradigm called Cross-Environment Cooperation\n(CEC), and show that it outperforms competitive baselines quantitatively and\nqualitatively when collaborating with real people. Our findings suggest that\nlearning to collaborate across many unique scenarios encourages agents to\ndevelop general norms, which prove effective for collaboration with different\npartners. Together, our results suggest a new route toward designing generalist\ncooperative agents capable of interacting with humans without requiring human\ndata.\n","authors":["Kunal Jha","Wilka Carvalho","Yancheng Liang","Simon S. Du","Max Kleiman-Weiner","Natasha Jaques"],"pdf_url":"https://arxiv.org/pdf/2504.12714v2.pdf","comment":"Accepted to CogSci 2025, In-review for ICML 2025"},{"id":"http://arxiv.org/abs/2504.14732v1","updated":"2025-04-20T20:09:19Z","published":"2025-04-20T20:09:19Z","title":"Reinforcement Learning from Multi-level and Episodic Human Feedback","summary":"  Designing an effective reward function has long been a challenge in\nreinforcement learning, particularly for complex tasks in unstructured\nenvironments. To address this, various learning paradigms have emerged that\nleverage different forms of human input to specify or refine the reward\nfunction. Reinforcement learning from human feedback is a prominent approach\nthat utilizes human comparative feedback, expressed as a preference for one\nbehavior over another, to tackle this problem. In contrast to comparative\nfeedback, we explore multi-level human feedback, which is provided in the form\nof a score at the end of each episode. This type of feedback offers more coarse\nbut informative signals about the underlying reward function than binary\nfeedback. Additionally, it can handle non-Markovian rewards, as it is based on\nthe evaluation of an entire episode. We propose an algorithm to efficiently\nlearn both the reward function and the optimal policy from this form of\nfeedback. Moreover, we show that the proposed algorithm achieves sublinear\nregret and demonstrate its empirical effectiveness through extensive\nsimulations.\n","authors":["Muhammad Qasim Elahi","Somtochukwu Oguchienti","Maheed H. Ahmed","Mahsa Ghasemi"],"pdf_url":"https://arxiv.org/pdf/2504.14732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09775v3","updated":"2025-04-20T19:57:16Z","published":"2025-04-14T00:29:49Z","title":"Understanding and Optimizing Multi-Stage AI Inference Pipelines","summary":"  The rapid evolution of Large Language Models (LLMs) has driven the need for\nincreasingly sophisticated inference pipelines and hardware platforms. Modern\nLLM serving extends beyond traditional prefill-decode workflows, incorporating\nmulti-stage processes such as Retrieval Augmented Generation (RAG), key-value\n(KV) cache retrieval, dynamic model routing, and multi step reasoning. These\nstages exhibit diverse computational demands, requiring distributed systems\nthat integrate GPUs, ASICs, CPUs, and memory-centric architectures. However,\nexisting simulators lack the fidelity to model these heterogeneous,\nmulti-engine workflows, limiting their ability to inform architectural\ndecisions.\n  To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM\ninference Execution Simulator. HERMES models diverse request stages; including\nRAG, KV retrieval, reasoning, prefill, and decode across complex hardware\nhierarchies. HERMES supports heterogeneous clients executing multiple models\nconcurrently unlike prior frameworks while incorporating advanced batching\nstrategies and multi-level memory hierarchies. By integrating real hardware\ntraces with analytical modeling, HERMES captures critical trade-offs such as\nmemory bandwidth contention, inter-cluster communication latency, and batching\nefficiency in hybrid CPU-accelerator deployments. Through case studies, we\nexplore the impact of reasoning stages on end-to-end latency, optimal batching\nstrategies for hybrid pipelines, and the architectural implications of remote\nKV cache retrieval. HERMES empowers system designers to navigate the evolving\nlandscape of LLM inference, providing actionable insights into optimizing\nhardware-software co-design for next-generation AI workloads.\n","authors":["Abhimanyu Rajeshkumar Bambhaniya","Hanjiang Wu","Suvinay Subramanian","Sudarshan Srinivasan","Souvik Kundu","Amir Yazdanbakhsh","Midhilesh Elavazhagan","Madhu Kumar","Tushar Krishna"],"pdf_url":"https://arxiv.org/pdf/2504.09775v3.pdf","comment":"Inference System Design for Multi-Stage AI Inference Pipelines. 13\n  Pages, 15 Figues, 3 Tables"},{"id":"http://arxiv.org/abs/2504.14728v1","updated":"2025-04-20T19:56:41Z","published":"2025-04-20T19:56:41Z","title":"Geometric Learning Dynamics","summary":"  We present a unified geometric framework for modeling learning dynamics in\nphysical, biological, and machine learning systems. The theory reveals three\nfundamental regimes, each emerging from the power-law relationship $g \\propto\n\\kappa^a$ between the metric tensor $g$ in the space of trainable variables and\nthe noise covariance matrix $\\kappa$. The quantum regime corresponds to $a = 1$\nand describes Schr\\\"odinger-like dynamics that emerges from a discrete shift\nsymmetry. The efficient learning regime corresponds to $a = \\tfrac{1}{2}$ and\ndescribes very fast machine learning algorithms. The equilibration regime\ncorresponds to $a = 0$ and describes classical models of biological evolution.\nWe argue that the emergence of the intermediate regime $a = \\tfrac{1}{2}$ is a\nkey mechanism underlying the emergence of biological complexity.\n","authors":["Vitaly Vanchurin"],"pdf_url":"https://arxiv.org/pdf/2504.14728v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2504.14727v1","updated":"2025-04-20T19:53:13Z","published":"2025-04-20T19:53:13Z","title":"Semi-parametric Memory Consolidation: Towards Brain-like Deep Continual\n  Learning","summary":"  Humans and most animals inherently possess a distinctive capacity to\ncontinually acquire novel experiences and accumulate worldly knowledge over\ntime. This ability, termed continual learning, is also critical for deep neural\nnetworks (DNNs) to adapt to the dynamically evolving world in open\nenvironments. However, DNNs notoriously suffer from catastrophic forgetting of\npreviously learned knowledge when trained on sequential tasks. In this work,\ninspired by the interactive human memory and learning system, we propose a\nnovel biomimetic continual learning framework that integrates semi-parametric\nmemory and the wake-sleep consolidation mechanism. For the first time, our\nmethod enables deep neural networks to retain high performance on novel tasks\nwhile maintaining prior knowledge in real-world challenging continual learning\nscenarios, e.g., class-incremental learning on ImageNet. This study\ndemonstrates that emulating biological intelligence provides a promising path\nto enable deep neural networks with continual learning capabilities.\n","authors":["Geng Liu","Fei Zhu","Rong Feng","Zhiqiang Yi","Shiqi Wang","Gaofeng Meng","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.14727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.17211v2","updated":"2025-04-20T19:49:45Z","published":"2025-03-21T15:20:28Z","title":"A Language Anchor-Guided Method for Robust Noisy Domain Generalization","summary":"  Real-world machine learning applications often struggle with two major\nchallenges: distribution shift and label noise. Models tend to overfit by\nfocusing on redundant and uninformative features in the training data, which\nmakes it hard for them to generalize to the target domain. Noisy data worsens\nthis problem by causing further overfitting to the noise, meaning that existing\nmethods often fail to tell the difference between true, invariant features and\nmisleading, spurious ones. To tackle these issues, we introduce Anchor\nAlignment and Adaptive Weighting (A3W). This new algorithm uses sample\nreweighting guided by natural language processing (NLP) anchors to extract more\nrepresentative features. In simple terms, A3W leverages semantic\nrepresentations from natural language models as a source of domain-invariant\nprior knowledge. Additionally, it employs a weighted loss function that adjusts\neach sample's contribution based on its similarity to the corresponding NLP\nanchor. This adjustment makes the model more robust to noisy labels. Extensive\nexperiments on standard benchmark datasets show that A3W consistently\noutperforms state-of-the-art domain generalization methods, offering\nsignificant improvements in both accuracy and robustness across different\ndatasets and noise levels.\n","authors":["Zilin Dai","Lehong Wang","Fangzhou Lin","Yidong Wang","Zhigang Li","Kazunori D Yamada","Ziming Zhang","Wang Lu"],"pdf_url":"https://arxiv.org/pdf/2503.17211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.05967v7","updated":"2025-04-20T19:42:23Z","published":"2024-02-05T18:00:07Z","title":"The last Dance : Robust backdoor attack via diffusion models and\n  bayesian approach","summary":"  Diffusion models are state-of-the-art deep learning generative models that\nare trained on the principle of learning forward and backward diffusion\nprocesses via the progressive addition of noise and denoising. In this paper,\nwe aim to fool audio-based DNN models, such as those from the Hugging Face\nframework, primarily those that focus on audio, in particular transformer-based\nartificial intelligence models, which are powerful machine learning models that\nsave time and achieve results faster and more efficiently. We demonstrate the\nfeasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers\nderived from Hugging Face, a popular framework in the world of artificial\nintelligence research. The backdoor attack developed in this paper is based on\npoisoning model training data uniquely by incorporating backdoor diffusion\nsampling and a Bayesian approach to the distribution of poisoned data.\n","authors":["Orson Mengara"],"pdf_url":"https://arxiv.org/pdf/2402.05967v7.pdf","comment":"Preprint (Last update, will never be modified again( correction of a\n  sketch)): audio backdoor attack on Hugging Face's Transformer pre-trained\n  models. This attack incorporates state-of-the-art Bayesian techniques, a\n  modified Fokker-Planck equation (via Yang-Mills), and a diffusion model\n  approach"},{"id":"http://arxiv.org/abs/2504.14720v1","updated":"2025-04-20T19:18:13Z","published":"2025-04-20T19:18:13Z","title":"Video QoE Metrics from Encrypted Traffic: Application-agnostic\n  Methodology","summary":"  Instant Messaging-Based Video Call Applications (IMVCAs) and Video\nConferencing Applications (VCAs) have become integral to modern communication.\nEnsuring a high Quality of Experience (QoE) for users in this context is\ncritical for network operators, as network conditions significantly impact user\nQoE. However, network operators lack access to end-device QoE metrics due to\nencrypted traffic. Existing solutions estimate QoE metrics from encrypted\ntraffic traversing the network, with the most advanced approaches leveraging\nmachine learning models. Subsequently, the need for ground truth QoE metrics\nfor training and validation poses a challenge, as not all video applications\nprovide these metrics. To address this challenge, we propose an\napplication-agnostic approach for objective QoE estimation from encrypted\ntraffic. Independent of the video application, we obtained key video QoE\nmetrics, enabling broad applicability to various proprietary IMVCAs and VCAs.\nTo validate our solution, we created a diverse dataset from WhatsApp video\nsessions under various network conditions, comprising 25,680 seconds of traffic\ndata and QoE metrics. Our evaluation shows high performance across the entire\ndataset, with 85.2% accuracy for FPS predictions within an error margin of two\nFPS, and 90.2% accuracy for PIQE-based quality rating classification.\n","authors":["Tamir Berger","Jonathan Sterenson","Raz Birman","Ofer Hadar"],"pdf_url":"https://arxiv.org/pdf/2504.14720v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.14717v1","updated":"2025-04-20T19:09:43Z","published":"2025-04-20T19:09:43Z","title":"TAPIP3D: Tracking Any Point in Persistent 3D Geometry","summary":"  We introduce TAPIP3D, a novel approach for long-term 3D point tracking in\nmonocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized\nspatio-temporal feature clouds, leveraging depth and camera motion information\nto lift 2D video features into a 3D world space where camera motion is\neffectively canceled. TAPIP3D iteratively refines multi-frame 3D motion\nestimates within this stabilized representation, enabling robust tracking over\nextended periods. To manage the inherent irregularities of 3D point\ndistributions, we propose a Local Pair Attention mechanism. This 3D\ncontextualization strategy effectively exploits spatial relationships in 3D,\nforming informative feature neighborhoods for precise 3D trajectory estimation.\nOur 3D-centric approach significantly outperforms existing 3D point tracking\nmethods and even enhances 2D tracking accuracy compared to conventional 2D\npixel trackers when accurate depth is available. It supports inference in both\ncamera coordinates (i.e., unstabilized) and world coordinates, and our results\ndemonstrate that compensating for camera motion improves tracking performance.\nOur approach replaces the conventional 2D square correlation neighborhoods used\nin prior 2D and 3D trackers, leading to more robust and accurate results across\nvarious 3D point tracking benchmarks. Project Page: https://tapip3d.github.io\n","authors":["Bowei Zhang","Lei Ke","Adam W. Harley","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2504.14717v1.pdf","comment":"Long-term feed-forward 3D point tracking in persistent 3D point maps.\n  Code:https://github.com/zbw001/TAPIP3D"},{"id":"http://arxiv.org/abs/2504.14716v1","updated":"2025-04-20T19:05:59Z","published":"2025-04-20T19:05:59Z","title":"Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in\n  LLM-Based Evaluation","summary":"  Large Language Models (LLMs) are widely used as proxies for human labelers in\nboth training (Reinforcement Learning from AI Feedback) and large-scale\nresponse evaluation (LLM-as-a-judge). Alignment and evaluation are critical\ncomponents in the development of reliable LLMs, and the choice of feedback\nprotocol plays a central role in both but remains understudied. In this work,\nwe show that the choice of feedback protocol (absolute scores versus relative\npreferences) can significantly affect evaluation reliability and induce\nsystematic biases. In particular, we show that pairwise evaluation protocols\nare more vulnerable to distracted evaluation. Generator models can exploit\nspurious attributes (or distractor features) favored by the LLM judge,\nresulting in inflated scores for lower-quality outputs and misleading training\nsignals. We find that absolute scoring is more robust to such manipulation,\nproducing judgments that better reflect response quality and are less\ninfluenced by distractor features. Our results demonstrate that generator\nmodels can flip preferences by embedding distractor features, skewing\nLLM-as-a-judge comparisons and leading to inaccurate conclusions about model\nquality in benchmark evaluations. Pairwise preferences flip in about 35% of the\ncases, compared to only 9% for absolute scores. We offer recommendations for\nchoosing feedback protocols based on dataset characteristics and evaluation\nobjectives.\n","authors":["Tuhina Tripathi","Manya Wadhwa","Greg Durrett","Scott Niekum"],"pdf_url":"https://arxiv.org/pdf/2504.14716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14708v1","updated":"2025-04-20T18:51:10Z","published":"2025-04-20T18:51:10Z","title":"Time Frequency Analysis of EMG Signal for Gesture Recognition using Fine\n  grained Features","summary":"  Electromyography (EMG) based hand gesture recognition converts forearm muscle\nactivity into control commands for prosthetics, rehabilitation, and human\ncomputer interaction. This paper proposes a novel approach to EMG-based hand\ngesture recognition that uses fine-grained classification and presents XMANet,\nwhich unifies low-level local and high level semantic cues through cross layer\nmutual attention among shallow to deep CNN experts. Using stacked spectrograms\nand scalograms derived from the Short Time Fourier Transform (STFT) and Wavelet\nTransform (WT), we benchmark XMANet against ResNet50, DenseNet-121,\nMobileNetV3, and EfficientNetB0. Experimental results on the Grabmyo dataset\nindicate that, using STFT, the proposed XMANet model outperforms the baseline\nResNet50, EfficientNetB0, MobileNetV3, and DenseNet121 models with improvement\nof approximately 1.72%, 4.38%, 5.10%, and 2.53%, respectively. When employing\nthe WT approach, improvements of around 1.57%, 1.88%, 1.46%, and 2.05% are\nobserved over the same baselines. Similarly, on the FORS EMG dataset, the\nXMANet(ResNet50) model using STFT shows an improvement of about 5.04% over the\nbaseline ResNet50. In comparison, the XMANet(DenseNet121) and\nXMANet(MobileNetV3) models yield enhancements of approximately 4.11% and 2.81%,\nrespectively. Moreover, when using WT, the proposed XMANet achieves gains of\naround 4.26%, 9.36%, 5.72%, and 6.09% over the baseline ResNet50, DenseNet121,\nMobileNetV3, and EfficientNetB0 models, respectively. These results confirm\nthat XMANet consistently improves performance across various architectures and\nsignal processing techniques, demonstrating the strong potential of fine\ngrained features for accurate and robust EMG classification.\n","authors":["Parshuram N. Aarotale","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2504.14708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.09542v2","updated":"2025-04-20T18:49:55Z","published":"2024-09-14T21:42:38Z","title":"MANGO: Learning Disentangled Image Transformation Manifolds with Grouped\n  Operators","summary":"  Learning semantically meaningful image transformations (i.e. rotation,\nthickness, blur) directly from examples can be a challenging task. Recently,\nthe Manifold Autoencoder (MAE) proposed using a set of Lie group operators to\nlearn image transformations directly from examples. However, this approach has\nlimitations, as the learned operators are not guaranteed to be disentangled and\nthe training routine is prohibitively expensive when scaling up the model. To\naddress these limitations, we propose MANGO (transformation Manifolds with\nGrouped Operators) for learning disentangled operators that describe image\ntransformations in distinct latent subspaces. Moreover, our approach allows\npractitioners the ability to define which transformations they aim to model,\nthus improving the semantic meaning of the learned operators. Through our\nexperiments, we demonstrate that MANGO enables composition of image\ntransformations and introduces a one-phase training routine that leads to a\n100x speedup over prior works.\n","authors":["Brighton Ancelin","Yenho Chen","Peimeng Guan","Chiraag Kaushik","Belen Martin-Urcelay","Alex Saad-Falcon","Nakul Singh"],"pdf_url":"https://arxiv.org/pdf/2409.09542v2.pdf","comment":"Submitted to SampTA 2025. This work has been submitted to the IEEE\n  for possible publication"},{"id":"http://arxiv.org/abs/2504.14704v1","updated":"2025-04-20T18:37:51Z","published":"2025-04-20T18:37:51Z","title":"Can We Ignore Labels In Out of Distribution Detection?","summary":"  Out-of-distribution (OOD) detection methods have recently become more\nprominent, serving as a core element in safety-critical autonomous systems. One\nmajor purpose of OOD detection is to reject invalid inputs that could lead to\nunpredictable errors and compromise safety. Due to the cost of labeled data,\nrecent works have investigated the feasibility of self-supervised learning\n(SSL) OOD detection, unlabeled OOD detection, and zero shot OOD detection. In\nthis work, we identify a set of conditions for a theoretical guarantee of\nfailure in unlabeled OOD detection algorithms from an information-theoretic\nperspective. These conditions are present in all OOD tasks dealing with\nreal-world data: I) we provide theoretical proof of unlabeled OOD detection\nfailure when there exists zero mutual information between the learning\nobjective and the in-distribution labels, a.k.a. 'label blindness', II) we\ndefine a new OOD task - Adjacent OOD detection - that tests for label blindness\nand accounts for a previously ignored safety gap in all OOD detection\nbenchmarks, and III) we perform experiments demonstrating that existing\nunlabeled OOD methods fail under conditions suggested by our label blindness\ntheory and analyze the implications for future research in unlabeled OOD\nmethods.\n","authors":["Hong Yang","Qi Yu","Travis Desel"],"pdf_url":"https://arxiv.org/pdf/2504.14704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14701v1","updated":"2025-04-20T18:29:39Z","published":"2025-04-20T18:29:39Z","title":"Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using\n  Sketched Methods","summary":"  Recently, it has been observed that when training a deep neural net with SGD,\nthe majority of the loss landscape's curvature quickly concentrates in a tiny\n*top* eigenspace of the loss Hessian, which remains largely stable thereafter.\nIndependently, it has been shown that successful magnitude pruning masks for\ndeep neural nets emerge early in training and remain stable thereafter. In this\nwork, we study these two phenomena jointly and show that they are connected: We\ndevelop a methodology to measure the similarity between arbitrary parameter\nmasks and Hessian eigenspaces via Grassmannian metrics. We identify *overlap*\nas the most useful such metric due to its interpretability and stability. To\ncompute *overlap*, we develop a matrix-free algorithm based on sketched SVDs\nthat allows us to compute over 1000 Hessian eigenpairs for nets with over 10M\nparameters --an unprecedented scale by several orders of magnitude. Our\nexperiments reveal an *overlap* between magnitude parameter masks and top\nHessian eigenspaces consistently higher than chance-level, and that this effect\ngets accentuated for larger network sizes. This result indicates that *top\nHessian eigenvectors tend to be concentrated around larger parameters*, or\nequivalently, that *larger parameters tend to align with directions of larger\nloss curvature*. Our work provides a methodology to approximate and analyze\ndeep learning Hessians at scale, as well as a novel insight on the structure of\ntheir eigenspace.\n","authors":["Andres Fernandez","Frank Schneider","Maren Mahsereci","Philipp Hennig"],"pdf_url":"https://arxiv.org/pdf/2504.14701v1.pdf","comment":"Accepted at TMLR 2025"},{"id":"http://arxiv.org/abs/2504.14697v1","updated":"2025-04-20T18:21:34Z","published":"2025-04-20T18:21:34Z","title":"Quantitative Clustering in Mean-Field Transformer Models","summary":"  The evolution of tokens through a deep transformer models can be modeled as\nan interacting particle system that has been shown to exhibit an asymptotic\nclustering behavior akin to the synchronization phenomenon in Kuramoto models.\nIn this work, we investigate the long-time clustering of mean-field transformer\nmodels. More precisely, we establish exponential rates of contraction to a\nDirac point mass for any suitably regular initialization under some assumptions\non the parameters of transformer models, any suitably regular mean-field\ninitialization synchronizes exponentially fast with some quantitative rates.\n","authors":["Shi Chen","Zhengjiang Lin","Yury Polyanskiy","Philippe Rigollet"],"pdf_url":"https://arxiv.org/pdf/2504.14697v1.pdf","comment":"47 pages, 4 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2504.14720v1","updated":"2025-04-20T19:18:13Z","published":"2025-04-20T19:18:13Z","title":"Video QoE Metrics from Encrypted Traffic: Application-agnostic\n  Methodology","summary":"  Instant Messaging-Based Video Call Applications (IMVCAs) and Video\nConferencing Applications (VCAs) have become integral to modern communication.\nEnsuring a high Quality of Experience (QoE) for users in this context is\ncritical for network operators, as network conditions significantly impact user\nQoE. However, network operators lack access to end-device QoE metrics due to\nencrypted traffic. Existing solutions estimate QoE metrics from encrypted\ntraffic traversing the network, with the most advanced approaches leveraging\nmachine learning models. Subsequently, the need for ground truth QoE metrics\nfor training and validation poses a challenge, as not all video applications\nprovide these metrics. To address this challenge, we propose an\napplication-agnostic approach for objective QoE estimation from encrypted\ntraffic. Independent of the video application, we obtained key video QoE\nmetrics, enabling broad applicability to various proprietary IMVCAs and VCAs.\nTo validate our solution, we created a diverse dataset from WhatsApp video\nsessions under various network conditions, comprising 25,680 seconds of traffic\ndata and QoE metrics. Our evaluation shows high performance across the entire\ndataset, with 85.2% accuracy for FPS predictions within an error margin of two\nFPS, and 90.2% accuracy for PIQE-based quality rating classification.\n","authors":["Tamir Berger","Jonathan Sterenson","Raz Birman","Ofer Hadar"],"pdf_url":"https://arxiv.org/pdf/2504.14720v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.00837v2","updated":"2025-04-20T12:55:44Z","published":"2025-04-01T14:26:25Z","title":"A Survey on Music Generation from Single-Modal, Cross-Modal, and\n  Multi-Modal Perspectives","summary":"  Multi-modal music generation, using multiple modalities like text, images,\nand video alongside musical scores and audio as guidance, is an emerging\nresearch area with broad applications. This paper reviews this field,\ncategorizing music generation systems from the perspective of modalities. The\nreview covers modality representation, multi-modal data alignment, and their\nutilization to guide music generation. Current datasets and evaluation methods\nare also discussed. Key challenges in this area include effective multi-modal\nintegration, large-scale comprehensive datasets, and systematic evaluation\nmethods. Finally, an outlook on future research directions is provided,\nfocusing on creativity, efficiency, multi-modal alignment, and evaluation.\n","authors":["Shuyu Li","Shulei Ji","Zihao Wang","Songruoyao Wu","Jiaxing Yu","Kejun Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.00837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18547v2","updated":"2025-04-20T02:50:48Z","published":"2025-02-25T15:56:09Z","title":"Steganography Beyond Space-Time with Chain of Multimodal AI","summary":"  Steganography is the art and science of covert writing, with a broad range of\napplications interwoven within the realm of cybersecurity. As artificial\nintelligence continues to evolve, its ability to synthesise realistic content\nemerges as a threat in the hands of cybercriminals who seek to manipulate and\nmisrepresent the truth. Such synthetic content introduces a non-trivial risk of\noverwriting the subtle changes made for the purpose of steganography. When the\nsignals in both the spatial and temporal domains are vulnerable to unforeseen\noverwriting, it calls for reflection on what, if any, remains invariant. This\nstudy proposes a paradigm in steganography for audiovisual media, where\nmessages are concealed beyond both spatial and temporal domains. A chain of\nmultimodal artificial intelligence is developed to deconstruct audiovisual\ncontent into a cover text, embed a message within the linguistic domain, and\nthen reconstruct the audiovisual content through synchronising both auditory\nand visual modalities with the resultant stego text. The message is encoded by\nbiasing the word sampling process of a language generation model and decoded by\nanalysing the probability distribution of word choices. The accuracy of message\ntransmission is evaluated under both zero-bit and multi-bit capacity settings.\nFidelity is assessed through both biometric and semantic similarities,\ncapturing the identities of the recorded face and voice, as well as the core\nideas conveyed through the media. Secrecy is examined through statistical\ncomparisons between cover and stego texts. Robustness is tested across various\nscenarios, including audiovisual resampling, face-swapping, voice-cloning and\ntheir combinations.\n","authors":["Ching-Chun Chang","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2502.18547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.10442v2","updated":"2025-04-20T02:17:12Z","published":"2024-12-11T12:02:36Z","title":"Steganography in Game Actions","summary":"  The exchange of messages has always carried with it the timeless challenge of\nsecrecy. From whispers in shadows to the enigmatic notes written in the margins\nof history, humanity has long sought ways to convey thoughts that remain\nimperceptible to all but the chosen few. The challenge of subliminal\ncommunication has been addressed in various forms of steganography. However,\nthe field faces a fundamental paradox: as the art of concealment advances, so\ntoo does the science of revelation, leading to an ongoing evolutionary\ninterplay. This study seeks to extend the boundaries of what is considered a\nviable steganographic medium. We explore a steganographic paradigm, in which\nhidden information is communicated through the episodes of multiple agents\ninteracting with an environment. Each agent, acting as an encoder, learns a\npolicy to disguise the very existence of hidden messages within actions\nseemingly directed toward innocent objectives. Meanwhile, an observer, serving\nas a decoder, learns to associate behavioural patterns with their respective\nagents despite their dynamic nature, thereby unveiling the hidden messages. The\ninteractions of agents are governed by the framework of multi-agent\nreinforcement learning and shaped by feedback from the observer. This framework\nencapsulates a game-theoretic dilemma, wherein agents face decisions between\ncooperating to create distinguishable behavioural patterns or defecting to\npursue individually optimal yet potentially overlapping episodic actions. As a\nproof of concept, we exemplify action steganography through the game of\nlabyrinth, a navigation task where subliminal communication is concealed within\nthe act of steering toward a destination, and systematically validate the\nstego-system in terms of distortion, capacity, secrecy and robustness when\nsubjected to simulated passive and active adversaries.\n","authors":["Ching-Chun Chang","Isao Echizen"],"pdf_url":"https://arxiv.org/pdf/2412.10442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21088v2","updated":"2025-04-20T00:37:24Z","published":"2025-03-27T02:03:25Z","title":"ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging","summary":"  This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:\nUnlearning Sensitive Content from Large Language Models. This task aims to\nselectively erase sensitive knowledge from large language models, avoiding both\nover-forgetting and under-forgetting issues. We propose an unlearning system\nthat leverages Model Merging (specifically TIES-Merging), combining two\nspecialized models into a more balanced unlearned model. Our system achieves\ncompetitive results, ranking second among 26 teams, with an online score of\n0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, we\nalso conduct local experiments and perform a comprehensive analysis of the\nunlearning process, examining performance trajectories, loss dynamics, and\nweight perspectives, along with several supplementary experiments, to\nunderstand the effectiveness of our method. Furthermore, we analyze the\nshortcomings of our method and evaluation metrics, emphasizing that MIA scores\nand ROUGE-based metrics alone are insufficient to fully evaluate successful\nunlearning. Finally, we emphasize the need for more comprehensive evaluation\nmethodologies and rethinking of unlearning objectives in future research. Code\nis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.\n","authors":["Haoming Xu","Shuxun Wang","Yanqiu Zhao","Yi Zhong","Ziyan Jiang","Ningyuan Zhao","Shumin Deng","Huajun Chen","Ningyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.21088v2.pdf","comment":"SemEval@ACL 2025"}]},"2025-04-19T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.12851v7","updated":"2025-04-19T23:14:54Z","published":"2024-10-10T17:59:17Z","title":"VibeCheck: Discover and Quantify Qualitative Differences in Large\n  Language Models","summary":"  Large language models (LLMs) often exhibit subtle yet distinctive\ncharacteristics in their outputs that users intuitively recognize, but struggle\nto quantify. These \"vibes\" -- such as tone, formatting, or writing style --\ninfluence user preferences, yet traditional evaluations focus primarily on the\nsingular axis of correctness. We introduce VibeCheck, a system for\nautomatically comparing a pair of LLMs by discovering identifying traits of a\nmodel (vibes) that are well-defined, differentiating, and user-aligned.\nVibeCheck iteratively discovers vibes from model outputs and then utilizes a\npanel of LLM judges to quantitatively measure the utility of each vibe. We\nvalidate that the vibes generated by VibeCheck align with those found in human\ndiscovery and run VibeCheck on pairwise preference data from real-world user\nconversations with Llama-3-70b vs GPT-4. VibeCheck reveals that Llama has a\nfriendly, funny, and somewhat controversial vibe. These vibes predict model\nidentity with 80% accuracy and human preference with 61% accuracy. Lastly, we\nrun VibeCheck on a variety of models and tasks including summarization, math,\nand captioning to provide insight into differences in model behavior. VibeCheck\ndiscovers vibes like Command X prefers to add concrete intros and conclusions\nwhen summarizing in comparison to TNGL, Llama-405b often overexplains its\nthought process on math problems compared to GPT-4o, and GPT-4 prefers to focus\non the mood and emotions of the scene when captioning compared to\nGemini-1.5-Flash. Code and vibe visualizer found at https://bench-mark.org/\n","authors":["Lisa Dunlap","Krishna Mandal","Trevor Darrell","Jacob Steinhardt","Joseph E Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2410.12851v7.pdf","comment":"unironic use of the word 'vibe', added more analysis and cooler\n  graphs. added website link"},{"id":"http://arxiv.org/abs/2501.14249v7","updated":"2025-04-19T21:49:12Z","published":"2025-01-24T05:27:46Z","title":"Humanity's Last Exam","summary":"  Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,500\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.\n","authors":["Long Phan","Alice Gatti","Ziwen Han","Nathaniel Li","Josephina Hu","Hugh Zhang","Chen Bo Calvin Zhang","Mohamed Shaaban","John Ling","Sean Shi","Michael Choi","Anish Agrawal","Arnav Chopra","Adam Khoja","Ryan Kim","Richard Ren","Jason Hausenloy","Oliver Zhang","Mantas Mazeika","Dmitry Dodonov","Tung Nguyen","Jaeho Lee","Daron Anderson","Mikhail Doroshenko","Alun Cennyth Stokes","Mobeen Mahmood","Oleksandr Pokutnyi","Oleg Iskra","Jessica P. Wang","John-Clark Levin","Mstyslav Kazakov","Fiona Feng","Steven Y. Feng","Haoran Zhao","Michael Yu","Varun Gangal","Chelsea Zou","Zihan Wang","Serguei Popov","Robert Gerbicz","Geoff Galgon","Johannes Schmitt","Will Yeadon","Yongki Lee","Scott Sauers","Alvaro Sanchez","Fabian Giska","Marc Roth","Søren Riis","Saiteja Utpala","Noah Burns","Gashaw M. Goshu","Mohinder Maheshbhai Naiya","Chidozie Agu","Zachary Giboney","Antrell Cheatom","Francesco Fournier-Facio","Sarah-Jane Crowson","Lennart Finke","Zerui Cheng","Jennifer Zampese","Ryan G. Hoerr","Mark Nandor","Hyunwoo Park","Tim Gehrunger","Jiaqi Cai","Ben McCarty","Alexis C Garretson","Edwin Taylor","Damien Sileo","Qiuyu Ren","Usman Qazi","Lianghui Li","Jungbae Nam","John B. Wydallis","Pavel Arkhipov","Jack Wei Lun Shi","Aras Bacho","Chris G. Willcocks","Hangrui Cao","Sumeet Motwani","Emily de Oliveira Santos","Johannes Veith","Edward Vendrow","Doru Cojoc","Kengo Zenitani","Joshua Robinson","Longke Tang","Yuqi Li","Joshua Vendrow","Natanael Wildner Fraga","Vladyslav Kuchkin","Andrey Pupasov Maksimov","Pierre Marion","Denis Efremov","Jayson Lynch","Kaiqu Liang","Aleksandar Mikov","Andrew Gritsevskiy","Julien Guillod","Gözdenur Demir","Dakotah Martinez","Ben Pageler","Kevin Zhou","Saeed Soori","Ori Press","Henry Tang","Paolo Rissone","Sean R. Green","Lina Brüssel","Moon Twayana","Aymeric Dieuleveut","Joseph Marvin Imperial","Ameya Prabhu","Jinzhou Yang","Nick Crispino","Arun Rao","Dimitri Zvonkine","Gabriel Loiseau","Mikhail Kalinin","Marco Lukas","Ciprian Manolescu","Nate Stambaugh","Subrata Mishra","Tad Hogg","Carlo Bosio","Brian P Coppola","Julian Salazar","Jaehyeok Jin","Rafael Sayous","Stefan Ivanov","Philippe Schwaller","Shaipranesh Senthilkuma","Andres M Bran","Andres Algaba","Kelsey Van den Houte","Lynn Van Der Sypt","Brecht Verbeken","David Noever","Alexei Kopylov","Benjamin Myklebust","Bikun Li","Lisa Schut","Evgenii Zheltonozhskii","Qiaochu Yuan","Derek Lim","Richard Stanley","Tong Yang","John Maar","Julian Wykowski","Martí Oller","Anmol Sahu","Cesare Giulio Ardito","Yuzheng Hu","Ariel Ghislain Kemogne Kamdoum","Alvin Jin","Tobias Garcia Vilchis","Yuexuan Zu","Martin Lackner","James Koppel","Gongbo Sun","Daniil S. Antonenko","Steffi Chern","Bingchen Zhao","Pierrot Arsene","Joseph M Cavanagh","Daofeng Li","Jiawei Shen","Donato Crisostomi","Wenjin Zhang","Ali Dehghan","Sergey Ivanov","David Perrella","Nurdin Kaparov","Allen Zang","Ilia Sucholutsky","Arina Kharlamova","Daniil Orel","Vladislav Poritski","Shalev Ben-David","Zachary Berger","Parker Whitfill","Michael Foster","Daniel Munro","Linh Ho","Shankar Sivarajan","Dan Bar Hava","Aleksey Kuchkin","David Holmes","Alexandra Rodriguez-Romero","Frank Sommerhage","Anji Zhang","Richard Moat","Keith Schneider","Zakayo Kazibwe","Don Clarke","Dae Hyun Kim","Felipe Meneguitti Dias","Sara Fish","Veit Elser","Tobias Kreiman","Victor Efren Guadarrama Vilchis","Immo Klose","Ujjwala Anantheswaran","Adam Zweiger","Kaivalya Rawal","Jeffery Li","Jeremy Nguyen","Nicolas Daans","Haline Heidinger","Maksim Radionov","Václav Rozhoň","Vincent Ginis","Christian Stump","Niv Cohen","Rafał Poświata","Josef Tkadlec","Alan Goldfarb","Chenguang Wang","Piotr Padlewski","Stanislaw Barzowski","Kyle Montgomery","Ryan Stendall","Jamie Tucker-Foltz","Jack Stade","T. Ryan Rogers","Tom Goertzen","Declan Grabb","Abhishek Shukla","Alan Givré","John Arnold Ambay","Archan Sen","Muhammad Fayez Aziz","Mark H Inlow","Hao He","Ling Zhang","Younesse Kaddar","Ivar Ängquist","Yanxu Chen","Harrison K Wang","Kalyan Ramakrishnan","Elliott Thornley","Antonio Terpin","Hailey Schoelkopf","Eric Zheng","Avishy Carmi","Ethan D. L. Brown","Kelin Zhu","Max Bartolo","Richard Wheeler","Martin Stehberger","Peter Bradshaw","JP Heimonen","Kaustubh Sridhar","Ido Akov","Jennifer Sandlin","Yury Makarychev","Joanna Tam","Hieu Hoang","David M. Cunningham","Vladimir Goryachev","Demosthenes Patramanis","Michael Krause","Andrew Redenti","David Aldous","Jesyin Lai","Shannon Coleman","Jiangnan Xu","Sangwon Lee","Ilias Magoulas","Sandy Zhao","Ning Tang","Michael K. Cohen","Orr Paradise","Jan Hendrik Kirchner","Maksym Ovchynnikov","Jason O. Matos","Adithya Shenoy","Michael Wang","Yuzhou Nie","Anna Sztyber-Betley","Paolo Faraboschi","Robin Riblet","Jonathan Crozier","Shiv Halasyamani","Shreyas Verma","Prashant Joshi","Eli Meril","Ziqiao Ma","Jérémy Andréoletti","Raghav Singhal","Jacob Platnick","Volodymyr Nevirkovets","Luke Basler","Alexander Ivanov","Seri Khoury","Nils Gustafsson","Marco Piccardo","Hamid Mostaghimi","Qijia Chen","Virendra Singh","Tran Quoc Khánh","Paul Rosu","Hannah Szlyk","Zachary Brown","Himanshu Narayan","Aline Menezes","Jonathan Roberts","William Alley","Kunyang Sun","Arkil Patel","Max Lamparth","Anka Reuel","Linwei Xin","Hanmeng Xu","Jacob Loader","Freddie Martin","Zixuan Wang","Andrea Achilleos","Thomas Preu","Tomek Korbak","Ida Bosio","Fereshteh Kazemi","Ziye Chen","Biró Bálint","Eve J. Y. Lo","Jiaqi Wang","Maria Inês S. Nunes","Jeremiah Milbauer","M Saiful Bari","Zihao Wang","Behzad Ansarinejad","Yewen Sun","Stephane Durand","Hossam Elgnainy","Guillaume Douville","Daniel Tordera","George Balabanian","Hew Wolff","Lynna Kvistad","Hsiaoyun Milliron","Ahmad Sakor","Murat Eron","Andrew Favre D. O.","Shailesh Shah","Xiaoxiang Zhou","Firuz Kamalov","Sherwin Abdoli","Tim Santens","Shaul Barkan","Allison Tee","Robin Zhang","Alessandro Tomasiello","G. Bruno De Luca","Shi-Zhuo Looi","Vinh-Kha Le","Noam Kolt","Jiayi Pan","Emma Rodman","Jacob Drori","Carl J Fossum","Niklas Muennighoff","Milind Jagota","Ronak Pradeep","Honglu Fan","Jonathan Eicher","Michael Chen","Kushal Thaman","William Merrill","Moritz Firsching","Carter Harris","Stefan Ciobâcă","Jason Gross","Rohan Pandey","Ilya Gusev","Adam Jones","Shashank Agnihotri","Pavel Zhelnov","Mohammadreza Mofayezi","Alexander Piperski","David K. Zhang","Kostiantyn Dobarskyi","Roman Leventov","Ignat Soroko","Joshua Duersch","Vage Taamazyan","Andrew Ho","Wenjie Ma","William Held","Ruicheng Xian","Armel Randy Zebaze","Mohanad Mohamed","Julian Noah Leser","Michelle X Yuan","Laila Yacar","Johannes Lengler","Katarzyna Olszewska","Claudio Di Fratta","Edson Oliveira","Joseph W. Jackson","Andy Zou","Muthu Chidambaram","Timothy Manik","Hector Haffenden","Dashiell Stander","Ali Dasouqi","Alexander Shen","Bita Golshani","David Stap","Egor Kretov","Mikalai Uzhou","Alina Borisovna Zhidkovskaya","Nick Winter","Miguel Orbegozo Rodriguez","Robert Lauff","Dustin Wehr","Colin Tang","Zaki Hossain","Shaun Phillips","Fortuna Samuele","Fredrik Ekström","Angela Hammon","Oam Patel","Faraz Farhidi","George Medley","Forough Mohammadzadeh","Madellene Peñaflor","Haile Kassahun","Alena Friedrich","Rayner Hernandez Perez","Daniel Pyda","Taom Sakal","Omkar Dhamane","Ali Khajegili Mirabadi","Eric Hallman","Kenchi Okutsu","Mike Battaglia","Mohammad Maghsoudimehrabani","Alon Amit","Dave Hulbert","Roberto Pereira","Simon Weber"," Handoko","Anton Peristyy","Stephen Malina","Mustafa Mehkary","Rami Aly","Frank Reidegeld","Anna-Katharina Dick","Cary Friday","Mukhwinder Singh","Hassan Shapourian","Wanyoung Kim","Mariana Costa","Hubeyb Gurdogan","Harsh Kumar","Chiara Ceconello","Chao Zhuang","Haon Park","Micah Carroll","Andrew R. Tawfeek","Stefan Steinerberger","Daattavya Aggarwal","Michael Kirchhof","Linjie Dai","Evan Kim","Johan Ferret","Jainam Shah","Yuzhou Wang","Minghao Yan","Krzysztof Burdzy","Lixin Zhang","Antonio Franca","Diana T. Pham","Kang Yong Loh","Joshua Robinson","Abram Jackson","Paolo Giordano","Philipp Petersen","Adrian Cosma","Jesus Colino","Colin White","Jacob Votava","Vladimir Vinnikov","Ethan Delaney","Petr Spelda","Vit Stritecky","Syed M. Shahid","Jean-Christophe Mourrat","Lavr Vetoshkin","Koen Sponselee","Renas Bacho","Zheng-Xin Yong","Florencia de la Rosa","Nathan Cho","Xiuyu Li","Guillaume Malod","Orion Weller","Guglielmo Albani","Leon Lang","Julien Laurendeau","Dmitry Kazakov","Fatimah Adesanya","Julien Portier","Lawrence Hollom","Victor Souza","Yuchen Anna Zhou","Julien Degorre","Yiğit Yalın","Gbenga Daniel Obikoya"," Rai","Filippo Bigi","M. C. Boscá","Oleg Shumar","Kaniuar Bacho","Gabriel Recchia","Mara Popescu","Nikita Shulga","Ngefor Mildred Tanwie","Thomas C. H. Lux","Ben Rank","Colin Ni","Matthew Brooks","Alesia Yakimchyk"," Huanxu"," Liu","Stefano Cavalleri","Olle Häggström","Emil Verkama","Joshua Newbould","Hans Gundlach","Leonor Brito-Santana","Brian Amaro","Vivek Vajipey","Rynaa Grover","Ting Wang","Yosi Kratish","Wen-Ding Li","Sivakanth Gopi","Andrea Caciolai","Christian Schroeder de Witt","Pablo Hernández-Cámara","Emanuele Rodolà","Jules Robins","Dominic Williamson","Vincent Cheng","Brad Raynor","Hao Qi","Ben Segev","Jingxuan Fan","Sarah Martinson","Erik Y. Wang","Kaylie Hausknecht","Michael P. Brenner","Mao Mao","Christoph Demian","Peyman Kassani","Xinyu Zhang","David Avagian","Eshawn Jessica Scipio","Alon Ragoler","Justin Tan","Blake Sims","Rebeka Plecnik","Aaron Kirtland","Omer Faruk Bodur","D. P. Shinde","Yan Carlos Leyva Labrador","Zahra Adoul","Mohamed Zekry","Ali Karakoc","Tania C. B. Santos","Samir Shamseldeen","Loukmane Karim","Anna Liakhovitskaia","Nate Resman","Nicholas Farina","Juan Carlos Gonzalez","Gabe Maayan","Earth Anderson","Rodrigo De Oliveira Pena","Elizabeth Kelley","Hodjat Mariji","Rasoul Pouriamanesh","Wentao Wu","Ross Finocchio","Ismail Alarab","Joshua Cole","Danyelle Ferreira","Bryan Johnson","Mohammad Safdari","Liangti Dai","Siriphan Arthornthurasuk","Isaac C. McAlister","Alejandro José Moyano","Alexey Pronin","Jing Fan","Angel Ramirez-Trinidad","Yana Malysheva","Daphiny Pottmaier","Omid Taheri","Stanley Stepanic","Samuel Perry","Luke Askew","Raúl Adrián Huerta Rodríguez","Ali M. R. Minissi","Ricardo Lorena","Krishnamurthy Iyer","Arshad Anil Fasiludeen","Ronald Clark","Josh Ducey","Matheus Piza","Maja Somrak","Eric Vergo","Juehang Qin","Benjámin Borbás","Eric Chu","Jack Lindsey","Antoine Jallon","I. M. J. McInnis","Evan Chen","Avi Semler","Luk Gloor","Tej Shah","Marc Carauleanu","Pascal Lauer","Tran Đuc Huy","Hossein Shahrtash","Emilien Duc","Lukas Lewark","Assaf Brown","Samuel Albanie","Brian Weber","Warren S. Vaz","Pierre Clavier","Yiyang Fan","Gabriel Poesia Reis e Silva"," Long"," Lian","Marcus Abramovitch","Xi Jiang","Sandra Mendoza","Murat Islam","Juan Gonzalez","Vasilios Mavroudis","Justin Xu","Pawan Kumar","Laxman Prasad Goswami","Daniel Bugas","Nasser Heydari","Ferenc Jeanplong","Thorben Jansen","Antonella Pinto","Archimedes Apronti","Abdallah Galal","Ng Ze-An","Ankit Singh","Tong Jiang","Joan of Arc Xavier","Kanu Priya Agarwal","Mohammed Berkani","Gang Zhang","Zhehang Du","Benedito Alves de Oliveira Junior","Dmitry Malishev","Nicolas Remy","Taylor D. Hartman","Tim Tarver","Stephen Mensah","Gautier Abou Loume","Wiktor Morak","Farzad Habibi","Sarah Hoback","Will Cai","Javier Gimenez","Roselynn Grace Montecillo","Jakub Łucki","Russell Campbell","Asankhaya Sharma","Khalida Meer","Shreen Gul","Daniel Espinosa Gonzalez","Xavier Alapont","Alex Hoover","Gunjan Chhablani","Freddie Vargus","Arunim Agarwal","Yibo Jiang","Deepakkumar Patil","David Outevsky","Kevin Joseph Scaria","Rajat Maheshwari","Abdelkader Dendane","Priti Shukla","Ashley Cartwright","Sergei Bogdanov","Niels Mündler","Sören Möller","Luca Arnaboldi","Kunvar Thaman","Muhammad Rehan Siddiqi","Prajvi Saxena","Himanshu Gupta","Tony Fruhauff","Glen Sherman","Mátyás Vincze","Siranut Usawasutsakorn","Dylan Ler","Anil Radhakrishnan","Innocent Enyekwe","Sk Md Salauddin","Jiang Muzhen","Aleksandr Maksapetyan","Vivien Rossbach","Chris Harjadi","Mohsen Bahaloohoreh","Claire Sparrow","Jasdeep Sidhu","Sam Ali","Song Bian","John Lai","Eric Singer","Justine Leon Uro","Greg Bateman","Mohamed Sayed","Ahmed Menshawy","Darling Duclosel","Dario Bezzi","Yashaswini Jain","Ashley Aaron","Murat Tiryakioglu","Sheeshram Siddh","Keith Krenek","Imad Ali Shah","Jun Jin","Scott Creighton","Denis Peskoff","Zienab EL-Wasif","Ragavendran P V","Michael Richmond","Joseph McGowan","Tejal Patwardhan","Hao-Yu Sun","Ting Sun","Nikola Zubić","Samuele Sala","Stephen Ebert","Jean Kaddour","Manuel Schottdorf","Dianzhuo Wang","Gerol Petruzella","Alex Meiburg","Tilen Medved","Ali ElSheikh","S Ashwin Hebbar","Lorenzo Vaquero","Xianjun Yang","Jason Poulos","Vilém Zouhar","Sergey Bogdanik","Mingfang Zhang","Jorge Sanz-Ros","David Anugraha","Yinwei Dai","Anh N. Nhu","Xue Wang","Ali Anil Demircali","Zhibai Jia","Yuyin Zhou","Juncheng Wu","Mike He","Nitin Chandok","Aarush Sinha","Gaoxiang Luo","Long Le","Mickaël Noyé","Michał Perełkiewicz","Ioannis Pantidis","Tianbo Qi","Soham Sachin Purohit","Letitia Parcalabescu","Thai-Hoa Nguyen","Genta Indra Winata","Edoardo M. Ponti","Hanchen Li","Kaustubh Dhole","Jongee Park","Dario Abbondanza","Yuanli Wang","Anupam Nayak","Diogo M. Caetano","Antonio A. W. L. Wong","Maria del Rio-Chanona","Dániel Kondor","Pieter Francois","Ed Chalstrey","Jakob Zsambok","Dan Hoyer","Jenny Reddish","Jakob Hauser","Francisco-Javier Rodrigo-Ginés","Suchandra Datta","Maxwell Shepherd","Thom Kamphuis","Qizheng Zhang","Hyunjun Kim","Ruiji Sun","Jianzhu Yao","Franck Dernoncourt","Satyapriya Krishna","Sina Rismanchian","Bonan Pu","Francesco Pinto","Yingheng Wang","Kumar Shridhar","Kalon J. Overholt","Glib Briia","Hieu Nguyen"," David","Soler Bartomeu","Tony CY Pang","Adam Wecker","Yifan Xiong","Fanfei Li","Lukas S. Huber","Joshua Jaeger","Romano De Maddalena","Xing Han Lù","Yuhui Zhang","Claas Beger","Patrick Tser Jern Kon","Sean Li","Vivek Sanker","Ming Yin","Yihao Liang","Xinlu Zhang","Ankit Agrawal","Li S. Yifei","Zechen Zhang","Mu Cai","Yasin Sonmez","Costin Cozianu","Changhao Li","Alex Slen","Shoubin Yu","Hyun Kyu Park","Gabriele Sarti","Marcin Briański","Alessandro Stolfo","Truong An Nguyen","Mike Zhang","Yotam Perlitz","Jose Hernandez-Orallo","Runjia Li","Amin Shabani","Felix Juefei-Xu","Shikhar Dhingra","Orr Zohar","My Chiffon Nguyen","Alexander Pondaven","Abdurrahim Yilmaz","Xuandong Zhao","Chuanyang Jin","Muyan Jiang","Stefan Todoran","Xinyao Han","Jules Kreuer","Brian Rabern","Anna Plassart","Martino Maggetti","Luther Yap","Robert Geirhos","Jonathon Kean","Dingsu Wang","Sina Mollaei","Chenkai Sun","Yifan Yin","Shiqi Wang","Rui Li","Yaowen Chang","Anjiang Wei","Alice Bizeul","Xiaohan Wang","Alexandre Oliveira Arrais","Kushin Mukherjee","Jorge Chamorro-Padial","Jiachen Liu","Xingyu Qu","Junyi Guan","Adam Bouyamourn","Shuyu Wu","Martyna Plomecka","Junda Chen","Mengze Tang","Jiaqi Deng","Shreyas Subramanian","Haocheng Xi","Haoxuan Chen","Weizhi Zhang","Yinuo Ren","Haoqin Tu","Sejong Kim","Yushun Chen","Sara Vera Marjanović","Junwoo Ha","Grzegorz Luczyna","Jeff J. Ma","Zewen Shen","Dawn Song","Cedegao E. Zhang","Zhun Wang","Gaël Gendron","Yunze Xiao","Leo Smucker","Erica Weng","Kwok Hao Lee","Zhe Ye","Stefano Ermon","Ignacio D. Lopez-Miguel","Theo Knights","Anthony Gitter","Namkyu Park","Boyi Wei","Hongzheng Chen","Kunal Pai","Ahmed Elkhanany","Han Lin","Philipp D. Siedler","Jichao Fang","Ritwik Mishra","Károly Zsolnai-Fehér","Xilin Jiang","Shadab Khan","Jun Yuan","Rishab Kumar Jain","Xi Lin","Mike Peterson","Zhe Wang","Aditya Malusare","Maosen Tang","Isha Gupta","Ivan Fosin","Timothy Kang","Barbara Dworakowska","Kazuki Matsumoto","Guangyao Zheng","Gerben Sewuster","Jorge Pretel Villanueva","Ivan Rannev","Igor Chernyavsky","Jiale Chen","Deepayan Banik","Ben Racz","Wenchao Dong","Jianxin Wang","Laila Bashmal","Duarte V. Gonçalves","Wei Hu","Kaushik Bar","Ondrej Bohdal","Atharv Singh Patlan","Shehzaad Dhuliawala","Caroline Geirhos","Julien Wist","Yuval Kansal","Bingsen Chen","Kutay Tire","Atak Talay Yücel","Brandon Christof","Veerupaksh Singla","Zijian Song","Sanxing Chen","Jiaxin Ge","Kaustubh Ponkshe","Isaac Park","Tianneng Shi","Martin Q. Ma","Joshua Mak","Sherwin Lai","Antoine Moulin","Zhuo Cheng","Zhanda Zhu","Ziyi Zhang","Vaidehi Patil","Ketan Jha","Qiutong Men","Jiaxuan Wu","Tianchi Zhang","Bruno Hebling Vieira","Alham Fikri Aji","Jae-Won Chung","Mohammed Mahfoud","Ha Thi Hoang","Marc Sperzel","Wei Hao","Kristof Meding","Sihan Xu","Vassilis Kostakos","Davide Manini","Yueying Liu","Christopher Toukmaji","Jay Paek","Eunmi Yu","Arif Engin Demircali","Zhiyi Sun","Ivan Dewerpe","Hongsen Qin","Roman Pflugfelder","James Bailey","Johnathan Morris","Ville Heilala","Sybille Rosset","Zishun Yu","Peter E. Chen","Woongyeong Yeo","Eeshaan Jain","Ryan Yang","Sreekar Chigurupati","Julia Chernyavsky","Sai Prajwal Reddy","Subhashini Venugopalan","Hunar Batra","Core Francisco Park","Hieu Tran","Guilherme Maximiano","Genghan Zhang","Yizhuo Liang","Hu Shiyu","Rongwu Xu","Rui Pan","Siddharth Suresh","Ziqi Liu","Samaksh Gulati","Songyang Zhang","Peter Turchin","Christopher W. Bartlett","Christopher R. Scotese","Phuong M. Cao","Aakaash Nattanmai","Gordon McKellips","Anish Cheraku","Asim Suhail","Ethan Luo","Marvin Deng","Jason Luo","Ashley Zhang","Kavin Jindel","Jay Paek","Kasper Halevy","Allen Baranov","Michael Liu","Advaith Avadhanam","David Zhang","Vincent Cheng","Brad Ma","Evan Fu","Liam Do","Joshua Lass","Hubert Yang","Surya Sunkari","Vishruth Bharath","Violet Ai","James Leung","Rishit Agrawal","Alan Zhou","Kevin Chen","Tejas Kalpathi","Ziqi Xu","Gavin Wang","Tyler Xiao","Erik Maung","Sam Lee","Ryan Yang","Roy Yue","Ben Zhao","Julia Yoon","Sunny Sun","Aryan Singh","Ethan Luo","Clark Peng","Tyler Osbey","Taozhi Wang","Daryl Echeazu","Hubert Yang","Timothy Wu","Spandan Patel","Vidhi Kulkarni","Vijaykaarti Sundarapandiyan","Ashley Zhang","Andrew Le","Zafir Nasim","Srikar Yalam","Ritesh Kasamsetty","Soham Samal","Hubert Yang","David Sun","Nihar Shah","Abhijeet Saha","Alex Zhang","Leon Nguyen","Laasya Nagumalli","Kaixin Wang","Alan Zhou","Aidan Wu","Jason Luo","Anwith Telluri","Summer Yue","Alexandr Wang","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2501.14249v7.pdf","comment":"29 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.18036v3","updated":"2025-04-19T21:43:39Z","published":"2025-02-25T09:48:53Z","title":"Harnessing Multiple Large Language Models: A Survey on LLM Ensemble","summary":"  LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference'', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.\n","authors":["Zhijun Chen","Jingzheng Li","Pengpeng Chen","Zhuoran Li","Kai Sun","Yuankai Luo","Qianren Mao","Dingqi Yang","Hailong Sun","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.18036v3.pdf","comment":"9 pages, 2 figures, codebase:\n  https://github.com/junchenzhi/Awesome-LLM-Ensemble"},{"id":"http://arxiv.org/abs/2502.09284v2","updated":"2025-04-19T20:20:36Z","published":"2025-02-13T12:57:15Z","title":"SparQLe: Speech Queries to Text Translation Through LLMs","summary":"  With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that leverages self-supervised speech\nrepresentations in combination with instruction-tuned LLMs for speech-to-text\ntranslation. The proposed approach leverages a modality adapter to align\nextracted speech features with instruction-tuned LLMs using English-language\ndata. Our experiments demonstrate that this method effectively preserves the\nsemantic content of the input speech and serves as an effective bridge between\nself-supervised speech models and instruction-tuned LLMs, offering a promising\nsolution for various speech understanding applications.\n","authors":["Amirbek Djanibekov","Hanan Aldarmaki"],"pdf_url":"https://arxiv.org/pdf/2502.09284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09689v2","updated":"2025-04-19T19:56:31Z","published":"2025-04-13T18:47:22Z","title":"EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental\n  Health Safety","summary":"  The rise of LLM-driven AI characters raises safety concerns, particularly for\nvulnerable human users with psychological disorders. To address these risks, we\npropose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate\nmental health hazards in human-AI interactions. EmoAgent comprises two\ncomponents: EmoEval simulates virtual users, including those portraying\nmentally vulnerable individuals, to assess mental health changes before and\nafter interactions with AI characters. It uses clinically proven psychological\nand psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks\ninduced by LLM. EmoGuard serves as an intermediary, monitoring users' mental\nstatus, predicting potential harm, and providing corrective feedback to\nmitigate risks. Experiments conducted in popular character-based chatbots show\nthat emotionally engaging dialogues can lead to psychological deterioration in\nvulnerable users, with mental state deterioration in more than 34.4% of the\nsimulations. EmoGuard significantly reduces these deterioration rates,\nunderscoring its role in ensuring safer AI-human interactions. Our code is\navailable at: https://github.com/1akaman/EmoAgent\n","authors":["Jiahao Qiu","Yinghui He","Xinzhe Juan","Yiming Wang","Yuhan Liu","Zixin Yao","Yue Wu","Xun Jiang","Ling Yang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09689v2.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.13031v2","updated":"2025-04-19T19:15:04Z","published":"2025-03-17T10:33:39Z","title":"Halving transcription time: A fast, user-friendly and GDPR-compliant\n  workflow to create AI-assisted transcripts for content analysis","summary":"  In qualitative research, data transcription is often labor-intensive and\ntime-consuming. To expedite this process, a workflow utilizing artificial\nintelligence (AI) was developed. This workflow not only enhances transcription\nspeed but also addresses the issue of AI-generated transcripts often lacking\ncompatibility with standard content analysis software. Within this workflow,\nautomatic speech recognition is employed to create initial transcripts from\naudio recordings, which are then formatted to be compatible with content\nanalysis software such as ATLAS or MAXQDA. Empirical data from a study of 12\ninterviews suggests that this workflow can reduce transcription time by up to\n76.4%. Furthermore, by using widely used standard software, this process is\nsuitable for both students and researchers while also being adaptable to a\nvariety of learning, teaching, and research environments. It is also\nparticularly beneficial for non-native speakers. In addition, the workflow is\nGDPR-compliant and facilitates local, offline transcript generation, which is\ncrucial when dealing with sensitive data.\n","authors":["Jakob Sponholz","Andreas Weilinghoff","Juliane Schopf"],"pdf_url":"https://arxiv.org/pdf/2503.13031v2.pdf","comment":"10 pages, 1 table"},{"id":"http://arxiv.org/abs/2412.14354v2","updated":"2025-04-19T19:13:42Z","published":"2024-12-18T21:42:15Z","title":"State Space Models are Strong Text Rerankers","summary":"  Transformers dominate NLP and IR; but their inference inefficiencies and\nchallenges in extrapolating to longer contexts have sparked interest in\nalternative model architectures. Among these, state space models (SSMs) like\nMamba offer promising advantages, particularly $O(1)$ time complexity in\ninference. Despite their potential, SSMs' effectiveness at text reranking\\, --\n\\,a task requiring fine-grained query-document interaction and long-context\nunderstanding\\, -- \\,remains underexplored. This study benchmarks SSM-based\narchitectures (specifically, Mamba-1 and Mamba-2) against transformer-based\nmodels across various scales, architectures, and pre-training objectives,\nfocusing on performance and efficiency in text reranking tasks. We find that\n(1) Mamba architectures achieve competitive text ranking performance,\ncomparable to transformer-based models of similar size; (2) they are less\nefficient in training and inference compared to transformers with flash\nattention; and (3) Mamba-2 outperforms Mamba-1 in both performance and\nefficiency. These results underscore the potential of state space models as a\ntransformer alternative and highlight areas for improvement in future IR\napplications.\n","authors":["Zhichao Xu","Jinghua Yan","Ashim Gupta","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2412.14354v2.pdf","comment":"Accepted to RepL4NLP 2025. The first two authors contributed equally,\n  order decided randomly"},{"id":"http://arxiv.org/abs/2504.02051v2","updated":"2025-04-19T19:05:03Z","published":"2025-04-02T18:15:41Z","title":"Self-Resource Allocation in Multi-Agent LLM Systems","summary":"  With the development of LLMs as agents, there is a growing interest in\nconnecting multiple agents into multi-agent systems to solve tasks\nconcurrently, focusing on their role in task assignment and coordination. This\npaper explores how LLMs can effectively allocate computational tasks among\nmultiple agents, considering factors such as cost, efficiency, and performance.\nIn this work, we address key questions, including the effectiveness of LLMs as\norchestrators and planners, comparing their effectiveness in task assignment\nand coordination. Our experiments demonstrate that LLMs can achieve high\nvalidity and accuracy in resource allocation tasks. We find that the planner\nmethod outperforms the orchestrator method in handling concurrent actions,\nresulting in improved efficiency and better utilization of agents.\nAdditionally, we show that providing explicit information about worker\ncapabilities enhances the allocation strategies of planners, particularly when\ndealing with suboptimal workers.\n","authors":["Alfonso Amayuelas","Jingbo Yang","Saaket Agashe","Ashwin Nagarajan","Antonis Antoniades","Xin Eric Wang","William Wang"],"pdf_url":"https://arxiv.org/pdf/2504.02051v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14370v1","updated":"2025-04-19T18:08:18Z","published":"2025-04-19T18:08:18Z","title":"Density Measures for Language Generation","summary":"  The recent successes of large language models (LLMs) have led to a surge of\ntheoretical research into language generation. A recent line of work proposes\nan abstract view, called language generation in the limit, where generation is\nseen as a game between an adversary and an algorithm: the adversary generates\nstrings from an unknown language $K$, chosen from a countable collection of\ncandidate languages, and after seeing a finite set of these strings, the\nalgorithm must generate new strings from $K$ that it has not seen before. This\nformalism highlights a key tension: the trade-off between validity (the\nalgorithm should only produce strings from the language) and breadth (it should\nbe able to produce many strings from the language). This trade-off is central\nin applied language generation as well, where it appears as a balance between\nhallucination (generating invalid utterances) and mode collapse (generating\nonly a restricted set of outputs). Despite its importance, this trade-off has\nbeen challenging to study quantitatively. We develop ways to quantify this\ntrade-off by formalizing breadth using measures of density. Existing algorithms\nfor language generation in the limit produce output sets that can have zero\ndensity in the true language, and this important failure of breadth might seem\nunavoidable. We show, however, that such a failure is not necessary: we provide\nan algorithm for language generation in the limit whose outputs have strictly\npositive density in $K$. We also study the internal representations built by\nthese algorithms, specifically the sequence of hypothesized candidate languages\nthey consider, and show that achieving the strongest form of breadth may\nrequire oscillating indefinitely between high- and low-density representations.\nOur analysis introduces a novel topology on language families, with notions of\nconvergence and limit points playing a key role.\n","authors":["Jon Kleinberg","Fan Wei"],"pdf_url":"https://arxiv.org/pdf/2504.14370v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14367v1","updated":"2025-04-19T17:50:34Z","published":"2025-04-19T17:50:34Z","title":"Diverse Prompts: Illuminating the Prompt Space of Large Language Models\n  with MAP-Elites","summary":"  Prompt engineering is essential for optimizing large language models (LLMs),\nyet the link between prompt structures and task performance remains\nunderexplored. This work introduces an evolutionary approach that combines\ncontext-free grammar (CFG) with the MAP-Elites algorithm to systematically\nexplore the prompt space. Our method prioritizes quality and diversity,\ngenerating high-performing and structurally varied prompts while analyzing\ntheir alignment with diverse tasks by varying traits such as the number of\nexamples (shots) and reasoning depth. By systematically mapping the phenotypic\nspace, we reveal how structural variations influence LLM performance, offering\nactionable insights for task-specific and adaptable prompt design. Evaluated on\nseven BigBench Lite tasks across multiple LLMs, our results underscore the\ncritical interplay of quality and diversity, advancing the effectiveness and\nversatility of LLMs.\n","authors":["Gabriel Machado Santos","Rita Maria da Silva Julia","Marcelo Zanchetta do Nascimento"],"pdf_url":"https://arxiv.org/pdf/2504.14367v1.pdf","comment":"8 pages Accepted for publication in IEEE CEC 2025"},{"id":"http://arxiv.org/abs/2504.14366v1","updated":"2025-04-19T17:49:52Z","published":"2025-04-19T17:49:52Z","title":"Empirical Evaluation of Knowledge Distillation from Transformers to\n  Subquadratic Language Models","summary":"  Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs) by training a smaller student model to mimic a larger\nteacher model. Typically, both the teacher and student are Transformer-based\narchitectures, leveraging softmax attention for sequence modeling. However, the\nquadratic complexity of self-attention at inference time remains a significant\nbottleneck, motivating the exploration of subquadratic alternatives such as\nstructured state-space models (SSMs), linear attention, and recurrent\narchitectures. In this work, we systematically evaluate the transferability of\nknowledge distillation from a Transformer teacher to nine subquadratic student\narchitectures. Our study aims to determine which subquadratic model best aligns\nwith the teacher's learned representations and how different architectural\nconstraints influence the distillation process. We also investigate the impact\nof intelligent initialization strategies, including matrix mixing and\nquery-key-value (QKV) copying, on the adaptation process. Our empirical results\non multiple NLP benchmarks provide insights into the trade-offs between\nefficiency and performance, highlighting key factors for successful knowledge\ntransfer to subquadratic architectures.\n","authors":["Patrick Haller","Jonas Golde","Alan Akbik"],"pdf_url":"https://arxiv.org/pdf/2504.14366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15379v3","updated":"2025-04-19T17:40:47Z","published":"2024-08-27T19:33:15Z","title":"DualKanbaFormer: An Efficient Selective Sparse Framework for Multimodal\n  Aspect-based Sentiment Analysis","summary":"  Multimodal Aspect-based Sentiment Analysis (MABSA) enhances sentiment\ndetection by integrating textual data with complementary modalities, such as\nimages, to provide a more refined and comprehensive understanding of sentiment.\nHowever, conventional attention mechanisms, despite notable benchmarks, are\nhindered by quadratic complexity, limiting their ability to fully capture\nglobal contextual dependencies and rich semantic information in both\nmodalities. To address this limitation, we introduce DualKanbaFormer, a novel\nframework that leverages parallel Textual and Visual KanbaFormer modules for\nrobust multimodal analysis. Our approach incorporates Aspect-Driven Sparse\nAttention (ADSA) to dynamically balance coarse-grained aggregation and\nfine-grained selection for aspect-focused precision, ensuring the preservation\nof both global context awareness and local precision in textual and visual\nrepresentations. Additionally, we utilize the Selective State Space Model\n(Mamba) to capture extensive global semantic information across both\nmodalities. Furthermore, We replace traditional feed-forward networks and\nnormalization with Kolmogorov-Arnold Networks (KANs) and Dynamic Tanh (DyT) to\nenhance non-linear expressivity and inference stability. To facilitate the\neffective integration of textual and visual features, we design a multimodal\ngated fusion layer that dynamically optimizes inter-modality interactions,\nsignificantly enhancing the models efficacy in MABSA tasks. Comprehensive\nexperiments on two publicly available datasets reveal that DualKanbaFormer\nconsistently outperforms several state-of-the-art (SOTA) models.\n","authors":["Adamu Lawan","Juhua Pu","Haruna Yunusa","Muhammad Lawan","Aliyu Umar","Adamu Sani Yahya","Mahmoud Basi"],"pdf_url":"https://arxiv.org/pdf/2408.15379v3.pdf","comment":"12 pages, 2 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2504.14363v1","updated":"2025-04-19T17:40:04Z","published":"2025-04-19T17:40:04Z","title":"Improving RL Exploration for LLM Reasoning through Retrospective Replay","summary":"  Reinforcement learning (RL) has increasingly become a pivotal technique in\nthe post-training of large language models (LLMs). The effective exploration of\nthe output space is essential for the success of RL. We observe that for\ncomplex problems, during the early stages of training, the model exhibits\nstrong exploratory capabilities and can identify promising solution ideas.\nHowever, its limited capability at this stage prevents it from successfully\nsolving these problems. The early suppression of these potentially valuable\nsolution ideas by the policy gradient hinders the model's ability to revisit\nand re-explore these ideas later. Consequently, although the LLM's capabilities\nimprove in the later stages of training, it still struggles to effectively\naddress these complex problems. To address this exploration issue, we propose a\nnovel algorithm named Retrospective Replay-based Reinforcement Learning (RRL),\nwhich introduces a dynamic replay mechanism throughout the training process.\nRRL enables the model to revisit promising states identified in the early\nstages, thereby improving its efficiency and effectiveness in exploration. To\nevaluate the effectiveness of RRL, we conduct extensive experiments on complex\nreasoning tasks, including mathematical reasoning and code generation, and\ngeneral dialogue tasks. The results indicate that RRL maintains high\nexploration efficiency throughout the training period, significantly enhancing\nthe effectiveness of RL in optimizing LLMs for complicated reasoning tasks.\nMoreover, it also improves the performance of RLHF, making the model both safer\nand more helpful.\n","authors":["Shihan Dou","Muling Wu","Jingwen Xu","Rui Zheng","Tao Gui","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2504.14363v1.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.14361v1","updated":"2025-04-19T17:35:54Z","published":"2025-04-19T17:35:54Z","title":"Integrating Single-Cell Foundation Models with Graph Neural Networks for\n  Drug Response Prediction","summary":"  In this study, we propose an innovative methodology for predicting Cancer\nDrug Response (CDR) through the integration of the scGPT foundation model\nwithin the DeepCDR model. Our approach utilizes scGPT to generate embeddings\nfrom gene expression data, which are then used as gene expression input data\nfor DeepCDR. The experimental findings demonstrate the efficacy of this\nscGPT-based method in outperforming previous related works, including the\noriginal DeepCDR model and the scFoundation-based model. This study highlights\nthe potential of scGPT embeddings to enhance the accuracy of CDR predictions\nand offers a promising alternative to existing approaches.\n","authors":["Till Rossner","Ziteng Li","Jonas Balke","Nikoo Salehfard","Tom Seifert","Ming Tang"],"pdf_url":"https://arxiv.org/pdf/2504.14361v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.14359v1","updated":"2025-04-19T17:23:12Z","published":"2025-04-19T17:23:12Z","title":"A Multimodal Recaptioning Framework to Account for Perceptual Diversity\n  in Multilingual Vision-Language Modeling","summary":"  There are many ways to describe, name, and group objects when captioning an\nimage. Differences are evident when speakers come from diverse cultures due to\nthe unique experiences that shape perception. Machine translation of captions\nhas pushed multilingual capabilities in vision-language models (VLMs), but data\ncomes mainly from English speakers, indicating a perceptual bias and lack of\nmodel flexibility. In this work, we address this challenge and outline a\ndata-efficient framework to instill multilingual VLMs with greater\nunderstanding of perceptual diversity. We specifically propose an LLM-based,\nmultimodal recaptioning strategy that alters the object descriptions of English\ncaptions before translation. The greatest benefits are demonstrated in a\ntargeted multimodal mechanism guided by native speaker data. By adding produced\nrewrites as augmentations in training, we improve on German and Japanese\ntext-image retrieval cases studies (up to +3.5 mean recall overall, +4.7 on\nnon-native error cases). We further propose a mechanism to analyze the specific\nobject description differences across datasets, and we offer insights into\ncross-dataset and cross-language generalization.\n","authors":["Kyle Buettner","Jacob Emmerson","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2504.14359v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16714v3","updated":"2025-04-19T15:59:33Z","published":"2024-10-22T05:51:34Z","title":"Magnetic Preference Optimization: Achieving Last-iterate Convergence for\n  Language Model Alignment","summary":"  Self-play methods have demonstrated remarkable success in enhancing model\ncapabilities across various domains. In the context of Reinforcement Learning\nfrom Human Feedback (RLHF), self-play not only boosts Large Language Model\n(LLM) performance but also overcomes the limitations of traditional\nBradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a\npreference-based, two-player constant-sum game. However, existing methods\neither guarantee only average-iterate convergence, incurring high storage and\ninference costs, or converge to the NE of a regularized game, failing to\naccurately reflect true human preferences. In this paper, we introduce Magnetic\nPreference Optimization (MPO), a novel approach capable of achieving\nlast-iterate convergence to the NE of the original game, effectively overcoming\nthe limitations of existing methods. Building upon Magnetic Mirror Descent\n(MMD), MPO attains a linear convergence rate, making it particularly suitable\nfor fine-tuning LLMs. To ensure our algorithm is both theoretically sound and\npractically viable, we present a simple yet effective implementation that\nadapts the theoretical insights to the RLHF setting. Empirical results\ndemonstrate that MPO can significantly enhance the performance of LLMs,\nhighlighting the potential of self-play methods in alignment.\n","authors":["Mingzhi Wang","Chengdong Ma","Qizhi Chen","Linjian Meng","Yang Han","Jiancong Xiao","Zhaowei Zhang","Jing Huo","Weijie J. Su","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2410.16714v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2411.00927v2","updated":"2025-04-19T15:43:36Z","published":"2024-11-01T15:57:45Z","title":"ReSpAct: Harmonizing Reasoning, Speaking, and Acting Towards Building\n  Large Language Model-Based Conversational AI Agents","summary":"  Large language model (LLM)-based agents are increasingly employed to interact\nwith external environments (e.g., games, APIs, world models) to solve\nuser-provided tasks. However, current frameworks often lack the ability to\ncollaborate effectively with users in fully conversational settings.\nConversations are essential for aligning on task details, achieving\nuser-defined goals, and satisfying preferences. While existing agents address\nambiguity through clarification questions, they underutilize the broader\npotential of an LLM's conversational capabilities. In this work, we introduce\nReSpAct, an LLM-based agent designed to seamlessly integrate reasoning,\ndecision-making, and dynamic dialogue for task-solving. Expanding on\nreasoning-first approaches like ReAct, ReSpAct employs active, free-flowing\ndialogues to interpret instructions, clarify goals, provide status updates,\nresolve subtask failures, and refine plans based on user inputs without any\nexplicit dialogue schema. By alternating between task-solving actions and\ninteractive conversations, ReSpAct demonstrates improved performance across\ndiverse environments. We evaluate ReSpAct in user-interactive settings,\nincluding task-oriented dialogue systems (MultiWOZ) and decision-making tasks\n(ALFWorld, WebShop). ReSpAct outperforms ReAct with absolute success rate\nimprovements of 6% and 4% in ALFWorld and WebShop, respectively, and achieves a\n5.5% gain in Inform and a 3% gain in Success scores in MultiWOZ. These results\nhighlight the value of integrating dynamic user-agent collaboration for more\neffective task resolution.\n","authors":["Vardhan Dongre","Xiaocheng Yang","Emre Can Acikgoz","Suvodip Dey","Gokhan Tur","Dilek Hakkani-Tür"],"pdf_url":"https://arxiv.org/pdf/2411.00927v2.pdf","comment":"31 pages, 10 Figures, 25 Tables"},{"id":"http://arxiv.org/abs/2504.14321v1","updated":"2025-04-19T15:15:59Z","published":"2025-04-19T15:15:59Z","title":"Multimodal Coreference Resolution for Chinese Social Media Dialogues:\n  Dataset and Benchmark Approach","summary":"  Multimodal coreference resolution (MCR) aims to identify mentions referring\nto the same entity across different modalities, such as text and visuals, and\nis essential for understanding multimodal content. In the era of rapidly\ngrowing mutimodal content and social media, MCR is particularly crucial for\ninterpreting user interactions and bridging text-visual references to improve\ncommunication and personalization. However, MCR research for real-world\ndialogues remains unexplored due to the lack of sufficient data resources.To\naddress this gap, we introduce TikTalkCoref, the first Chinese multimodal\ncoreference dataset for social media in real-world scenarios, derived from the\npopular Douyin short-video platform. This dataset pairs short videos with\ncorresponding textual dialogues from user comments and includes manually\nannotated coreference clusters for both person mentions in the text and the\ncoreferential person head regions in the corresponding video frames. We also\npresent an effective benchmark approach for MCR, focusing on the celebrity\ndomain, and conduct extensive experiments on our dataset, providing reliable\nbenchmark results for this newly constructed dataset. We will release the\nTikTalkCoref dataset to facilitate future research on MCR for real-world social\nmedia dialogues.\n","authors":["Xingyu Li","Chen Gong","Guohong Fu"],"pdf_url":"https://arxiv.org/pdf/2504.14321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16260v3","updated":"2025-04-19T14:43:00Z","published":"2024-11-25T10:23:11Z","title":"Unraveling Arithmetic in Large Language Models: The Role of Algebraic\n  Structures","summary":"  Large language models (LLMs) have demonstrated remarkable mathematical\ncapabilities, largely driven by chain-of-thought (CoT) prompting, which\ndecomposes complex reasoning into step-by-step solutions. This approach has\nenabled significant advancements, as evidenced by performance on benchmarks\nlike GSM8K and MATH. However, the mechanisms underlying LLMs' ability to\nperform arithmetic in a single step of CoT remain poorly understood. Existing\nstudies debate whether LLMs encode numerical values or rely on symbolic\nreasoning, while others explore attention and multi-layered processing in\narithmetic tasks. In this work, we propose that LLMs learn arithmetic by\ncapturing algebraic structures, such as commutativity and identity properties.\nSince these structures are observable through input-output relationships, they\ncan generalize to unseen data. We empirically demonstrate that LLMs can learn\nalgebraic structures using a custom dataset of arithmetic problems, as well as\nproviding theoretical evidence showing that, under specific configurations of\nweights and biases, the transformer-based LLMs can generate embeddings that\nremain invariant to both permutations of input tokens and the presence of\nidentity elements. Our findings indicate that leveraging algebraic structures\ncan enhance the LLMs' arithmetic capabilities, offering insights into improving\ntheir arithmetic performance.\n","authors":["Fu-Chieh Chang","You-Chen Lin","Pei-Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2411.16260v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11986v2","updated":"2025-04-19T13:53:16Z","published":"2025-04-16T11:27:47Z","title":"Large Language Models as Quasi-crystals: Coherence Without Repetition in\n  Generative Text","summary":"  This essay proposes an interpretive analogy between large language models\n(LLMs) and quasicrystals, systems that exhibit global coherence without\nperiodic repetition, generated through local constraints. While LLMs are\ntypically evaluated in terms of predictive accuracy, factuality, or alignment,\nthis structural perspective suggests that one of their most characteristic\nbehaviors is the production of internally resonant linguistic patterns. Drawing\non the history of quasicrystals, which forced a redefinition of structural\norder in physical systems, the analogy highlights an alternative mode of\ncoherence in generative language: constraint-based organization without\nrepetition or symbolic intent. Rather than viewing LLMs as imperfect agents or\nstochastic approximators, we suggest understanding them as generators of\nquasi-structured outputs. This framing complements existing evaluation\nparadigms by foregrounding formal coherence and pattern as interpretable\nfeatures of model behavior. While the analogy has limits, it offers a\nconceptual tool for exploring how coherence might arise and be assessed in\nsystems where meaning is emergent, partial, or inaccessible. In support of this\nperspective, we draw on philosophy of science and language, including\nmodel-based accounts of scientific representation, structural realism, and\ninferentialist views of meaning. We further propose the notion of structural\nevaluation: a mode of assessment that examines how well outputs propagate\nconstraint, variation, and order across spans of generated text. This essay\naims to reframe the current discussion around large language models, not by\nrejecting existing methods, but by suggesting an additional axis of\ninterpretation grounded in structure rather than semantics.\n","authors":["Jose Manuel Guevara-Vela"],"pdf_url":"https://arxiv.org/pdf/2504.11986v2.pdf","comment":"The discussion was restructured to add limitations to the analogy and\n  other clarifications"},{"id":"http://arxiv.org/abs/2504.05216v2","updated":"2025-04-19T13:16:08Z","published":"2025-04-07T16:03:59Z","title":"Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling","summary":"  Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin.\n","authors":["Hengran Zhang","Keping Bi","Jiafeng Guo","Xiaojie Sun","Shihao Liu","Daiting Shi","Dawei Yin","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.05216v2.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.14287v1","updated":"2025-04-19T13:11:50Z","published":"2025-04-19T13:11:50Z","title":"Probing the Subtle Ideological Manipulation of Large Language Models","summary":"  Large Language Models (LLMs) have transformed natural language processing,\nbut concerns have emerged about their susceptibility to ideological\nmanipulation, particularly in politically sensitive areas. Prior work has\nfocused on binary Left-Right LLM biases, using explicit prompts and fine-tuning\non political QA datasets. In this work, we move beyond this binary approach to\nexplore the extent to which LLMs can be influenced across a spectrum of\npolitical ideologies, from Progressive-Left to Conservative-Right. We introduce\na novel multi-task dataset designed to reflect diverse ideological positions\nthrough tasks such as ideological QA, statement ranking, manifesto cloze\ncompletion, and Congress bill comprehension. By fine-tuning three LLMs-Phi-2,\nMistral, and Llama-3-on this dataset, we evaluate their capacity to adopt and\nexpress these nuanced ideologies. Our findings indicate that fine-tuning\nsignificantly enhances nuanced ideological alignment, while explicit prompts\nprovide only minor refinements. This highlights the models' susceptibility to\nsubtle ideological manipulation, suggesting a need for more robust safeguards\nto mitigate these risks.\n","authors":["Demetris Paschalides","George Pallis","Marios D. Dikaiakos"],"pdf_url":"https://arxiv.org/pdf/2504.14287v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12323v2","updated":"2025-04-19T12:49:01Z","published":"2025-04-11T10:17:10Z","title":"The Other Side of the Coin: Exploring Fairness in Retrieval-Augmented\n  Generation","summary":"  Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nretrieving relevant document from external knowledge sources. By referencing\nthis external knowledge, RAG effectively reduces the generation of factually\nincorrect content and addresses hallucination issues within LLMs. Recently,\nthere has been growing attention to improving the performance and efficiency of\nRAG systems from various perspectives. While these advancements have yielded\nsignificant results, the application of RAG in domains with considerable\nsocietal implications raises a critical question about fairness: What impact\ndoes the introduction of the RAG paradigm have on the fairness of LLMs? To\naddress this question, we conduct extensive experiments by varying the LLMs,\nretrievers, and retrieval sources. Our experimental analysis reveals that the\nscale of the LLMs plays a significant role in influencing fairness outcomes\nwithin the RAG framework. When the model scale is smaller than 8B, the\nintegration of retrieval mechanisms often exacerbates unfairness in small-scale\nLLMs (e.g., LLaMA3.2-1B, Mistral-7B, and LLaMA3-8B). To mitigate the fairness\nissues introduced by RAG for small-scale LLMs, we propose two approaches,\nFairFT and FairFilter. Specifically, in FairFT, we align the retriever with the\nLLM in terms of fairness, enabling it to retrieve documents that facilitate\nfairer model outputs. In FairFilter, we propose a fairness filtering mechanism\nto filter out biased content after retrieval. Finally, we validate our proposed\napproaches on real-world datasets, demonstrating their effectiveness in\nimproving fairness while maintaining performance.\n","authors":["Zheng Zhang","Ning Li","Qi Liu","Rui Li","Weibo Gao","Qingyang Mao","Zhenya Huang","Baosheng Yu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2504.12323v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2504.12474v2","updated":"2025-04-19T12:14:06Z","published":"2025-04-16T20:25:11Z","title":"Integrating Structural and Semantic Signals in Text-Attributed Graphs\n  with BiGTex","summary":"  Text-attributed graphs (TAGs) present unique challenges in representation\nlearning by requiring models to capture both the semantic richness of\nnode-associated texts and the structural dependencies of the graph. While graph\nneural networks (GNNs) excel at modeling topological information, they lack the\ncapacity to process unstructured text. Conversely, large language models (LLMs)\nare proficient in text understanding but are typically unaware of graph\nstructure. In this work, we propose BiGTex (Bidirectional Graph Text), a novel\narchitecture that tightly integrates GNNs and LLMs through stacked Graph-Text\nFusion Units. Each unit allows for mutual attention between textual and\nstructural representations, enabling information to flow in both directions,\ntext influencing structure and structure guiding textual interpretation. The\nproposed architecture is trained using parameter-efficient fine-tuning (LoRA),\nkeeping the LLM frozen while adapting to task-specific signals. Extensive\nexperiments on five benchmark datasets demonstrate that BiGTex achieves\nstate-of-the-art performance in node classification and generalizes effectively\nto link prediction. An ablation study further highlights the importance of soft\nprompting and bi-directional attention in the model's success.\n","authors":["Azadeh Beiranvand","Seyed Mehdi Vahidipour"],"pdf_url":"https://arxiv.org/pdf/2504.12474v2.pdf","comment":"17 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.14260v1","updated":"2025-04-19T10:47:51Z","published":"2025-04-19T10:47:51Z","title":"Cross-attention for State-based model RWKV-7","summary":"  We introduce CrossWKV, a novel cross-attention mechanism for the state-based\nRWKV-7 model, designed to enhance the expressive power of text-to-image\ngeneration. Leveraging RWKV-7's linear-complexity Weighted Key-Value (WKV)\narchitecture, CrossWKV integrates text and image modalities in a single pass,\nutilizing a generalized delta rule with vector-valued gating and low-rank\nadaptations (LoRA) to achieve superior cross-modal alignment. Unlike\nTransformer-based models, CrossWKV's non-diagonal, input-dependent transition\nmatrix enables it to represent complex functions beyond the $\\mathrm{TC}^0$\ncomplexity class, including all regular languages, as demonstrated by its\nability to perform state-tracking tasks like $S_5$ permutation modeling.\nEvaluated within the Diffusion in RWKV-7 (DIR-7) on datasets such as LAION-5B\nand ImageNet, CrossWKV achieves a Frechet Inception Distance (FID) of 2.88 and\na CLIP score of 0.33 on ImageNet 256x256, matching state-of-the-art performance\nwhile offering robust generalization across diverse prompts. The model's\nenhanced expressivity, combined with constant memory usage and linear scaling,\npositions it as a powerful solution for advanced cross-modal tasks, with\npotential applications in high-resolution generation and dynamic state\nmanipulation.Code at https://github.com/TorchRWKV/flash-linear-attention\n","authors":["Liu Xiao","Li Zhiyuan","Lin Yueyu"],"pdf_url":"https://arxiv.org/pdf/2504.14260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12254v2","updated":"2025-04-19T09:55:40Z","published":"2025-04-16T17:05:14Z","title":"Advancing Arabic Speech Recognition Through Large-Scale Weakly\n  Supervised Learning","summary":"  Automatic speech recognition (ASR) is crucial for human-machine interaction\nin diverse applications like conversational agents, industrial robotics, call\ncenter automation, and automated subtitling. However, developing\nhigh-performance ASR models remains challenging, particularly for low-resource\nlanguages like Arabic, due to the scarcity of large, labeled speech datasets,\nwhich are costly and labor-intensive to produce. In this work, we employ weakly\nsupervised learning to train an Arabic ASR model using the Conformer\narchitecture. Our model is trained from scratch on 15,000 hours of weakly\nannotated speech data covering both Modern Standard Arabic (MSA) and Dialectal\nArabic (DA), eliminating the need for costly manual transcriptions. Despite the\nabsence of human-verified labels, our approach achieves state-of-the-art (SOTA)\nresults in Arabic ASR, surpassing both open and closed-source models on\nstandard benchmarks. By demonstrating the effectiveness of weak supervision as\na scalable, cost-efficient alternative to traditional supervised approaches,\npaving the way for improved ASR systems in low resource settings.\n","authors":["Mahmoud Salhab","Marwan Elghitany","Shameed Sait","Syed Sibghat Ullah","Mohammad Abusheikh","Hasan Abusheikh"],"pdf_url":"https://arxiv.org/pdf/2504.12254v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2504.14401v1","updated":"2025-04-19T20:38:09Z","published":"2025-04-19T20:38:09Z","title":"LLM-Driven Usefulness Judgment for Web Search Evaluation","summary":"  Evaluation is fundamental in optimizing search experiences and supporting\ndiverse user intents in Information Retrieval (IR). Traditional search\nevaluation methods primarily rely on relevance labels, which assess how well\nretrieved documents match a user's query. However, relevance alone fails to\ncapture a search system's effectiveness in helping users achieve their search\ngoals, making usefulness a critical evaluation criterion. In this paper, we\nexplore an alternative approach: LLM-generated usefulness labels, which\nincorporate both implicit and explicit user behavior signals to evaluate\ndocument usefulness. We propose Task-aware Rubric-based Usefulness Evaluation\n(TRUE), a rubric-driven evaluation method that employs iterative sampling and\nreasoning to model complex search behavior patterns. Our findings show that (i)\nLLMs can generate moderate usefulness labels by leveraging comprehensive search\nsession history incorporating personalization and contextual understanding, and\n(ii) fine-tuned LLMs improve usefulness judgments when provided with structured\nsearch session contexts. Additionally, we examine whether LLMs can distinguish\nbetween relevance and usefulness, particularly in cases where this divergence\nimpacts search success. We also conduct an ablation study to identify key\nmetrics for accurate usefulness label generation, optimizing for token\nefficiency and cost-effectiveness in real-world applications. This study\nadvances LLM-based usefulness evaluation by refining key user metrics,\nexploring LLM-generated label reliability, and ensuring feasibility for\nlarge-scale search systems.\n","authors":["Mouly Dewan","Jiqun Liu","Aditya Gautam","Chirag Shah"],"pdf_url":"https://arxiv.org/pdf/2504.14401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14354v2","updated":"2025-04-19T19:13:42Z","published":"2024-12-18T21:42:15Z","title":"State Space Models are Strong Text Rerankers","summary":"  Transformers dominate NLP and IR; but their inference inefficiencies and\nchallenges in extrapolating to longer contexts have sparked interest in\nalternative model architectures. Among these, state space models (SSMs) like\nMamba offer promising advantages, particularly $O(1)$ time complexity in\ninference. Despite their potential, SSMs' effectiveness at text reranking\\, --\n\\,a task requiring fine-grained query-document interaction and long-context\nunderstanding\\, -- \\,remains underexplored. This study benchmarks SSM-based\narchitectures (specifically, Mamba-1 and Mamba-2) against transformer-based\nmodels across various scales, architectures, and pre-training objectives,\nfocusing on performance and efficiency in text reranking tasks. We find that\n(1) Mamba architectures achieve competitive text ranking performance,\ncomparable to transformer-based models of similar size; (2) they are less\nefficient in training and inference compared to transformers with flash\nattention; and (3) Mamba-2 outperforms Mamba-1 in both performance and\nefficiency. These results underscore the potential of state space models as a\ntransformer alternative and highlight areas for improvement in future IR\napplications.\n","authors":["Zhichao Xu","Jinghua Yan","Ashim Gupta","Vivek Srikumar"],"pdf_url":"https://arxiv.org/pdf/2412.14354v2.pdf","comment":"Accepted to RepL4NLP 2025. The first two authors contributed equally,\n  order decided randomly"},{"id":"http://arxiv.org/abs/2504.05694v2","updated":"2025-04-19T14:01:51Z","published":"2025-04-08T05:35:38Z","title":"Large Language Models Enhanced Hyperbolic Space Recommender Systems","summary":"  Large Language Models (LLMs) have attracted significant attention in\nrecommender systems for their excellent world knowledge capabilities. However,\nexisting methods that rely on Euclidean space struggle to capture the rich\nhierarchical information inherent in textual and semantic data, which is\nessential for capturing user preferences. The geometric properties of\nhyperbolic space offer a promising solution to address this issue.\nNevertheless, integrating LLMs-based methods with hyperbolic space to\neffectively extract and incorporate diverse hierarchical information is\nnon-trivial. To this end, we propose a model-agnostic framework, named\nHyperLLM, which extracts and integrates hierarchical information from both\nstructural and semantic perspectives. Structurally, HyperLLM uses LLMs to\ngenerate multi-level classification tags with hierarchical parent-child\nrelationships for each item. Then, tag-item and user-item interactions are\njointly learned and aligned through contrastive learning, thereby providing the\nmodel with clear hierarchical information. Semantically, HyperLLM introduces a\nnovel meta-optimized strategy to extract hierarchical information from semantic\nembeddings and bridge the gap between the semantic and collaborative spaces for\nseamless integration. Extensive experiments show that HyperLLM significantly\noutperforms recommender systems based on hyperbolic space and LLMs, achieving\nperformance improvements of over 40%. Furthermore, HyperLLM not only improves\nrecommender performance but also enhances training stability, highlighting the\ncritical role of hierarchical information in recommender systems.\n","authors":["Wentao Cheng","Zhida Qin","Zexue Wu","Pengzhan Zhou","Tianyu Huang"],"pdf_url":"https://arxiv.org/pdf/2504.05694v2.pdf","comment":"Accepted as a SIGIR'25 full paper"},{"id":"http://arxiv.org/abs/2411.07770v3","updated":"2025-04-19T13:35:24Z","published":"2024-11-12T13:06:16Z","title":"A Theoretical Analysis of Recommendation Loss Functions under Negative\n  Sampling","summary":"  Loss functions like Categorical Cross Entropy (CCE), Binary Cross Entropy\n(BCE), and Bayesian Personalized Ranking (BPR) are commonly used in training\nRecommender Systems (RSs) to differentiate positive items - those interacted\nwith by users - and negative items. While prior works empirically showed that\nCCE outperforms BCE and BPR when using the full set of negative items, we\nprovide a theoretical explanation for this by proving that CCE offers the\ntightest lower bound on ranking metrics like Normalized Discounted Cumulative\nGain (NDCG) and Mean Reciprocal Rank (MRR), followed by BPR and BCE. However,\nusing the full set of negative items is computationally infeasible for\nlarge-scale RSs, prompting the use of negative sampling techniques. Under\nnegative sampling, we reveal that BPR and CCE are equivalent when a single\nnegative sample is drawn, and all three losses converge to the same global\nminimum. We further demonstrate that the sampled losses remain lower bounds for\nNDCG (MRR), albeit in a probabilistic sense. Our worst-case analysis shows that\nBCE offers the strongest bound on NDCG (MRR). Experiments on five datasets and\nfour models empirically support these theoretical findings. Our code and\nsupplementary material are available at\nhttps://github.com/federicosiciliano/recsys_losses.git.\n","authors":["Giulia Di Teodoro","Federico Siciliano","Nicola Tonellotto","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2411.07770v3.pdf","comment":"main paper 12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.05216v2","updated":"2025-04-19T13:16:08Z","published":"2025-04-07T16:03:59Z","title":"Unleashing the Power of LLMs in Dense Retrieval with Query Likelihood\n  Modeling","summary":"  Dense retrieval is a crucial task in Information Retrieval (IR) and is the\nfoundation for downstream tasks such as re-ranking. Recently, large language\nmodels (LLMs) have shown compelling semantic understanding capabilities and are\nappealing to researchers studying dense retrieval. LLMs, as decoder-style\ngenerative models, are competent at language generation while falling short on\nmodeling global information due to the lack of attention to tokens afterward.\nInspired by the classical word-based language modeling approach for IR, i.e.,\nthe query likelihood (QL) model, we seek to sufficiently utilize LLMs'\ngenerative ability by QL maximization. However, instead of ranking documents\nwith QL estimation, we introduce an auxiliary task of QL maximization to yield\na better backbone for contrastively learning a discriminative retriever. We\nname our model as LLM-QL. To condense global document semantics to a single\nvector during QL modeling, LLM-QL has two major components, Attention Stop (AS)\nand Input Corruption (IC). AS stops the attention of predictive tokens to\nprevious tokens until the ending token of the document. IC masks a portion of\ntokens in the input documents during prediction. Experiments on MSMARCO show\nthat LLM-QL can achieve significantly better performance than other LLM-based\nretrievers and using QL estimated by LLM-QL for ranking outperforms word-based\nQL by a large margin.\n","authors":["Hengran Zhang","Keping Bi","Jiafeng Guo","Xiaojie Sun","Shihao Liu","Daiting Shi","Dawei Yin","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2504.05216v2.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.14243v1","updated":"2025-04-19T09:35:11Z","published":"2025-04-19T09:35:11Z","title":"Unconstrained Monotonic Calibration of Predictions in Deep Ranking\n  Systems","summary":"  Ranking models primarily focus on modeling the relative order of predictions\nwhile often neglecting the significance of the accuracy of their absolute\nvalues. However, accurate absolute values are essential for certain downstream\ntasks, necessitating the calibration of the original predictions. To address\nthis, existing calibration approaches typically employ predefined\ntransformation functions with order-preserving properties to adjust the\noriginal predictions. Unfortunately, these functions often adhere to fixed\nforms, such as piece-wise linear functions, which exhibit limited\nexpressiveness and flexibility, thereby constraining their effectiveness in\ncomplex calibration scenarios. To mitigate this issue, we propose implementing\na calibrator using an Unconstrained Monotonic Neural Network (UMNN), which can\nlearn arbitrary monotonic functions with great modeling power. This approach\nsignificantly relaxes the constraints on the calibrator, improving its\nflexibility and expressiveness while avoiding excessively distorting the\noriginal predictions by requiring monotonicity. Furthermore, to optimize this\nhighly flexible network for calibration, we introduce a novel additional loss\nfunction termed Smooth Calibration Loss (SCLoss), which aims to fulfill a\nnecessary condition for achieving the ideal calibration state. Extensive\noffline experiments confirm the effectiveness of our method in achieving\nsuperior calibration performance. Moreover, deployment in Kuaishou's\nlarge-scale online video ranking system demonstrates that the method's\ncalibration improvements translate into enhanced business metrics. The source\ncode is available at https://github.com/baiyimeng/UMC.\n","authors":["Yimeng Bai","Shunyu Zhang","Yang Zhang","Hu Liu","Wentian Bao","Enyun Yu","Fuli Feng","Wenwu Ou"],"pdf_url":"https://arxiv.org/pdf/2504.14243v1.pdf","comment":"Accepted by SIGIR'25"},{"id":"http://arxiv.org/abs/2405.18560v4","updated":"2025-04-19T09:27:53Z","published":"2024-05-28T20:10:06Z","title":"Potential Field Based Deep Metric Learning","summary":"  Deep metric learning (DML) involves training a network to learn a\nsemantically meaningful representation space. Many current approaches mine\nn-tuples of examples and model interactions within each tuplets. We present a\nnovel, compositional DML model that instead of in tuples, represents the\ninfluence of each example (embedding) by a continuous potential field, and\nsuperposes the fields to obtain their combined global potential field. We use\nattractive/repulsive potential fields to represent interactions among\nembeddings from images of the same/different classes. Contrary to typical\nlearning methods, where mutual influence of samples is proportional to their\ndistance, we enforce reduction in such influence with distance, leading to a\ndecaying field. We show that such decay helps improve performance on real world\ndatasets with large intra-class variations and label noise. Like other\nproxy-based methods, we also use proxies to succinctly represent\nsub-populations of examples. We evaluate our method on three standard DML\nbenchmarks- Cars-196, CUB-200-2011, and SOP datasets where it outperforms\nstate-of-the-art baselines.\n","authors":["Shubhang Bhatnagar","Narendra Ahuja"],"pdf_url":"https://arxiv.org/pdf/2405.18560v4.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2504.14233v1","updated":"2025-04-19T09:06:13Z","published":"2025-04-19T09:06:13Z","title":"Template-Based Financial Report Generation in Agentic and Decomposed\n  Information Retrieval","summary":"  Tailoring structured financial reports from companies' earnings releases is\ncrucial for understanding financial performance and has been widely adopted in\nreal-world analytics. However, existing summarization methods often generate\nbroad, high-level summaries, which may lack the precision and detail required\nfor financial reports that typically focus on specific, structured sections.\nWhile Large Language Models (LLMs) hold promise, generating reports adhering to\npredefined multi-section templates remains challenging. This paper investigates\ntwo LLM-based approaches popular in industry for generating templated financial\nreports: an agentic information retrieval (IR) framework and a decomposed IR\napproach, namely AgenticIR and DecomposedIR. The AgenticIR utilizes\ncollaborative agents prompted with the full template. In contrast, the\nDecomposedIR approach applies a prompt chaining workflow to break down the\ntemplate and reframe each section as a query answered by the LLM using the\nearnings release. To quantitatively assess the generated reports, we evaluated\nboth methods in two scenarios: one using a financial dataset without direct\nhuman references, and another with a weather-domain dataset featuring\nexpert-written reports. Experimental results show that while AgenticIR may\nexcel in orchestrating tasks and generating concise reports through agent\ncollaboration, DecomposedIR statistically significantly outperforms AgenticIR\napproach in providing broader and more detailed coverage in both scenarios,\noffering reflection on the utilization of the agentic framework in real-world\napplications.\n","authors":["Yong-En Tian","Yu-Chien Tang","Kuang-Da Wang","An-Zi Yen","Wen-Chih Peng"],"pdf_url":"https://arxiv.org/pdf/2504.14233v1.pdf","comment":"5 pages; 3 figures. Accepted by SIGIR 2025 short paper track. Code\n  available at\n  https://github.com/bryant-nn/Template-Based-Financial-Report-Generation"},{"id":"http://arxiv.org/abs/2504.14214v1","updated":"2025-04-19T07:37:03Z","published":"2025-04-19T07:37:03Z","title":"Teach Me How to Denoise: A Universal Framework for Denoising Multi-modal\n  Recommender Systems via Guided Calibration","summary":"  The surge in multimedia content has led to the development of Multi-Modal\nRecommender Systems (MMRecs), which use diverse modalities such as text,\nimages, videos, and audio for more personalized recommendations. However,\nMMRecs struggle with noisy data caused by misalignment among modal content and\nthe gap between modal semantics and recommendation semantics. Traditional\ndenoising methods are inadequate due to the complexity of multi-modal data. To\naddress this, we propose a universal guided in-sync distillation denoising\nframework for multi-modal recommendation (GUIDER), designed to improve MMRecs\nby denoising user feedback. Specifically, GUIDER uses a re-calibration strategy\nto identify clean and noisy interactions from modal content. It incorporates a\nDenoising Bayesian Personalized Ranking (DBPR) loss function to handle implicit\nuser feedback. Finally, it applies a denoising knowledge distillation objective\nbased on Optimal Transport distance to guide the alignment from modality\nrepresentations to recommendation semantics. GUIDER can be seamlessly\nintegrated into existing MMRecs methods as a plug-and-play solution.\nExperimental results on four public datasets demonstrate its effectiveness and\ngeneralizability. Our source code is available at\nhttps://github.com/Neon-Jing/Guider\n","authors":["Hongji Li","Hanwen Du","Youhua Li","Junchen Fu","Chunxiao Li","Ziyi Zhuang","Jiakang Li","Yongxin Ni"],"pdf_url":"https://arxiv.org/pdf/2504.14214v1.pdf","comment":"Accepted to ACM Web Search and Data Mining (WSDM) 2025"},{"id":"http://arxiv.org/abs/2503.01776v3","updated":"2025-04-19T07:14:13Z","published":"2025-03-03T17:59:48Z","title":"Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation","summary":"  Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep\n","authors":["Tiansheng Wen","Yifei Wang","Zequn Zeng","Zhong Peng","Yudi Su","Xinyang Liu","Bo Chen","Hongwei Liu","Stefanie Jegelka","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2503.01776v3.pdf","comment":"A novel sparse coding framework designed for learning adaptive\n  representation"},{"id":"http://arxiv.org/abs/2504.14208v1","updated":"2025-04-19T06:59:34Z","published":"2025-04-19T06:59:34Z","title":"FedCIA: Federated Collaborative Information Aggregation for\n  Privacy-Preserving Recommendation","summary":"  Recommendation algorithms rely on user historical interactions to deliver\npersonalized suggestions, which raises significant privacy concerns. Federated\nrecommendation algorithms tackle this issue by combining local model training\nwith server-side model aggregation, where most existing algorithms use a\nuniform weighted summation to aggregate item embeddings from different client\nmodels. This approach has three major limitations: 1) information loss during\naggregation, 2) failure to retain personalized local features, and 3)\nincompatibility with parameter-free recommendation algorithms. To address these\nlimitations, we first review the development of recommendation algorithms and\nrecognize that their core function is to share collaborative information,\nspecifically the global relationship between users and items. With this\nunderstanding, we propose a novel aggregation paradigm named collaborative\ninformation aggregation, which focuses on sharing collaborative information\nrather than item parameters. Based on this new paradigm, we introduce the\nfederated collaborative information aggregation (FedCIA) method for\nprivacy-preserving recommendation. This method requires each client to upload\nitem similarity matrices for aggregation, which allows clients to align their\nlocal models without constraining embeddings to a unified vector space. As a\nresult, it mitigates information loss caused by direct summation, preserves the\npersonalized embedding distributions of individual clients, and supports the\naggregation of parameter-free models. Theoretical analysis and experimental\nresults on real-world datasets demonstrate the superior performance of FedCIA\ncompared with the state-of-the-art federated recommendation algorithms. Code is\navailable at https://github.com/Mingzhe-Han/FedCIA.\n","authors":["Mingzhe Han","Dongsheng Li","Jiafeng Xia","Jiahao Liu","Hansu Gu","Peng Zhang","Ning Gu","Tun Lu"],"pdf_url":"https://arxiv.org/pdf/2504.14208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.14203v1","updated":"2025-04-19T06:31:54Z","published":"2025-04-19T06:31:54Z","title":"EIoU-EMC: A Novel Loss for Domain-specific Nested Entity Recognition","summary":"  In recent years, research has mainly focused on the general NER task. There\nstill have some challenges with nested NER task in the specific domains.\nSpecifically, the scenarios of low resource and class imbalance impede the wide\napplication for biomedical and industrial domains. In this study, we design a\nnovel loss EIoU-EMC, by enhancing the implement of Intersection over Union loss\nand Multiclass loss. Our proposed method specially leverages the information of\nentity boundary and entity classification, thereby enhancing the model's\ncapacity to learn from a limited number of data samples. To validate the\nperformance of this innovative method in enhancing NER task, we conducted\nexperiments on three distinct biomedical NER datasets and one dataset\nconstructed by ourselves from industrial complex equipment maintenance\ndocuments. Comparing to strong baselines, our method demonstrates the\ncompetitive performance across all datasets. During the experimental analysis,\nour proposed method exhibits significant advancements in entity boundary\nrecognition and entity classification. Our code are available here.\n","authors":["Jian Zhang","Tianqing Zhang","Qi Li","Hongwei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.14203v1.pdf","comment":"Accepted by SIGIR'2025"},{"id":"http://arxiv.org/abs/2504.14175v1","updated":"2025-04-19T04:32:38Z","published":"2025-04-19T04:32:38Z","title":"Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query\n  Expansion","summary":"  Query expansion methods powered by large language models (LLMs) have\ndemonstrated effectiveness in zero-shot retrieval tasks. These methods assume\nthat LLMs can generate hypothetical documents that, when incorporated into a\nquery vector, enhance the retrieval of real evidence. However, we challenge\nthis assumption by investigating whether knowledge leakage in benchmarks\ncontributes to the observed performance gains. Using fact verification as a\ntestbed, we analyzed whether the generated documents contained information\nentailed by ground truth evidence and assessed their impact on performance. Our\nfindings indicate that performance improvements occurred consistently only for\nclaims whose generated documents included sentences entailed by ground truth\nevidence. This suggests that knowledge leakage may be present in these\nbenchmarks, inflating the perceived performance of LLM-based query expansion\nmethods, particularly in real-world scenarios that require retrieving niche or\nnovel knowledge.\n","authors":["Yejun Yoon","Jaeyoon Jung","Seunghyun Yoon","Kunwoo Park"],"pdf_url":"https://arxiv.org/pdf/2504.14175v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2504.14147v1","updated":"2025-04-19T02:46:10Z","published":"2025-04-19T02:46:10Z","title":"HF4Rec: Human-Like Feedback-Driven Optimization Framework for\n  Explainable Recommendation","summary":"  Recent advancements in explainable recommendation have greatly bolstered user\nexperience by elucidating the decision-making rationale. However, the existing\nmethods actually fail to provide effective feedback signals for potentially\nbetter or worse generated explanations due to their reliance on traditional\nsupervised learning paradigms in sparse interaction data. To address these\nissues, we propose a novel human-like feedback-driven optimization framework.\nThis framework employs a dynamic interactive optimization mechanism for\nachieving human-centered explainable requirements without incurring high labor\ncosts. Specifically, we propose to utilize large language models (LLMs) as\nhuman simulators to predict human-like feedback for guiding the learning\nprocess. To enable the LLMs to deeply understand the task essence and meet\nuser's diverse personalized requirements, we introduce a human-induced\ncustomized reward scoring method, which helps stimulate the language\nunderstanding and logical reasoning capabilities of LLMs. Furthermore,\nconsidering the potential conflicts between different perspectives of\nexplanation quality, we introduce a principled Pareto optimization that\ntransforms the multi-perspective quality enhancement task into a\nmulti-objective optimization problem for improving explanation performance. At\nlast, to achieve efficient model training, we design an off-policy optimization\npipeline. By incorporating a replay buffer and addressing the data distribution\nbiases, we can effectively improve data utilization and enhance model\ngenerality. Extensive experiments on four datasets demonstrate the superiority\nof our approach.\n","authors":["Jiakai Tang","Jingsen Zhang","Zihang Tian","Xueyang Feng","Lei Wang","Xu Chen"],"pdf_url":"https://arxiv.org/pdf/2504.14147v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01469v3","updated":"2025-04-19T02:36:58Z","published":"2025-03-03T12:23:54Z","title":"HeterRec: Heterogeneous Information Transformer for Scalable Sequential\n  Recommendation","summary":"  Transformer-based sequential recommendation (TSR) models have shown superior\nperformance in recommendation systems, where the quality of item\nrepresentations plays a crucial role. Classical representation methods\nintegrate item features using concatenation or neural networks to generate\nhomogeneous representation sequences. While straightforward, these methods\noverlook the heterogeneity of item features, limiting the transformer's ability\nto capture fine-grained patterns and restricting scalability. Recent studies\nhave attempted to integrate user-side heterogeneous features into item\nrepresentation sequences, but item-side heterogeneous features, which are vital\nfor performance, remain excluded. To address these challenges, we propose a\nHeterogeneous Information Transformer model for Sequential Recommendation\n(HeterRec), which incorporates Heterogeneous Token Flatten Layer (HTFL) and\nHierarchical Causal Transformer Layer (HCT). Our HTFL is a novel item\ntokenization method that converts items into a heterogeneous token set and\norganizes these tokens into heterogeneous sequences, effectively enhancing\nperformance gains when scaling up the model. Moreover, HCT introduces\ntoken-level and item-level causal transformers to extract fine-grained patterns\nfrom the heterogeneous sequences. Additionally, we design a Listwise Multi-step\nPrediction (LMP) Loss function to further improve performance. Extensive\nexperiments on both offline and online datasets show that the HeterRec model\nachieves superior performance.\n","authors":["Hao Deng","Haibo Xing","Kanefumi Matsuyama","Yulei Huang","Jinxin Hu","Hong Wen","Jia Xu","Zulong Chen","Yu Zhang","Xiaoyi Zeng","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.01469v3.pdf","comment":"6 pages, 3 figures, Proceedings of the 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR '25),\n  July 13--18, 2025, Padua, Italy"},{"id":"http://arxiv.org/abs/2504.14130v1","updated":"2025-04-19T01:14:55Z","published":"2025-04-19T01:14:55Z","title":"Personalized News Recommendation with Multi-granularity Candidate-aware\n  User Modeling","summary":"  Matching candidate news with user interests is crucial for personalized news\nrecommendations. Most existing methods can represent a user's reading interests\nthrough a single profile based on clicked news, which may not fully capture the\ndiversity of user interests. Although some approaches incorporate candidate\nnews or topic information, they remain insufficient because they neglect the\nmulti-granularity relatedness between candidate news and user interests. To\naddress this, this study proposed a multi-granularity candidate-aware user\nmodeling framework that integrated user interest features across various levels\nof granularity. It consisted of two main components: candidate news encoding\nand user modeling. A news textual information extractor and a\nknowledge-enhanced entity information extractor can capture candidate news\nfeatures, and word-level, entity-level, and news-level candidate-aware\nmechanisms can provide a comprehensive representation of user interests.\nExtensive experiments on a real-world dataset demonstrated that the proposed\nmodel could significantly outperform baseline models.\n","authors":["Qiang Li","Xinze Lin","Shenghao Lv","Faliang Huang","Xiangju Li"],"pdf_url":"https://arxiv.org/pdf/2504.14130v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2504.14301v1","updated":"2025-04-19T13:52:33Z","published":"2025-04-19T13:52:33Z","title":"Balancing Privacy and Action Performance: A Penalty-Driven Approach to\n  Image Anonymization","summary":"  The rapid development of video surveillance systems for object detection,\ntracking, activity recognition, and anomaly detection has revolutionized our\nday-to-day lives while setting alarms for privacy concerns. It isn't easy to\nstrike a balance between visual privacy and action recognition performance in\nmost computer vision models. Is it possible to safeguard privacy without\nsacrificing performance? It poses a formidable challenge, as even minor privacy\nenhancements can lead to substantial performance degradation. To address this\nchallenge, we propose a privacy-preserving image anonymization technique that\noptimizes the anonymizer using penalties from the utility branch, ensuring\nimproved action recognition performance while minimally affecting privacy\nleakage. This approach addresses the trade-off between minimizing privacy\nleakage and maintaining high action performance. The proposed approach is\nprimarily designed to align with the regulatory standards of the EU AI Act and\nGDPR, ensuring the protection of personally identifiable information while\nmaintaining action performance. To the best of our knowledge, we are the first\nto introduce a feature-based penalty scheme that exclusively controls the\naction features, allowing freedom to anonymize private attributes. Extensive\nexperiments were conducted to validate the effectiveness of the proposed\nmethod. The results demonstrate that applying a penalty to anonymizer from\nutility branch enhances action performance while maintaining nearly consistent\nprivacy leakage across different penalty settings.\n","authors":["Nazia Aslam","Kamal Nasrollahi"],"pdf_url":"https://arxiv.org/pdf/2504.14301v1.pdf","comment":"Accepted to CVPRW 2025"},{"id":"http://arxiv.org/abs/2504.14240v1","updated":"2025-04-19T09:31:37Z","published":"2025-04-19T09:31:37Z","title":"ROI-Guided Point Cloud Geometry Compression Towards Human and Machine\n  Vision","summary":"  Point cloud data is pivotal in applications like autonomous driving, virtual\nreality, and robotics. However, its substantial volume poses significant\nchallenges in storage and transmission. In order to obtain a high compression\nratio, crucial semantic details usually confront severe damage, leading to\ndifficulties in guaranteeing the accuracy of downstream tasks. To tackle this\nproblem, we are the first to introduce a novel Region of Interest (ROI)-guided\nPoint Cloud Geometry Compression (RPCGC) method for human and machine vision.\nOur framework employs a dual-branch parallel structure, where the base layer\nencodes and decodes a simplified version of the point cloud, and the\nenhancement layer refines this by focusing on geometry details. Furthermore,\nthe residual information of the enhancement layer undergoes refinement through\nan ROI prediction network. This network generates mask information, which is\nthen incorporated into the residuals, serving as a strong supervision signal.\nAdditionally, we intricately apply these mask details in the Rate-Distortion\n(RD) optimization process, with each point weighted in the distortion\ncalculation. Our loss function includes RD loss and detection loss to better\nguide point cloud encoding for the machine. Experiment results demonstrate that\nRPCGC achieves exceptional compression performance and better detection\naccuracy (10% gain) than some learning-based compression methods at high\nbitrates in ScanNet and SUN RGB-D datasets.\n","authors":["Xie Liang","Gao Wei","Zhenghui Ming","Li Ge"],"pdf_url":"https://arxiv.org/pdf/2504.14240v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.07759v3","updated":"2025-04-19T05:21:50Z","published":"2024-09-12T05:33:15Z","title":"SwinGS: Sliding Window Gaussian Splatting for Volumetric Video Streaming\n  with Arbitrary Length","summary":"  Recent advances in 3D Gaussian Splatting (3DGS) have garnered significant\nattention in computer vision and computer graphics due to its high rendering\nspeed and remarkable quality. While extant research has endeavored to extend\nthe application of 3DGS from static to dynamic scenes, such efforts have been\nconsistently impeded by excessive model sizes, constraints on video duration,\nand content deviation. These limitations significantly compromise the\nstreamability of dynamic 3D Gaussian models, thereby restricting their utility\nin downstream applications, including volumetric video, autonomous vehicle, and\nimmersive technologies such as virtual, augmented, and mixed reality.\n  This paper introduces SwinGS, a novel framework for training, delivering, and\nrendering volumetric video in a real-time streaming fashion. To address the\naforementioned challenges and enhance streamability, SwinGS integrates\nspacetime Gaussian with Markov Chain Monte Carlo (MCMC) to adapt the model to\nfit various 3D scenes across frames, in the meantime employing a sliding window\ncaptures Gaussian snapshots for each frame in an accumulative way. We implement\na prototype of SwinGS and demonstrate its streamability across various datasets\nand scenes. Additionally, we develop an interactive WebGL viewer enabling\nreal-time volumetric video playback on most devices with modern browsers,\nincluding smartphones and tablets. Experimental results show that SwinGS\nreduces transmission costs by 83.6% compared to previous work and could be\neasily scaled to volumetric videos with arbitrary length with no increasing of\nrequired GPU resources.\n","authors":["Bangya Liu","Suman Banerjee"],"pdf_url":"https://arxiv.org/pdf/2409.07759v3.pdf","comment":null}]},"2025-04-18T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2504.14098v1","updated":"2025-04-18T22:48:26Z","published":"2025-04-18T22:48:26Z","title":"Enhancing Math Learning in an LMS Using AI-Driven Question\n  Recommendations","summary":"  This paper presents an AI-driven approach to enhance math learning in a\nmodern Learning Management System (LMS) by recommending similar math questions.\nDeep embeddings for math questions are generated using Meta's\nLlama-3.2-11B-Vision-Instruct model, and three recommendation methods-cosine\nsimilarity, Self-Organizing Maps (SOM), and Gaussian Mixture Models (GMM)-are\napplied to identify similar questions. User interaction data, including session\ndurations, response times, and correctness, are used to evaluate the methods.\nOur findings suggest that while cosine similarity produces nearly identical\nquestion matches, SOM yields higher user satisfaction whereas GMM generally\nunderperforms, indicating that introducing variety to a certain degree may\nenhance engagement and thereby potential learning outcomes until variety is no\nlonger balanced reasonably, which our data about the implementations of all\nthree methods demonstrate.\n","authors":["Justus Råmunddal"],"pdf_url":"https://arxiv.org/pdf/2504.14098v1.pdf","comment":"15 pages, 9 figures, 4 tables"},{"id":"http://arxiv.org/abs/2504.13993v1","updated":"2025-04-18T17:11:38Z","published":"2025-04-18T17:11:38Z","title":"CPR: Leveraging LLMs for Topic and Phrase Suggestion to Facilitate\n  Comprehensive Product Reviews","summary":"  Consumers often heavily rely on online product reviews, analyzing both\nquantitative ratings and textual descriptions to assess product quality.\nHowever, existing research hasn't adequately addressed how to systematically\nencourage the creation of comprehensive reviews that capture both customers\nsentiment and detailed product feature analysis. This paper presents CPR, a\nnovel methodology that leverages the power of Large Language Models (LLMs) and\nTopic Modeling to guide users in crafting insightful and well-rounded reviews.\nOur approach employs a three-stage process: first, we present users with\nproduct-specific terms for rating; second, we generate targeted phrase\nsuggestions based on these ratings; and third, we integrate user-written text\nthrough topic modeling, ensuring all key aspects are addressed. We evaluate CPR\nusing text-to-text LLMs, comparing its performance against real-world customer\nreviews from Walmart. Our results demonstrate that CPR effectively identifies\nrelevant product terms, even for new products lacking prior reviews, and\nprovides sentiment-aligned phrase suggestions, saving users time and enhancing\nreviews quality. Quantitative analysis reveals a 12.3% improvement in BLEU\nscore over baseline methods, further supported by manual evaluation of\ngenerated phrases. We conclude by discussing potential extensions and future\nresearch directions.\n","authors":["Ekta Gujral","Apurva Sinha","Lishi Ji","Bijayani Sanghamitra Mishra"],"pdf_url":"https://arxiv.org/pdf/2504.13993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14113v2","updated":"2025-04-18T15:32:46Z","published":"2024-12-18T17:58:58Z","title":"Adversarial Hubness in Multi-Modal Retrieval","summary":"  Hubness is a phenomenon in high-dimensional vector spaces where a single\npoint from the natural distribution is unusually close to many other points.\nThis is a well-known problem in information retrieval that causes some items to\naccidentally (and incorrectly) appear relevant to many queries.\n  In this paper, we investigate how attackers can exploit hubness to turn any\nimage or audio input in a multi-modal retrieval system into an adversarial hub.\nAdversarial hubs can be used to inject universal adversarial content (e.g.,\nspam) that will be retrieved in response to thousands of different queries, as\nwell as for targeted attacks on queries related to specific, attacker-chosen\nconcepts.\n  We present a method for creating adversarial hubs and evaluate the resulting\nhubs on benchmark multi-modal retrieval datasets and an image-to-image\nretrieval system implemented by Pinecone, a popular vector database. For\nexample, in text-caption-to-image retrieval, a single adversarial hub,\ngenerated with respect to 100 randomly selected target queries, is retrieved as\nthe top-1 most relevant image for more than 21,000 out of 25,000 test queries\n(by contrast, the most common natural hub is the top-1 response to only 102\nqueries), demonstrating the strong generalization capabilities of adversarial\nhubs. We also investigate whether techniques for mitigating natural hubness are\nan effective defense against adversarial hubs, and show that they are not\neffective against hubs that target queries related to specific concepts.\n","authors":["Tingwei Zhang","Fnu Suya","Rishi Jha","Collin Zhang","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2412.14113v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13703v1","updated":"2025-04-18T14:03:40Z","published":"2025-04-18T14:03:40Z","title":"Consensus-aware Contrastive Learning for Group Recommendation","summary":"  Group recommendation aims to provide personalized item suggestions to a group\nof users by reflecting their collective preferences. A fundamental challenge in\nthis task is deriving a consensus that adequately represents the diverse\ninterests of individual group members. Despite advancements made by deep\nlearning-based models, existing approaches still struggle in two main areas:\n(1) Capturing consensus in small-group settings, which are more prevalent in\nreal-world applications, and (2) Balancing individual preferences with overall\ngroup performance, particularly in hypergraph-based methods that tend to\nemphasize group accuracy at the expense of personalization. To address these\nchallenges, we introduce a Consensus-aware Contrastive Learning for Group\nRecommendation (CoCoRec) that models group consensus through contrastive\nlearning. CoCoRec utilizes a transformer encoder to jointly learn user and\ngroup representations, enabling richer modeling of intra-group dynamics.\nAdditionally, the contrastive objective helps reduce overfitting from\nhigh-frequency user interactions, leading to more robust and representative\ngroup embeddings. Experiments conducted on four benchmark datasets show that\nCoCoRec consistently outperforms state-of-the-art baselines in both individual\nand group recommendation scenarios, highlighting the effectiveness of\nconsensus-aware contrastive learning in group recommendation tasks.\n","authors":["Soyoung Kim","Dongjun Lee","Jaekwang Kim"],"pdf_url":"https://arxiv.org/pdf/2504.13703v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.11758v4","updated":"2025-04-18T13:58:24Z","published":"2024-12-16T13:22:34Z","title":"Establishing a Foundation for Tetun Ad-Hoc Text Retrieval: Stemming,\n  Indexing, Retrieval, and Ranking","summary":"  Searching for information on the internet and digital platforms to satisfy an\ninformation need requires effective retrieval solutions. However, such\nsolutions are not yet available for Tetun, making it challenging to find\nrelevant documents for text-based search queries in this language. To address\nthese challenges, we investigate Tetun text retrieval with a focus on the\nad-hoc retrieval task. The study begins by developing essential language\nresources -- including a list of stopwords, a stemmer, and a test collection --\nwhich serve as foundational components for solutions tailored to Tetun text\nretrieval. Various strategies are investigated using both document titles and\ncontent to evaluate retrieval effectiveness. The results demonstrate that\nretrieving document titles, after removing hyphens and apostrophes without\napplying stemming, significantly improves retrieval performance compared to the\nbaseline. Efficiency increases by 31.37%, while effectiveness achieves an\naverage relative gain of +9.40% in MAP@10 and +30.35% in NDCG@10 with DFR BM25.\nBeyond the top-10 cutoff point, Hiemstra LM shows strong performance across\nvarious retrieval strategies and evaluation metrics. Contributions of this work\ninclude the development of Labadain-Stopwords (a list of 160 Tetun stopwords),\nLabadain-Stemmer (a Tetun stemmer with three variants), and\nLabadain-Avaliad\\'or (a Tetun test collection containing 59 topics, 33,550\ndocuments, and 5,900 qrels). We make all resources publicly accessible to\nfacilitate future research in Tetun information retrieval.\n","authors":["Gabriel de Jesus","Sérgio Nunes"],"pdf_url":"https://arxiv.org/pdf/2412.11758v4.pdf","comment":"Version 3"},{"id":"http://arxiv.org/abs/2504.13655v1","updated":"2025-04-18T12:28:38Z","published":"2025-04-18T12:28:38Z","title":"Multi-Type Context-Aware Conversational Recommender Systems via\n  Mixture-of-Experts","summary":"  Conversational recommender systems enable natural language conversations and\nthus lead to a more engaging and effective recommendation scenario. As the\nconversations for recommender systems usually contain limited contextual\ninformation, many existing conversational recommender systems incorporate\nexternal sources to enrich the contextual information. However, how to combine\ndifferent types of contextual information is still a challenge. In this paper,\nwe propose a multi-type context-aware conversational recommender system, called\nMCCRS, effectively fusing multi-type contextual information via\nmixture-of-experts to improve conversational recommender systems. MCCRS\nincorporates both structured information and unstructured information,\nincluding the structured knowledge graph, unstructured conversation history,\nand unstructured item reviews. It consists of several experts, with each expert\nspecialized in a particular domain (i.e., one specific contextual information).\nMultiple experts are then coordinated by a ChairBot to generate the final\nresults. Our proposed MCCRS model takes advantage of different contextual\ninformation and the specialization of different experts followed by a ChairBot\nbreaks the model bottleneck on a single contextual information. Experimental\nresults demonstrate that our proposed MCCRS method achieves significantly\nhigher performance compared to existing baselines.\n","authors":["Jie Zou","Cheng Lin","Weikang Guo","Zheng Wang","Jiwei Wei","Yang Yang","Hengtao Shen"],"pdf_url":"https://arxiv.org/pdf/2504.13655v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2504.13614v1","updated":"2025-04-18T10:42:16Z","published":"2025-04-18T10:42:16Z","title":"Adaptive Long-term Embedding with Denoising and Augmentation for\n  Recommendation","summary":"  The rapid growth of the internet has made personalized recommendation systems\nindispensable. Graph-based sequential recommendation systems, powered by Graph\nNeural Networks (GNNs), effectively capture complex user-item interactions but\noften face challenges such as noise and static representations. In this paper,\nwe introduce the Adaptive Long-term Embedding with Denoising and Augmentation\nfor Recommendation (ALDA4Rec) method, a novel model that constructs an\nitem-item graph, filters noise through community detection, and enriches\nuser-item interactions. Graph Convolutional Networks (GCNs) are then employed\nto learn short-term representations, while averaging, GRUs, and attention\nmechanisms are utilized to model long-term embeddings. An MLP-based adaptive\nweighting strategy is further incorporated to dynamically optimize long-term\nuser preferences. Experiments conducted on four real-world datasets demonstrate\nthat ALDA4Rec outperforms state-of-the-art baselines, delivering notable\nimprovements in both accuracy and robustness. The source code is available at\nhttps://github.com/zahraakhlaghi/ALDA4Rec.\n","authors":["Zahra Akhlaghi","Mostafa Haghir Chehreghani"],"pdf_url":"https://arxiv.org/pdf/2504.13614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11094v2","updated":"2025-04-18T10:39:23Z","published":"2025-04-15T11:40:12Z","title":"Evaluation Report on MCP Servers","summary":"  With the rise of LLMs, a large number of Model Context Protocol (MCP)\nservices have emerged since the end of 2024. However, the effectiveness and\nefficiency of MCP servers have not been well studied. To study these questions,\nwe propose an evaluation framework, called MCPBench. We selected several widely\nused MCP server and conducted an experimental evaluation on their accuracy,\ntime, and token usage. Our experiments showed that the most effective MCP, Bing\nWeb Search, achieved an accuracy of 64%. Importantly, we found that the\naccuracy of MCP servers can be substantially enhanced by involving declarative\ninterface. This research paves the way for further investigations into\noptimized MCP implementations, ultimately leading to better AI-driven\napplications and data retrieval solutions.\n","authors":["Zhiling Luo","Xiaorong Shi","Xuanrui Lin","Jinyang Gao"],"pdf_url":"https://arxiv.org/pdf/2504.11094v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13572v1","updated":"2025-04-18T09:12:46Z","published":"2025-04-18T09:12:46Z","title":"Contextualizing Spotify's Audiobook List Recommendations with\n  Descriptive Shelves","summary":"  In this paper, we propose a pipeline to generate contextualized list\nrecommendations with descriptive shelves in the domain of audiobooks. By\ncreating several shelves for topics the user has an affinity to, e.g. Uplifting\nWomen's Fiction, we can help them explore their recommendations according to\ntheir interests and at the same time recommend a diverse set of items. To do\nso, we use Large Language Models (LLMs) to enrich each item's metadata based on\na taxonomy created for this domain. Then we create diverse descriptive shelves\nfor each user. A/B tests show improvements in user engagement and audiobook\ndiscovery metrics, demonstrating benefits for users and content creators.\n","authors":["Gustavo Penha","Alice Wang","Martin Achenbach","Kristen Sheets","Sahitya Mantravadi","Remi Galvez","Nico Guetta-Jeanrenaud","Divya Narayanan","Ofeliya Kalaydzhyan","Hugues Bouchard"],"pdf_url":"https://arxiv.org/pdf/2504.13572v1.pdf","comment":"Accepted for publication in the 47th European Conference on\n  Information Retrieval (ECIR'25)"},{"id":"http://arxiv.org/abs/2411.11677v2","updated":"2025-04-18T08:15:34Z","published":"2024-11-18T15:57:14Z","title":"Few-shot Model Extraction Attacks against Sequential Recommender Systems","summary":"  Among adversarial attacks against sequential recommender systems, model\nextraction attacks represent a method to attack sequential recommendation\nmodels without prior knowledge. Existing research has primarily concentrated on\nthe adversary's execution of black-box attacks through data-free model\nextraction. However, a significant gap remains in the literature concerning the\ndevelopment of surrogate models by adversaries with access to few-shot raw data\n(10\\% even less). That is, the challenge of how to construct a surrogate model\nwith high functional similarity within the context of few-shot data scenarios\nremains an issue that requires resolution.This study addresses this gap by\nintroducing a novel few-shot model extraction framework against sequential\nrecommenders, which is designed to construct a superior surrogate model with\nthe utilization of few-shot data. The proposed few-shot model extraction\nframework is comprised of two components: an autoregressive augmentation\ngeneration strategy and a bidirectional repair loss-facilitated model\ndistillation procedure. Specifically, to generate synthetic data that closely\napproximate the distribution of raw data, autoregressive augmentation\ngeneration strategy integrates a probabilistic interaction sampler to extract\ninherent dependencies and a synthesis determinant signal module to characterize\nuser behavioral patterns. Subsequently, bidirectional repair loss, which target\nthe discrepancies between the recommendation lists, is designed as auxiliary\nloss to rectify erroneous predictions from surrogate models, transferring\nknowledge from the victim model to the surrogate model effectively. Experiments\non three datasets show that the proposed few-shot model extraction framework\nyields superior surrogate models.\n","authors":["Hui Zhang","Fu Liu"],"pdf_url":"https://arxiv.org/pdf/2411.11677v2.pdf","comment":"It requires substantial modifications.The symbols in the mathematical\n  formulas are not explained in detail"},{"id":"http://arxiv.org/abs/2502.13843v2","updated":"2025-04-18T07:48:48Z","published":"2025-02-19T16:02:59Z","title":"AgentCF++: Memory-enhanced LLM-based Agents for Popularity-aware\n  Cross-domain Recommendations","summary":"  LLM-based user agents, which simulate user interaction behavior, are emerging\nas a promising approach to enhancing recommender systems. In real-world\nscenarios, users' interactions often exhibit cross-domain characteristics and\nare influenced by others. However, the memory design in current methods causes\nuser agents to introduce significant irrelevant information during\ndecision-making in cross-domain scenarios and makes them unable to recognize\nthe influence of other users' interactions, such as popularity factors. To\ntackle this issue, we propose a dual-layer memory architecture combined with a\ntwo-step fusion mechanism. This design avoids irrelevant information during\ndecision-making while ensuring effective integration of cross-domain\npreferences. We also introduce the concepts of interest groups and group-shared\nmemory to better capture the influence of popularity factors on users with\nsimilar interests. Comprehensive experiments validate the effectiveness of\nAgentCF++. Our code is available at https://github.com/jhliu0807/AgentCF-plus.\n","authors":["Jiahao Liu","Shengkang Gu","Dongsheng Li","Guangping Zhang","Mingzhe Han","Hansu Gu","Peng Zhang","Tun Lu","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13843v2.pdf","comment":"Accepted by SIGIR 2025, 6 pages"},{"id":"http://arxiv.org/abs/2502.13845v2","updated":"2025-04-18T07:45:55Z","published":"2025-02-19T16:08:17Z","title":"Improving LLM-powered Recommendations with Personalized Information","summary":"  Due to the lack of explicit reasoning modeling, existing LLM-powered\nrecommendations fail to leverage LLMs' reasoning capabilities effectively. In\nthis paper, we propose a pipeline called CoT-Rec, which integrates two key\nChain-of-Thought (CoT) processes -- user preference analysis and item\nperception analysis -- into LLM-powered recommendations, thereby enhancing the\nutilization of LLMs' reasoning abilities. CoT-Rec consists of two stages: (1)\npersonalized information extraction, where user preferences and item perception\nare extracted, and (2) personalized information utilization, where this\ninformation is incorporated into the LLM-powered recommendation process.\nExperimental results demonstrate that CoT-Rec shows potential for improving\nLLM-powered recommendations. The implementation is publicly available at\nhttps://github.com/jhliu0807/CoT-Rec.\n","authors":["Jiahao Liu","Xueshuo Yan","Dongsheng Li","Guangping Zhang","Hansu Gu","Peng Zhang","Tun Lu","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13845v2.pdf","comment":"Accepted by SIGIR 2025, 7 pages"},{"id":"http://arxiv.org/abs/2502.13840v2","updated":"2025-04-18T07:42:28Z","published":"2025-02-19T15:59:49Z","title":"Unbiased Collaborative Filtering with Fair Sampling","summary":"  Recommender systems leverage extensive user interaction data to model\npreferences; however, directly modeling these data may introduce biases that\ndisproportionately favor popular items. In this paper, we demonstrate that\npopularity bias arises from the influence of propensity factors during\ntraining. Building on this insight, we propose a fair sampling (FS) method that\nensures each user and each item has an equal likelihood of being selected as\nboth positive and negative instances, thereby mitigating the influence of\npropensity factors. The proposed FS method does not require estimating\npropensity scores, thus avoiding the risk of failing to fully eliminate\npopularity bias caused by estimation inaccuracies. Comprehensive experiments\ndemonstrate that the proposed FS method achieves state-of-the-art performance\nin both point-wise and pair-wise recommendation tasks. The code implementation\nis available at https://github.com/jhliu0807/Fair-Sampling.\n","authors":["Jiahao Liu","Dongsheng Li","Hansu Gu","Peng Zhang","Tun Lu","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2502.13840v2.pdf","comment":"Accept by SIGIR 2025, 5 pages"},{"id":"http://arxiv.org/abs/2504.13482v1","updated":"2025-04-18T05:46:27Z","published":"2025-04-18T05:46:27Z","title":"Improving Sequential Recommenders through Counterfactual Augmentation of\n  System Exposure","summary":"  In sequential recommendation (SR), system exposure refers to items that are\nexposed to the user. Typically, only a few of the exposed items would be\ninteracted with by the user. Although SR has achieved great success in\npredicting future user interests, existing SR methods still fail to fully\nexploit system exposure data. Most methods only model items that have been\ninteracted with, while the large volume of exposed but non-interacted items is\noverlooked. Even methods that consider the whole system exposure typically\ntrain the recommender using only the logged historical system exposure, without\nexploring unseen user interests.\n  In this paper, we propose counterfactual augmentation over system exposure\nfor sequential recommendation (CaseRec). To better model historical system\nexposure, CaseRec introduces reinforcement learning to account for different\nexposure rewards. CaseRec uses a decision transformer-based sequential model to\ntake an exposure sequence as input and assigns different rewards according to\nthe user feedback. To further explore unseen user interests, CaseRec proposes\nto perform counterfactual augmentation, where exposed original items are\nreplaced with counterfactual items. Then, a transformer-based user simulator is\nproposed to predict the user feedback reward for the augmented items.\nAugmentation, together with the user simulator, constructs counterfactual\nexposure sequences to uncover new user interests. Finally, CaseRec jointly uses\nthe logged exposure sequences with the counterfactual exposure sequences to\ntrain a decision transformer-based sequential model for generating\nrecommendation. Experiments on three real-world benchmarks show the\neffectiveness of CaseRec. Our code is available at\nhttps://github.com/ZiqiZhao1/CaseRec.\n","authors":["Ziqi Zhao","Zhaochun Ren","Jiyuan Yang","Zuming Yan","Zihan Wang","Liu Yang","Pengjie Ren","Zhumin Chen","Maarten de Rijke","Xin Xin"],"pdf_url":"https://arxiv.org/pdf/2504.13482v1.pdf","comment":"accepted at SIGIR 2025 (Proceedings of the 48th International ACM\n  SIGIR Conference on Research and Development in Information Retrieval)"},{"id":"http://arxiv.org/abs/2504.12309v2","updated":"2025-04-18T05:38:41Z","published":"2025-04-05T17:11:53Z","title":"Large Language Model-Based Knowledge Graph System Construction for\n  Sustainable Development Goals: An AI-Based Speculative Design Perspective","summary":"  From 2000 to 2015, the UN's Millennium Development Goals guided global\npriorities. The subsequent Sustainable Development Goals (SDGs) adopted a more\ndynamic approach, with annual indicator updates. As 2030 nears and progress\nlags, innovative acceleration strategies are critical. This study develops an\nAI-powered knowledge graph system to analyze SDG interconnections, discover\npotential new goals, and visualize them online. Using official SDG texts,\nElsevier's keyword dataset, and 1,127 TED Talk transcripts (2020.01-2024.04), a\npilot on 269 talks from 2023 applies AI-speculative design, large language\nmodels, and retrieval-augmented generation. Key findings include: (1) Heatmap\nanalysis reveals strong associations between Goal 10 and Goal 16, and minimal\ncoverage of Goal 6. (2) In the knowledge graph, simulated dialogue over time\nreveals new central nodes, showing how richer data supports divergent thinking\nand goal clarity. (3) Six potential new goals are proposed, centered on equity,\nresilience, and technology-driven inclusion. This speculative-AI framework\noffers fresh insights for policymakers and lays groundwork for future\nmultimodal and cross-system SDG applications.\n","authors":["Yi-De Lin","Guan-Ze Liao"],"pdf_url":"https://arxiv.org/pdf/2504.12309v2.pdf","comment":"This is a minor revision: fixed a typo in the abstract (time range)\n  and corrected minor textual errors"}],"Multimedia":[{"id":"http://arxiv.org/abs/2504.14011v1","updated":"2025-04-18T18:02:33Z","published":"2025-04-18T18:02:33Z","title":"Fashion-RAG: Multimodal Fashion Image Editing via Retrieval-Augmented\n  Generation","summary":"  In recent years, the fashion industry has increasingly adopted AI\ntechnologies to enhance customer experience, driven by the proliferation of\ne-commerce platforms and virtual applications. Among the various tasks, virtual\ntry-on and multimodal fashion image editing -- which utilizes diverse input\nmodalities such as text, garment sketches, and body poses -- have become a key\narea of research. Diffusion models have emerged as a leading approach for such\ngenerative tasks, offering superior image quality and diversity. However, most\nexisting virtual try-on methods rely on having a specific garment input, which\nis often impractical in real-world scenarios where users may only provide\ntextual specifications. To address this limitation, in this work we introduce\nFashion Retrieval-Augmented Generation (Fashion-RAG), a novel method that\nenables the customization of fashion items based on user preferences provided\nin textual form. Our approach retrieves multiple garments that match the input\nspecifications and generates a personalized image by incorporating attributes\nfrom the retrieved items. To achieve this, we employ textual inversion\ntechniques, where retrieved garment images are projected into the textual\nembedding space of the Stable Diffusion text encoder, allowing seamless\nintegration of retrieved elements into the generative process. Experimental\nresults on the Dress Code dataset demonstrate that Fashion-RAG outperforms\nexisting methods both qualitatively and quantitatively, effectively capturing\nfine-grained visual details from retrieved garments. To the best of our\nknowledge, this is the first work to introduce a retrieval-augmented generation\napproach specifically tailored for multimodal fashion image editing.\n","authors":["Fulvio Sanguigni","Davide Morelli","Marcella Cornia","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2504.14011v1.pdf","comment":"IJCNN 2025"},{"id":"http://arxiv.org/abs/2504.13567v1","updated":"2025-04-18T09:10:40Z","published":"2025-04-18T09:10:40Z","title":"PoEmotion: Can AI Utilize Chinese Calligraphy to Express Emotion from\n  Poems?","summary":"  This paper presents PoEmotion, an approach to visualizing emotions in poetry\nwith Chinese calligraphy strokes. Traditional textual emotion analysis often\nlacks emotional resonance due to its mechanical nature. PoEmotion combines\nnatural language processing with deep learning generative algorithms to create\nChinese calligraphy that effectively conveys the emotions in poetry. The\ncreated calligraphy represents four fundamental emotions: excitement, anger,\nsadness, and relaxation, making the visual representation of emotions intuitive\nand concise. Furthermore, the approach delves into the relationship be-tween\ntime, emotion, and cultural communication. Its goal is to provide a more\nnatural means of communicating emotions through non-verbal mediums to enhance\nhuman emotional expression.\n","authors":["Tiancheng Liu","Anqi Wang","Xinda Chen","Jing Yan","Yin Li","Pan Hui","Kang Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.13567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13535v1","updated":"2025-04-18T07:59:35Z","published":"2025-04-18T07:59:35Z","title":"MusFlow: Multimodal Music Generation via Conditional Flow Matching","summary":"  Music generation aims to create music segments that align with human\naesthetics based on diverse conditional information. Despite advancements in\ngenerating music from specific textual descriptions (e.g., style, genre,\ninstruments), the practical application is still hindered by ordinary users'\nlimited expertise or time to write accurate prompts. To bridge this application\ngap, this paper introduces MusFlow, a novel multimodal music generation model\nusing Conditional Flow Matching. We employ multiple Multi-Layer Perceptrons\n(MLPs) to align multimodal conditional information into the audio's CLAP\nembedding space. Conditional flow matching is trained to reconstruct the\ncompressed Mel-spectrogram in the pretrained VAE latent space guided by aligned\nfeature embedding. MusFlow can generate music from images, story texts, and\nmusic captions. To collect data for model training, inspired by multi-agent\ncollaboration, we construct an intelligent data annotation workflow centered\naround a fine-tuned Qwen2-VL model. Using this workflow, we build a new\nmultimodal music dataset, MMusSet, with each sample containing a quadruple of\nimage, story text, music caption, and music piece. We conduct four sets of\nexperiments: image-to-music, story-to-music, caption-to-music, and multimodal\nmusic generation. Experimental results demonstrate that MusFlow can generate\nhigh-quality music pieces whether the input conditions are unimodal or\nmultimodal. We hope this work can advance the application of music generation\nin multimedia field, making music creation more accessible. Our generated\nsamples, code and dataset are available at musflow.github.io.\n","authors":["Jiahao Song","Yuzhao Wang"],"pdf_url":"https://arxiv.org/pdf/2504.13535v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09258v2","updated":"2025-04-18T07:21:19Z","published":"2025-04-12T15:32:16Z","title":"PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for\n  Pathology Visual-Language Tasks","summary":"  The diagnosis of pathological images is often limited by expert availability\nand regional disparities, highlighting the importance of automated diagnosis\nusing Vision-Language Models (VLMs). Traditional multimodal models typically\nemphasize outcomes over the reasoning process, compromising the reliability of\nclinical decisions. To address the weak reasoning abilities and lack of\nsupervised processes in pathological VLMs, we have innovatively proposed\nPathVLM-R1, a visual language model designed specifically for pathological\nimages. We have based our model on Qwen2.5-VL-7B-Instruct and enhanced its\nperformance for pathological tasks through meticulously designed post-training\nstrategies. Firstly, we conduct supervised fine-tuning guided by pathological\ndata to imbue the model with foundational pathological knowledge, forming a new\npathological base model. Subsequently, we introduce Group Relative Policy\nOptimization (GRPO) and propose a dual reward-driven reinforcement learning\noptimization, ensuring strict constraint on logical supervision of the\nreasoning process and accuracy of results via cross-modal process reward and\noutcome accuracy reward. In the pathological image question-answering tasks,\nthe testing results of PathVLM-R1 demonstrate a 14% improvement in accuracy\ncompared to baseline methods, and it demonstrated superior performance compared\nto the Qwen2.5-VL-32B version despite having a significantly smaller parameter\nsize. Furthermore, in out-domain data evaluation involving four medical imaging\nmodalities: Computed Tomography (CT), dermoscopy, fundus photography, and\nOptical Coherence Tomography (OCT) images: PathVLM-R1's transfer performance\nimproved by an average of 17.3% compared to traditional SFT methods. These\nresults clearly indicate that PathVLM-R1 not only enhances accuracy but also\npossesses broad applicability and expansion potential.\n","authors":["Jianyu Wu","Hao Yang","Xinhua Zeng","Guibing He","Zhiyu Chen","Zihui Li","Xiaochuan Zhang","Yangyang Ma","Run Fang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2504.09258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13469v1","updated":"2025-04-18T05:24:08Z","published":"2025-04-18T05:24:08Z","title":"HMPE:HeatMap Embedding for Efficient Transformer-Based Small Object\n  Detection","summary":"  Current Transformer-based methods for small object detection continue\nemerging, yet they have still exhibited significant shortcomings. This paper\nintroduces HeatMap Position Embedding (HMPE), a novel Transformer Optimization\ntechnique that enhances object detection performance by dynamically integrating\npositional encoding with semantic detection information through heatmap-guided\nadaptive learning.We also innovatively visualize the HMPE method, offering\nclear visualization of embedded information for parameter fine-tuning.We then\ncreate Multi-Scale ObjectBox-Heatmap Fusion Encoder (MOHFE) and HeatMap Induced\nHigh-Quality Queries for Decoder (HIDQ) modules. These are designed for the\nencoder and decoder, respectively, to generate high-quality queries and reduce\nbackground noise queries.Using both heatmap embedding and Linear-Snake\nConv(LSConv) feature engineering, we enhance the embedding of massively diverse\nsmall object categories and reduced the decoder multihead layers, thereby\naccelerating both inference and training.In the generalization experiments, our\napproach outperforme the baseline mAP by 1.9% on the small object dataset (NWPU\nVHR-10) and by 1.2% on the general dataset (PASCAL VOC). By employing\nHMPE-enhanced embedding, we are able to reduce the number of decoder layers\nfrom eight to a minimum of three, significantly decreasing both inference and\ntraining costs.\n","authors":["YangChen Zeng"],"pdf_url":"https://arxiv.org/pdf/2504.13469v1.pdf","comment":null}]},"2025-04-17T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2311.10776v6","updated":"2025-04-17T22:42:04Z","published":"2023-11-16T01:21:33Z","title":"Chemist-X: Large Language Model-empowered Agent for Reaction Condition\n  Recommendation in Chemical Synthesis","summary":"  Recent AI research plots a promising future of automatic chemical reactions\nwithin the chemistry society. This study proposes Chemist-X, a comprehensive AI\nagent that automates the reaction condition optimization (RCO) task in chemical\nsynthesis with retrieval-augmented generation (RAG) technology and\nAI-controlled wet-lab experiment executions. To begin with, as an emulation on\nhow chemical experts solve the RCO task, Chemist-X utilizes a novel RAG scheme\nto interrogate available molecular and literature databases to narrow the\nsearching space for later processing. The agent then leverages a computer-aided\ndesign (CAD) tool we have developed through a large language model (LLM)\nsupervised programming interface. With updated chemical knowledge obtained via\nRAG, as well as the ability in using CAD tools, our agent significantly\noutperforms conventional RCO AIs confined to the fixed knowledge within its\ntraining data. Finally, Chemist-X interacts with the physical world through an\nautomated robotic system, which can validate the suggested chemical reaction\ncondition without human interventions. The control of the robotic system was\nachieved with a novel algorithm we have developed for the equipment, which\nrelies on LLMs for reliable script generation. Results of our automatic wet-lab\nexperiments, achieved by fully LLM-supervised end-to-end operation with no\nhuman in the lope, prove Chemist-X's ability in self-driving laboratories.\n","authors":["Kexin Chen","Jiamin Lu","Junyou Li","Xiaoran Yang","Yuyang Du","Kunyi Wang","Qiannuan Shi","Jiahui Yu","Lanqing Li","Jiezhong Qiu","Jianzhang Pan","Yi Huang","Qun Fang","Pheng Ann Heng","Guangyong Chen"],"pdf_url":"https://arxiv.org/pdf/2311.10776v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.04629v3","updated":"2025-04-17T21:33:22Z","published":"2024-12-05T21:51:05Z","title":"Argumentative Experience: Reducing Confirmation Bias on Controversial\n  Issues through LLM-Generated Multi-Persona Debates","summary":"  Large language models (LLMs) are enabling designers to give life to exciting\nnew user experiences for information access. In this work, we present a system\nthat generates LLM personas to debate a topic of interest from different\nperspectives. How might information seekers use and benefit from such a system?\nCan centering information access around diverse viewpoints help to mitigate\nthorny challenges like confirmation bias in which information seekers\nover-trust search results matching existing beliefs? How do potential biases\nand hallucinations in LLMs play out alongside human users who are also fallible\nand possibly biased?\n  Our study exposes participants to multiple viewpoints on controversial issues\nvia a mixed-methods, within-subjects study. We use eye-tracking metrics to\nquantitatively assess cognitive engagement alongside qualitative feedback.\nCompared to a baseline search system, we see more creative interactions and\ndiverse information-seeking with our multi-persona debate system, which more\neffectively reduces user confirmation bias and conviction toward their initial\nbeliefs. Overall, our study contributes to the emerging design space of\nLLM-based information access systems, specifically investigating the potential\nof simulated personas to promote greater exposure to information diversity,\nemulate collective intelligence, and mitigate bias in information seeking.\n","authors":["Li Shi","Houjiang Liu","Yian Wong","Utkarsh Mujumdar","Dan Zhang","Jacek Gwizdka","Matthew Lease"],"pdf_url":"https://arxiv.org/pdf/2412.04629v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13172v1","updated":"2025-04-17T17:59:27Z","published":"2025-04-17T17:59:27Z","title":"SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework\n  with MLLMs","summary":"  Cross-modal retrieval (CMR) is a fundamental task in multimedia research,\nfocused on retrieving semantically relevant targets across different\nmodalities. While traditional CMR methods match text and image via\nembedding-based similarity calculations, recent advancements in pre-trained\ngenerative models have established generative retrieval as a promising\nalternative. This paradigm assigns each target a unique identifier and\nleverages a generative model to directly predict identifiers corresponding to\ninput queries without explicit indexing. Despite its great potential, current\ngenerative CMR approaches still face semantic information insufficiency in both\nidentifier construction and generation processes. To address these limitations,\nwe propose a novel unified Semantic-enhanced generative Cross-mOdal REtrieval\nframework (SemCORE), designed to unleash the semantic understanding\ncapabilities in generative cross-modal retrieval task. Specifically, we first\nconstruct a Structured natural language IDentifier (SID) that effectively\naligns target identifiers with generative models optimized for natural language\ncomprehension and generation. Furthermore, we introduce a Generative Semantic\nVerification (GSV) strategy enabling fine-grained target discrimination.\nAdditionally, to the best of our knowledge, SemCORE is the first framework to\nsimultaneously consider both text-to-image and image-to-text retrieval tasks\nwithin generative cross-modal retrieval. Extensive experiments demonstrate that\nour framework outperforms state-of-the-art generative cross-modal retrieval\nmethods. Notably, SemCORE achieves substantial improvements across benchmark\ndatasets, with an average increase of 8.65 points in Recall@1 for text-to-image\nretrieval.\n","authors":["Haoxuan Li","Yi Bin","Yunshan Ma","Guoqing Wang","Yang Yang","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2504.13172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13128v1","updated":"2025-04-17T17:44:06Z","published":"2025-04-17T17:44:06Z","title":"FreshStack: Building Realistic Benchmarks for Evaluating Retrieval on\n  Technical Documents","summary":"  We introduce FreshStack, a reusable framework for automatically building\ninformation retrieval (IR) evaluation benchmarks from community-asked questions\nand answers. FreshStack conducts the following steps: (1) automatic corpus\ncollection from code and technical documentation, (2) nugget generation from\ncommunity-asked questions and answers, and (3) nugget-level support, retrieving\ndocuments using a fusion of retrieval techniques and hybrid architectures. We\nuse FreshStack to build five datasets on fast-growing, recent, and niche topics\nto ensure the tasks are sufficiently challenging. On FreshStack, existing\nretrieval models, when applied out-of-the-box, significantly underperform\noracle approaches on all five topics, denoting plenty of headroom to improve IR\nquality. In addition, we identify cases where rerankers do not clearly improve\nfirst-stage retrieval accuracy (two out of five topics). We hope that\nFreshStack will facilitate future work toward constructing realistic, scalable,\nand uncontaminated IR and RAG evaluation benchmarks. FreshStack datasets are\navailable at: https://fresh-stack.github.io.\n","authors":["Nandan Thakur","Jimmy Lin","Sam Havens","Michael Carbin","Omar Khattab","Andrew Drozdov"],"pdf_url":"https://arxiv.org/pdf/2504.13128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04228v2","updated":"2025-04-17T17:23:08Z","published":"2024-11-06T19:50:00Z","title":"dsld: A Socially Relevant Tool for Teaching Statistics","summary":"  The growing power of data science can play a crucial role in addressing\nsocial discrimination, necessitating nuanced understanding and effective\nmitigation strategies for biases. \"Data Science Looks At Discrimination\" (DSLD)\nis an R and Python package designed to provide users with a comprehensive\ntoolkit of statistical and graphical methods for assessing possible\ndiscrimination related to protected groups such as race, gender, and age. The\npackage addresses critical issues by identifying and mitigating confounders and\nreducing bias against protected groups in prediction algorithms.\n  In educational settings, DSLD offers instructors powerful tools to teach\nstatistical principles through motivating real world examples of discrimination\nanalysis. The inclusion of an 80 page Quarto book further supports users from\nstatistics educators to legal professionals in effectively applying these\nanalytical tools to real world scenarios.\n","authors":["Taha Abdullah","Arjun Ashok","Brandon Zarate","Shubhada Martha","Billy Ouattara","Norman Matloff","Aditya Mittal"],"pdf_url":"https://arxiv.org/pdf/2411.04228v2.pdf","comment":"To be submitted to journal"},{"id":"http://arxiv.org/abs/2504.13095v1","updated":"2025-04-17T17:01:17Z","published":"2025-04-17T17:01:17Z","title":"Should We Tailor the Talk? Understanding the Impact of Conversational\n  Styles on Preference Elicitation in Conversational Recommender Systems","summary":"  Conversational recommender systems (CRSs) provide users with an interactive\nmeans to express preferences and receive real-time personalized\nrecommendations. The success of these systems is heavily influenced by the\npreference elicitation process. While existing research mainly focuses on what\nquestions to ask during preference elicitation, there is a notable gap in\nunderstanding what role broader interaction patterns including tone, pacing,\nand level of proactiveness play in supporting users in completing a given task.\nThis study investigates the impact of different conversational styles on\npreference elicitation, task performance, and user satisfaction with CRSs. We\nconducted a controlled experiment in the context of scientific literature\nrecommendation, contrasting two distinct conversational styles, high\ninvolvement (fast paced, direct, and proactive with frequent prompts) and high\nconsiderateness (polite and accommodating, prioritizing clarity and user\ncomfort) alongside a flexible experimental condition where users could switch\nbetween the two. Our results indicate that adapting conversational strategies\nbased on user expertise and allowing flexibility between styles can enhance\nboth user satisfaction and the effectiveness of recommendations in CRSs.\nOverall, our findings hold important implications for the design of future\nCRSs.\n","authors":["Ivica Kostric","Krisztian Balog","Ujwal Gadiraju"],"pdf_url":"https://arxiv.org/pdf/2504.13095v1.pdf","comment":"To appear in: Proceedings of the 33rd ACM Conference on User\n  Modeling, Adaptation and Personalization (UMAP '25), June 16--19, 2025, New\n  York City, NY, USA"},{"id":"http://arxiv.org/abs/2504.13032v1","updated":"2025-04-17T15:41:39Z","published":"2025-04-17T15:41:39Z","title":"InstructRAG: Leveraging Retrieval-Augmented Generation on Instruction\n  Graphs for LLM-Based Task Planning","summary":"  Recent advancements in large language models (LLMs) have enabled their use as\nagents for planning complex tasks. Existing methods typically rely on a\nthought-action-observation (TAO) process to enhance LLM performance, but these\napproaches are often constrained by the LLMs' limited knowledge of complex\ntasks. Retrieval-augmented generation (RAG) offers new opportunities by\nleveraging external databases to ground generation in retrieved information. In\nthis paper, we identify two key challenges (enlargability and transferability)\nin applying RAG to task planning. We propose InstructRAG, a novel solution\nwithin a multi-agent meta-reinforcement learning framework, to address these\nchallenges. InstructRAG includes a graph to organize past instruction paths\n(sequences of correct actions), an RL-Agent with Reinforcement Learning to\nexpand graph coverage for enlargability, and an ML-Agent with Meta-Learning to\nimprove task generalization for transferability. The two agents are trained\nend-to-end to optimize overall planning performance. Our experiments on four\nwidely used task planning datasets demonstrate that InstructRAG significantly\nenhances performance and adapts efficiently to new tasks, achieving up to a\n19.2% improvement over the best existing approach.\n","authors":["Zheng Wang","Shu Xian Teo","Jun Jie Chew","Wei Shi"],"pdf_url":"https://arxiv.org/pdf/2504.13032v1.pdf","comment":"This paper has been accepted by SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.12920v1","updated":"2025-04-17T13:10:56Z","published":"2025-04-17T13:10:56Z","title":"CSMF: Cascaded Selective Mask Fine-Tuning for Multi-Objective\n  Embedding-Based Retrieval","summary":"  Multi-objective embedding-based retrieval (EBR) has become increasingly\ncritical due to the growing complexity of user behaviors and commercial\nobjectives. While traditional approaches often suffer from data sparsity and\nlimited information sharing between objectives, recent methods utilizing a\nshared network alongside dedicated sub-networks for each objective partially\naddress these limitations. However, such methods significantly increase the\nmodel parameters, leading to an increased retrieval latency and a limited\nability to model causal relationships between objectives. To address these\nchallenges, we propose the Cascaded Selective Mask Fine-Tuning (CSMF), a novel\nmethod that enhances both retrieval efficiency and serving performance for\nmulti-objective EBR. The CSMF framework selectively masks model parameters to\nfree up independent learning space for each objective, leveraging the cascading\nrelationships between objectives during the sequential fine-tuning. Without\nincreasing network parameters or online retrieval overhead, CSMF computes a\nlinearly weighted fusion score for multiple objective probabilities while\nsupporting flexible adjustment of each objective's weight across various\nrecommendation scenarios. Experimental results on real-world datasets\ndemonstrate the superior performance of CSMF, and online experiments validate\nits significant practical value.\n","authors":["Hao Deng","Haibo Xing","Kanefumi Matsuyama","Moyu Zhang","Jinxin Hu","Hong Wen","Yu Zhang","Xiaoyi Zeng","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.12920v1.pdf","comment":"10 pages, 8 figures, Proceedings of the 48th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR '25),\n  July 13--18, 2025, Padua, Italy"},{"id":"http://arxiv.org/abs/2504.12915v1","updated":"2025-04-17T13:05:14Z","published":"2025-04-17T13:05:14Z","title":"ConExion: Concept Extraction with Large Language Models","summary":"  In this paper, an approach for concept extraction from documents using\npre-trained large language models (LLMs) is presented. Compared with\nconventional methods that extract keyphrases summarizing the important\ninformation discussed in a document, our approach tackles a more challenging\ntask of extracting all present concepts related to the specific domain, not\njust the important ones. Through comprehensive evaluations of two widely used\nbenchmark datasets, we demonstrate that our method improves the F1 score\ncompared to state-of-the-art techniques. Additionally, we explore the potential\nof using prompts within these models for unsupervised concept extraction. The\nextracted concepts are intended to support domain coverage evaluation of\nontologies and facilitate ontology learning, highlighting the effectiveness of\nLLMs in concept extraction tasks. Our source code and datasets are publicly\navailable at https://github.com/ISE-FIZKarlsruhe/concept_extraction.\n","authors":["Ebrahim Norouzi","Sven Hertling","Harald Sack"],"pdf_url":"https://arxiv.org/pdf/2504.12915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12900v1","updated":"2025-04-17T12:41:41Z","published":"2025-04-17T12:41:41Z","title":"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct\n  Preference Optimization","summary":"  Personalized outfit generation aims to construct a set of compatible and\npersonalized fashion items as an outfit. Recently, generative AI models have\nreceived widespread attention, as they can generate fashion items for users to\ncomplete an incomplete outfit or create a complete outfit. However, they have\nlimitations in terms of lacking diversity and relying on the supervised\nlearning paradigm. Recognizing this gap, we propose a novel framework\nFashionDPO, which fine-tunes the fashion outfit generation model using direct\npreference optimization. This framework aims to provide a general fine-tuning\napproach to fashion generative models, refining a pre-trained fashion outfit\ngeneration model using automatically generated feedback, without the need to\ndesign a task-specific reward function. To make sure that the feedback is\ncomprehensive and objective, we design a multi-expert feedback generation\nmodule which covers three evaluation perspectives, \\ie quality, compatibility\nand personalization. Experiments on two established datasets, \\ie iFashion and\nPolyvore-U, demonstrate the effectiveness of our framework in enhancing the\nmodel's ability to align with users' personalized preferences while adhering to\nfashion compatibility principles. Our code and model checkpoints are available\nat https://github.com/Yzcreator/FashionDPO.\n","authors":["Mingzhe Yu","Yunshan Ma","Lei Wu","Changshuo Wang","Xue Li","Lei Meng"],"pdf_url":"https://arxiv.org/pdf/2504.12900v1.pdf","comment":"Accepted by SIGIR'25"},{"id":"http://arxiv.org/abs/2504.12879v1","updated":"2025-04-17T12:11:14Z","published":"2025-04-17T12:11:14Z","title":"Building Russian Benchmark for Evaluation of Information Retrieval\n  Models","summary":"  We introduce RusBEIR, a comprehensive benchmark designed for zero-shot\nevaluation of information retrieval (IR) models in the Russian language.\nComprising 17 datasets from various domains, it integrates adapted, translated,\nand newly created datasets, enabling systematic comparison of lexical and\nneural models. Our study highlights the importance of preprocessing for lexical\nmodels in morphologically rich languages and confirms BM25 as a strong baseline\nfor full-document retrieval. Neural models, such as mE5-large and BGE-M3,\ndemonstrate superior performance on most datasets, but face challenges with\nlong-document retrieval due to input size constraints. RusBEIR offers a\nunified, open-source framework that promotes research in Russian-language\ninformation retrieval.\n","authors":["Grigory Kovalev","Mikhail Tikhomirov","Evgeny Kozhevnikov","Max Kornilov","Natalia Loukachevitch"],"pdf_url":"https://arxiv.org/pdf/2504.12879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12229v3","updated":"2025-04-17T11:50:59Z","published":"2024-10-16T04:44:34Z","title":"Comprehending Knowledge Graphs with Large Language Models for\n  Recommender Systems","summary":"  In recent years, the introduction of knowledge graphs (KGs) has significantly\nadvanced recommender systems by facilitating the discovery of potential\nassociations between items. However, existing methods still face several\nlimitations. First, most KGs suffer from missing facts or limited scopes.\nSecond, existing methods convert textual information in KGs into IDs, resulting\nin the loss of natural semantic connections between different items. Third,\nexisting methods struggle to capture high-order connections in the global KG.\nTo address these limitations, we propose a novel method called CoLaKG, which\nleverages large language models (LLMs) to improve KG-based recommendations. The\nextensive knowledge and remarkable reasoning capabilities of LLMs enable our\nmethod to supplement missing facts in KGs, and their powerful text\nunderstanding abilities allow for better utilization of semantic information.\nSpecifically, CoLaKG extracts useful information from KGs at both local and\nglobal levels. By employing the item-centered subgraph extraction and prompt\nengineering, it can accurately understand the local information. In addition,\nthrough the semantic-based retrieval module, each item is enriched by related\nitems from the entire knowledge graph, effectively harnessing global\ninformation. Furthermore, the local and global information are effectively\nintegrated into the recommendation model through a representation fusion module\nand a retrieval-augmented representation learning module, respectively.\nExtensive experiments on four real-world datasets demonstrate the superiority\nof our method.\n","authors":["Ziqiang Cui","Yunpeng Weng","Xing Tang","Fuyuan Lyu","Dugang Liu","Xiuqiang He","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2410.12229v3.pdf","comment":"Accepted as a full paper by SIGIR'25"},{"id":"http://arxiv.org/abs/2504.12778v1","updated":"2025-04-17T09:18:58Z","published":"2025-04-17T09:18:58Z","title":"Towards Lossless Token Pruning in Late-Interaction Retrieval Models","summary":"  Late interaction neural IR models like ColBERT offer a competitive\neffectiveness-efficiency trade-off across many benchmarks. However, they\nrequire a huge memory space to store the contextual representation for all the\ndocument tokens. Some works have proposed using either heuristics or\nstatistical-based techniques to prune tokens from each document. This however\ndoesn't guarantee that the removed tokens have no impact on the retrieval\nscore. Our work uses a principled approach to define how to prune tokens\nwithout impacting the score between a document and a query. We introduce three\nregularization losses, that induce a solution with high pruning ratios, as well\nas two pruning strategies. We study them experimentally (in and out-domain),\nshowing that we can preserve ColBERT's performance while using only 30\\% of the\ntokens.\n","authors":["Yuxuan Zong","Benjamin Piwowarski"],"pdf_url":"https://arxiv.org/pdf/2504.12778v1.pdf","comment":"Accepted at SIGIR 2025 Full Paper Track"},{"id":"http://arxiv.org/abs/2504.12732v1","updated":"2025-04-17T08:14:45Z","published":"2025-04-17T08:14:45Z","title":"Validating LLM-Generated Relevance Labels for Educational Resource\n  Search","summary":"  Manual relevance judgements in Information Retrieval are costly and require\nexpertise, driving interest in using Large Language Models (LLMs) for automatic\nassessment. While LLMs have shown promise in general web search scenarios,\ntheir effectiveness for evaluating domain-specific search results, such as\neducational resources, remains unexplored. To investigate different ways of\nincluding domain-specific criteria in LLM prompts for relevance judgement, we\ncollected and released a dataset of 401 human relevance judgements from a user\nstudy involving teaching professionals performing search tasks related to\nlesson planning. We compared three approaches to structuring these prompts: a\nsimple two-aspect evaluation baseline from prior work on using LLMs as\nrelevance judges, a comprehensive 12-dimensional rubric derived from\neducational literature, and criteria directly informed by the study\nparticipants. Using domain-specific frameworks, LLMs achieved strong agreement\nwith human judgements (Cohen's $\\kappa$ up to 0.650), significantly\noutperforming the baseline approach. The participant-derived framework proved\nparticularly robust, with GPT-3.5 achieving $\\kappa$ scores of 0.639 and 0.613\nfor 10-dimension and 5-dimension versions respectively. System-level evaluation\nshowed that LLM judgements reliably identified top-performing retrieval\napproaches (RBO scores 0.71-0.76) while maintaining reasonable discrimination\nbetween systems (RBO 0.52-0.56). These findings suggest that LLMs can\neffectively evaluate educational resources when prompted with domain-specific\ncriteria, though performance varies with framework complexity and input\nstructure.\n","authors":["Ratan J. Sebastian","Anett Hoppe"],"pdf_url":"https://arxiv.org/pdf/2504.12732v1.pdf","comment":"Presented in the LLM4Eval Workshop Co-located with WSDM '25 in\n  Hannover, Germany"},{"id":"http://arxiv.org/abs/2504.12722v1","updated":"2025-04-17T07:57:23Z","published":"2025-04-17T07:57:23Z","title":"SimUSER: Simulating User Behavior with Large Language Models for\n  Recommender System Evaluation","summary":"  Recommender systems play a central role in numerous real-life applications,\nyet evaluating their performance remains a significant challenge due to the gap\nbetween offline metrics and online behaviors. Given the scarcity and limits\n(e.g., privacy issues) of real user data, we introduce SimUSER, an agent\nframework that serves as believable and cost-effective human proxies. SimUSER\nfirst identifies self-consistent personas from historical data, enriching user\nprofiles with unique backgrounds and personalities. Then, central to this\nevaluation are users equipped with persona, memory, perception, and brain\nmodules, engaging in interactions with the recommender system. SimUSER exhibits\ncloser alignment with genuine humans than prior work, both at micro and macro\nlevels. Additionally, we conduct insightful experiments to explore the effects\nof thumbnails on click rates, the exposure effect, and the impact of reviews on\nuser engagement. Finally, we refine recommender system parameters based on\noffline A/B test results, resulting in improved user engagement in the real\nworld.\n","authors":["Nicolas Bougie","Narimasa Watanabe"],"pdf_url":"https://arxiv.org/pdf/2504.12722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11509v2","updated":"2025-04-17T06:10:27Z","published":"2025-04-15T09:53:02Z","title":"PATFinger: Prompt-Adapted Transferable Fingerprinting against\n  Unauthorized Multimodal Dataset Usage","summary":"  The multimodal datasets can be leveraged to pre-train large-scale\nvision-language models by providing cross-modal semantics. Current endeavors\nfor determining the usage of datasets mainly focus on single-modal dataset\nownership verification through intrusive methods and non-intrusive techniques,\nwhile cross-modal approaches remain under-explored. Intrusive methods can adapt\nto multimodal datasets but degrade model accuracy, while non-intrusive methods\nrely on label-driven decision boundaries that fail to guarantee stable\nbehaviors for verification. To address these issues, we propose a novel\nprompt-adapted transferable fingerprinting scheme from a training-free\nperspective, called PATFinger, which incorporates the global optimal\nperturbation (GOP) and the adaptive prompts to capture dataset-specific\ndistribution characteristics. Our scheme utilizes inherent dataset attributes\nas fingerprints instead of compelling the model to learn triggers. The GOP is\nderived from the sample distribution to maximize embedding drifts between\ndifferent modalities. Subsequently, our PATFinger re-aligns the adaptive prompt\nwith GOP samples to capture the cross-modal interactions on the carefully\ncrafted surrogate model. This allows the dataset owner to check the usage of\ndatasets by observing specific prediction behaviors linked to the PATFinger\nduring retrieval queries. Extensive experiments demonstrate the effectiveness\nof our scheme against unauthorized multimodal dataset usage on various\ncross-modal retrieval architectures by 30% over state-of-the-art baselines.\n","authors":["Wenyi Zhang","Ju Jia","Xiaojun Jia","Yihao Huang","Xinfeng Li","Cong Wu","Lina Wang"],"pdf_url":"https://arxiv.org/pdf/2504.11509v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.12973v3","updated":"2025-04-17T05:48:09Z","published":"2023-03-23T00:42:48Z","title":"Uncertainty Calibration for Counterfactual Propensity Estimation in\n  Recommendation","summary":"  Post-click conversion rate (CVR) is a reliable indicator of online customers'\npreferences, making it crucial for developing recommender systems. A major\nchallenge in predicting CVR is severe selection bias, arising from users'\ninherent self-selection behavior and the system's item selection process. To\nmitigate this issue, the inverse propensity score (IPS) is employed to weight\nthe prediction error of each observed instance. However, current propensity\nscore estimations are unreliable due to the lack of a quality measure. To\naddress this, we evaluate the quality of propensity scores from the perspective\nof uncertainty calibration, proposing the use of Expected Calibration Error\n(ECE) as a measure of propensity-score quality, which quantifies the extent to\nwhich predicted probabilities are overconfident by assessing the difference\nbetween predicted probabilities and actual observed frequencies. Miscalibrated\npropensity scores can lead to distorted IPS weights, thereby compromising the\ndebiasing process in CVR prediction. In this paper, we introduce a\nmodel-agnostic calibration framework for propensity-based debiasing of CVR\npredictions. Theoretical analysis on bias and generalization bounds\ndemonstrates the superiority of calibrated propensity estimates over\nuncalibrated ones. Experiments conducted on the Coat, Yahoo and KuaiRand\ndatasets show improved uncertainty calibration, as evidenced by lower ECE\nvalues, leading to enhanced CVR prediction outcomes.\n","authors":["Wenbo Hu","Xin Sun","Qiang liu","Le Wu","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2303.12973v3.pdf","comment":"This is the accepted manuscript version of the IEEE TKDE paper. The\n  final published version will be available at:\n  https://doi.ieeecomputersociety.org/10.1109/TKDE.2025.3552658"},{"id":"http://arxiv.org/abs/2504.12558v1","updated":"2025-04-17T01:13:21Z","published":"2025-04-17T01:13:21Z","title":"Benchmarking LLM-based Relevance Judgment Methods","summary":"  Large Language Models (LLMs) are increasingly deployed in both academic and\nindustry settings to automate the evaluation of information seeking systems,\nparticularly by generating graded relevance judgments. Previous work on\nLLM-based relevance assessment has primarily focused on replicating graded\nhuman relevance judgments through various prompting strategies. However, there\nhas been limited exploration of alternative assessment methods or comprehensive\ncomparative studies. In this paper, we systematically compare multiple\nLLM-based relevance assessment methods, including binary relevance judgments,\ngraded relevance assessments, pairwise preference-based methods, and two\nnugget-based evaluation methods~--~document-agnostic and document-dependent. In\naddition to a traditional comparison based on system rankings using Kendall\ncorrelations, we also examine how well LLM judgments align with human\npreferences, as inferred from relevance grades. We conduct extensive\nexperiments on datasets from three TREC Deep Learning tracks 2019, 2020 and\n2021 as well as the ANTIQUE dataset, which focuses on non-factoid open-domain\nquestion answering. As part of our data release, we include relevance judgments\ngenerated by both an open-source (Llama3.2b) and a commercial (gpt-4o) model.\nOur goal is to \\textit{reproduce} various LLM-based relevance judgment methods\nto provide a comprehensive comparison. All code, data, and resources are\npublicly available in our GitHub Repository at\nhttps://github.com/Narabzad/llm-relevance-judgement-comparison.\n","authors":["Negar Arabzadeh","Charles L. A. Clarke"],"pdf_url":"https://arxiv.org/pdf/2504.12558v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2504.13351v1","updated":"2025-04-17T21:31:23Z","published":"2025-04-17T21:31:23Z","title":"Chain-of-Modality: Learning Manipulation Programs from Multimodal Human\n  Videos with Vision-Language-Models","summary":"  Learning to perform manipulation tasks from human videos is a promising\napproach for teaching robots. However, many manipulation tasks require changing\ncontrol parameters during task execution, such as force, which visual data\nalone cannot capture. In this work, we leverage sensing devices such as\narmbands that measure human muscle activities and microphones that record\nsound, to capture the details in the human manipulation process, and enable\nrobots to extract task plans and control parameters to perform the same task.\nTo achieve this, we introduce Chain-of-Modality (CoM), a prompting strategy\nthat enables Vision Language Models to reason about multimodal human\ndemonstration data -- videos coupled with muscle or audio signals. By\nprogressively integrating information from each modality, CoM refines a task\nplan and generates detailed control parameters, enabling robots to perform\nmanipulation tasks based on a single multimodal human video prompt. Our\nexperiments show that CoM delivers a threefold improvement in accuracy for\nextracting task plans and control parameters compared to baselines, with strong\ngeneralization to new task setups and objects in real-world robot experiments.\nVideos and code are available at https://chain-of-modality.github.io\n","authors":["Chen Wang","Fei Xia","Wenhao Yu","Tingnan Zhang","Ruohan Zhang","C. Karen Liu","Li Fei-Fei","Jie Tan","Jacky Liang"],"pdf_url":"https://arxiv.org/pdf/2504.13351v1.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2411.16750v2","updated":"2025-04-17T18:32:36Z","published":"2024-11-24T05:07:10Z","title":"PriorDiffusion: Leverage Language Prior in Diffusion Models for\n  Monocular Depth Estimation","summary":"  Traditional monocular depth estimation suffers from inherent ambiguity and\nvisual nuisance. We argue that language prior can enhance monocular depth\nestimation by leveraging the inductive bias learned during the text-to-image\npre-training of diffusion models. The ability of these models to generate\nimages that align with text indicates that they have learned the spatial\nrelationships, size, and shape of specified objects, which can be applied to\nimprove depth estimation. Thus, we propose PriorDiffusion, using a pre-trained\ntext-to-image diffusion model that takes both images and corresponding text\ndescriptions to infer affine-invariant depth through a denoising process. We\nalso show that language prior enhances the model's perception of specific\nregions of images that users care about and describe. Simultaneously, language\nprior acts as a constraint to accelerate the convergence of both training and\nthe inference diffusion trajectory. By training on HyperSim and Virtual KITTI,\nwe achieve faster training convergence, fewer inference diffusion steps, and\nstate-of-the-art zero-shot performance across NYUv2, KITTI, ETH3D, and ScanNet.\nCode will be released upon acceptance.\n","authors":["Ziyao Zeng","Jingcheng Ni","Daniel Wang","Patrick Rim","Younjoon Chung","Fengyu Yang","Byung-Woo Hong","Alex Wong"],"pdf_url":"https://arxiv.org/pdf/2411.16750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13172v1","updated":"2025-04-17T17:59:27Z","published":"2025-04-17T17:59:27Z","title":"SemCORE: A Semantic-Enhanced Generative Cross-Modal Retrieval Framework\n  with MLLMs","summary":"  Cross-modal retrieval (CMR) is a fundamental task in multimedia research,\nfocused on retrieving semantically relevant targets across different\nmodalities. While traditional CMR methods match text and image via\nembedding-based similarity calculations, recent advancements in pre-trained\ngenerative models have established generative retrieval as a promising\nalternative. This paradigm assigns each target a unique identifier and\nleverages a generative model to directly predict identifiers corresponding to\ninput queries without explicit indexing. Despite its great potential, current\ngenerative CMR approaches still face semantic information insufficiency in both\nidentifier construction and generation processes. To address these limitations,\nwe propose a novel unified Semantic-enhanced generative Cross-mOdal REtrieval\nframework (SemCORE), designed to unleash the semantic understanding\ncapabilities in generative cross-modal retrieval task. Specifically, we first\nconstruct a Structured natural language IDentifier (SID) that effectively\naligns target identifiers with generative models optimized for natural language\ncomprehension and generation. Furthermore, we introduce a Generative Semantic\nVerification (GSV) strategy enabling fine-grained target discrimination.\nAdditionally, to the best of our knowledge, SemCORE is the first framework to\nsimultaneously consider both text-to-image and image-to-text retrieval tasks\nwithin generative cross-modal retrieval. Extensive experiments demonstrate that\nour framework outperforms state-of-the-art generative cross-modal retrieval\nmethods. Notably, SemCORE achieves substantial improvements across benchmark\ndatasets, with an average increase of 8.65 points in Recall@1 for text-to-image\nretrieval.\n","authors":["Haoxuan Li","Yi Bin","Yunshan Ma","Guoqing Wang","Yang Yang","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2504.13172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09012v2","updated":"2025-04-17T17:14:09Z","published":"2025-01-15T18:56:22Z","title":"Multimodal LLMs Can Reason about Aesthetics in Zero-Shot","summary":"  The rapid progress of generative art has democratized the creation of\nvisually pleasing imagery. However, achieving genuine artistic impact - the\nkind that resonates with viewers on a deeper, more meaningful level - requires\na sophisticated aesthetic sensibility. This sensibility involves a\nmulti-faceted reasoning process extending beyond mere visual appeal, which is\noften overlooked by current computational models. This paper pioneers an\napproach to capture this complex process by investigating how the reasoning\ncapabilities of Multimodal LLMs (MLLMs) can be effectively elicited for\naesthetic judgment. Our analysis reveals a critical challenge: MLLMs exhibit a\ntendency towards hallucinations during aesthetic reasoning, characterized by\nsubjective opinions and unsubstantiated artistic interpretations. We further\ndemonstrate that these limitations can be overcome by employing an\nevidence-based, objective reasoning process, as substantiated by our proposed\nbaseline, ArtCoT. MLLMs prompted by this principle produce multi-faceted and\nin-depth aesthetic reasoning that aligns significantly better with human\njudgment. These findings have direct applications in areas such as AI art\ntutoring and as reward models for generative art. Ultimately, our work paves\nthe way for AI systems that can truly understand, appreciate, and generate\nartworks that align with the sensible human aesthetic standard.\n","authors":["Ruixiang Jiang","Changwen Chen"],"pdf_url":"https://arxiv.org/pdf/2501.09012v2.pdf","comment":"WIP, Homepage https://github.com/songrise/MLLM4Art"},{"id":"http://arxiv.org/abs/2504.13072v1","updated":"2025-04-17T16:33:39Z","published":"2025-04-17T16:33:39Z","title":"HiScene: Creating Hierarchical 3D Scenes with Isometric View Generation","summary":"  Scene-level 3D generation represents a critical frontier in multimedia and\ncomputer graphics, yet existing approaches either suffer from limited object\ncategories or lack editing flexibility for interactive applications. In this\npaper, we present HiScene, a novel hierarchical framework that bridges the gap\nbetween 2D image generation and 3D object generation and delivers high-fidelity\nscenes with compositional identities and aesthetic scene content. Our key\ninsight is treating scenes as hierarchical \"objects\" under isometric views,\nwhere a room functions as a complex object that can be further decomposed into\nmanipulatable items. This hierarchical approach enables us to generate 3D\ncontent that aligns with 2D representations while maintaining compositional\nstructure. To ensure completeness and spatial alignment of each decomposed\ninstance, we develop a video-diffusion-based amodal completion technique that\neffectively handles occlusions and shadows between objects, and introduce shape\nprior injection to ensure spatial coherence within the scene. Experimental\nresults demonstrate that our method produces more natural object arrangements\nand complete object instances suitable for interactive applications, while\nmaintaining physical plausibility and alignment with user inputs.\n","authors":["Wenqi Dong","Bangbang Yang","Zesong Yang","Yuan Li","Tao Hu","Hujun Bao","Yuewen Ma","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2504.13072v1.pdf","comment":"Project webpage: https://zju3dv.github.io/hiscene/"},{"id":"http://arxiv.org/abs/2501.08514v3","updated":"2025-04-17T15:16:30Z","published":"2025-01-15T01:52:54Z","title":"Multimodal Fake News Video Explanation: Dataset, Analysis and Evaluation","summary":"  Multimodal fake news videos are difficult to interpret because they require\ncomprehensive consideration of the correlation and consistency between multiple\nmodes. Existing methods deal with fake news videos as a classification problem,\nbut it's not clear why news videos are identified as fake. Without proper\nexplanation, the end user may not understand the underlying meaning of the\nfalsehood. Therefore, we propose a new problem - Fake news video Explanation\n(FNVE) - given a multimodal news post containing a video and title, our goal is\nto generate natural language explanations to reveal the falsity of the news\nvideo. To that end, we developed FakeVE, a new dataset of 2,672 fake news video\nposts that can definitively explain four real-life fake news video aspects. In\norder to understand the characteristics of fake news video explanation, we\nconducted an exploratory analysis of FakeVE from different perspectives. In\naddition, we propose a Multimodal Relation Graph Transformer (MRGT) based on\nthe architecture of multimodal Transformer to benchmark FakeVE. The empirical\nresults show that the results of the various benchmarks (adopted by FakeVE) are\nconvincing and provide a detailed analysis of the differences in explanation\ngeneration of the benchmark models.\n","authors":["Lizhi Chen","Zhong Qian","Peifeng Li","Qiaoming Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.08514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12900v1","updated":"2025-04-17T12:41:41Z","published":"2025-04-17T12:41:41Z","title":"FashionDPO:Fine-tune Fashion Outfit Generation Model using Direct\n  Preference Optimization","summary":"  Personalized outfit generation aims to construct a set of compatible and\npersonalized fashion items as an outfit. Recently, generative AI models have\nreceived widespread attention, as they can generate fashion items for users to\ncomplete an incomplete outfit or create a complete outfit. However, they have\nlimitations in terms of lacking diversity and relying on the supervised\nlearning paradigm. Recognizing this gap, we propose a novel framework\nFashionDPO, which fine-tunes the fashion outfit generation model using direct\npreference optimization. This framework aims to provide a general fine-tuning\napproach to fashion generative models, refining a pre-trained fashion outfit\ngeneration model using automatically generated feedback, without the need to\ndesign a task-specific reward function. To make sure that the feedback is\ncomprehensive and objective, we design a multi-expert feedback generation\nmodule which covers three evaluation perspectives, \\ie quality, compatibility\nand personalization. Experiments on two established datasets, \\ie iFashion and\nPolyvore-U, demonstrate the effectiveness of our framework in enhancing the\nmodel's ability to align with users' personalized preferences while adhering to\nfashion compatibility principles. Our code and model checkpoints are available\nat https://github.com/Yzcreator/FashionDPO.\n","authors":["Mingzhe Yu","Yunshan Ma","Lei Wu","Changshuo Wang","Xue Li","Lei Meng"],"pdf_url":"https://arxiv.org/pdf/2504.12900v1.pdf","comment":"Accepted by SIGIR'25"},{"id":"http://arxiv.org/abs/2504.12809v1","updated":"2025-04-17T10:15:10Z","published":"2025-04-17T10:15:10Z","title":"Saliency-Aware Diffusion Reconstruction for Effective Invisible\n  Watermark Removal","summary":"  As digital content becomes increasingly ubiquitous, the need for robust\nwatermark removal techniques has grown due to the inadequacy of existing\nembedding techniques, which lack robustness. This paper introduces a novel\nSaliency-Aware Diffusion Reconstruction (SADRE) framework for watermark\nelimination on the web, combining adaptive noise injection, region-specific\nperturbations, and advanced diffusion-based reconstruction. SADRE disrupts\nembedded watermarks by injecting targeted noise into latent representations\nguided by saliency masks although preserving essential image features. A\nreverse diffusion process ensures high-fidelity image restoration, leveraging\nadaptive noise levels determined by watermark strength. Our framework is\ntheoretically grounded with stability guarantees and achieves robust watermark\nremoval across diverse scenarios. Empirical evaluations on state-of-the-art\n(SOTA) watermarking techniques demonstrate SADRE's superiority in balancing\nwatermark disruption and image quality. SADRE sets a new benchmark for\nwatermark elimination, offering a flexible and reliable solution for real-world\nweb content. Code is available\non~\\href{https://github.com/inzamamulDU/SADRE}{\\textbf{https://github.com/inzamamulDU/SADRE}}.\n","authors":["Inzamamul Alam","Md Tanvir Islam","Simon S. Woo"],"pdf_url":"https://arxiv.org/pdf/2504.12809v1.pdf","comment":"Accepted at The Web Conference 2025"},{"id":"http://arxiv.org/abs/2504.12796v1","updated":"2025-04-17T09:58:38Z","published":"2025-04-17T09:58:38Z","title":"A Survey on Cross-Modal Interaction Between Music and Multimodal Data","summary":"  Multimodal learning has driven innovation across various industries,\nparticularly in the field of music. By enabling more intuitive interaction\nexperiences and enhancing immersion, it not only lowers the entry barriers to\nthe music but also increases its overall appeal. This survey aims to provide a\ncomprehensive review of multimodal tasks related to music, outlining how music\ncontributes to multimodal learning and offering insights for researchers\nseeking to expand the boundaries of computational music. Unlike text and\nimages, which are often semantically or visually intuitive, music primarily\ninteracts with humans through auditory perception, making its data\nrepresentation inherently less intuitive. Therefore, this paper first\nintroduces the representations of music and provides an overview of music\ndatasets. Subsequently, we categorize cross-modal interactions between music\nand multimodal data into three types: music-driven cross-modal interactions,\nmusic-oriented cross-modal interactions, and bidirectional music cross-modal\ninteractions. For each category, we systematically trace the development of\nrelevant sub-tasks, analyze existing limitations, and discuss emerging trends.\nFurthermore, we provide a comprehensive summary of datasets and evaluation\nmetrics used in multimodal tasks related to music, offering benchmark\nreferences for future research. Finally, we discuss the current challenges in\ncross-modal interactions involving music and propose potential directions for\nfuture research.\n","authors":["Sifei Li","Mining Tan","Feier Shen","Minyan Luo","Zijiao Yin","Fan Tang","Weiming Dong","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2504.12796v1.pdf","comment":"34 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.07521v2","updated":"2025-04-17T09:34:26Z","published":"2025-04-10T07:33:49Z","title":"Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal\n  Large Language Models","summary":"  Most existing emotion analysis emphasizes which emotion arises (e.g., happy,\nsad, angry) but neglects the deeper why. We propose Emotion Interpretation\n(EI), focusing on causal factors-whether explicit (e.g., observable objects,\ninterpersonal interactions) or implicit (e.g., cultural context, off-screen\nevents)-that drive emotional responses. Unlike traditional emotion recognition,\nEI tasks require reasoning about triggers instead of mere labeling. To\nfacilitate EI research, we present EIBench, a large-scale benchmark\nencompassing 1,615 basic EI samples and 50 complex EI samples featuring\nmultifaceted emotions. Each instance demands rationale-based explanations\nrather than straightforward categorization. We further propose a Coarse-to-Fine\nSelf-Ask (CFSA) annotation pipeline, which guides Vision-Language Models\n(VLLMs) through iterative question-answer rounds to yield high-quality labels\nat scale. Extensive evaluations on open-source and proprietary large language\nmodels under four experimental settings reveal consistent performance\ngaps-especially for more intricate scenarios-underscoring EI's potential to\nenrich empathetic, context-aware AI applications. Our benchmark and methods are\npublicly available at: https://github.com/Lum1104/EIBench, offering a\nfoundation for advanced multimodal causal analysis and next-generation\naffective computing.\n","authors":["Yuxiang Lin","Jingdong Sun","Zhi-Qi Cheng","Jue Wang","Haomin Liang","Zebang Cheng","Yifei Dong","Jun-Yan He","Xiaojiang Peng","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2504.07521v2.pdf","comment":"Accepted at CVPR Workshop NEXD 2025. 21 pages, Project:\n  https://github.com/Lum1104/EIBench"},{"id":"http://arxiv.org/abs/2410.10291v4","updated":"2025-04-17T08:31:14Z","published":"2024-10-14T08:45:35Z","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal\n  Perspective","summary":"  Accurate interpretation and visualization of human instructions are crucial\nfor text-to-image (T2I) synthesis. However, current models struggle to capture\nsemantic variations from word order changes, and existing evaluations, relying\non indirect metrics like text-image similarity, fail to reliably assess these\nchallenges. This often obscures poor performance on complex or uncommon\nlinguistic patterns by the focus on frequent word combinations. To address\nthese deficiencies, we propose a novel metric called SemVarEffect and a\nbenchmark named SemVarBench, designed to evaluate the causality between\nsemantic variations in inputs and outputs in T2I synthesis. Semantic variations\nare achieved through two types of linguistic permutations, while avoiding\neasily predictable literal variations. Experiments reveal that the\nCogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1.\nSemantic variations in object relations are less understood than attributes,\nscoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in\nUNet or Transformers plays a crucial role in handling semantic variations, a\nfactor previously overlooked by a focus on textual encoders. Our work\nestablishes an effective evaluation framework that advances the T2I synthesis\ncommunity's exploration of human instruction understanding. Our benchmark and\ncode are available at https://github.com/zhuxiangru/SemVarBench .\n","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"pdf_url":"https://arxiv.org/pdf/2410.10291v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2409.14319v2","updated":"2025-04-17T07:35:40Z","published":"2024-09-22T05:13:11Z","title":"Scene-Text Grounding for Text-Based Video Question Answering","summary":"  Existing efforts in text-based video question answering (TextVideoQA) are\ncriticized for their opaque decisionmaking and heavy reliance on scene-text\nrecognition. In this paper, we propose to study Grounded TextVideoQA by forcing\nmodels to answer questions and spatio-temporally localize the relevant\nscene-text regions, thus decoupling QA from scenetext recognition and promoting\nresearch towards interpretable QA. The task has three-fold significance. First,\nit encourages scene-text evidence versus other short-cuts for answer\npredictions. Second, it directly accepts scene-text regions as visual answers,\nthus circumventing the problem of ineffective answer evaluation by stringent\nstring matching. Third, it isolates the challenges inherited in VideoQA and\nscene-text recognition. This enables the diagnosis of the root causes for\nfailure predictions, e.g., wrong QA or wrong scene-text recognition? To achieve\nGrounded TextVideoQA, we propose the T2S-QA model that highlights a\ndisentangled temporal-to-spatial contrastive learning strategy for\nweakly-supervised scene-text grounding and grounded TextVideoQA. To facilitate\nevaluation, we construct a new dataset ViTXT-GQA which features 52K scene-text\nbounding boxes within 2.2K temporal segments related to 2K questions and 729\nvideos. With ViTXT-GQA, we perform extensive experiments and demonstrate the\nsevere limitations of existing techniques in Grounded TextVideoQA. While T2S-QA\nachieves superior results, the large performance gap with human leaves ample\nspace for improvement. Our further analysis of oracle scene-text inputs posits\nthat the major challenge is scene-text recognition. To advance the research of\nGrounded TextVideoQA, our dataset and code are at\nhttps://github.com/zhousheng97/ViTXT-GQA.git\n","authors":["Sheng Zhou","Junbin Xiao","Xun Yang","Peipei Song","Dan Guo","Angela Yao","Meng Wang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2409.14319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12704v1","updated":"2025-04-17T07:17:49Z","published":"2025-04-17T07:17:49Z","title":"SmartFreeEdit: Mask-Free Spatial-Aware Image Editing with Complex\n  Instruction Understanding","summary":"  Recent advancements in image editing have utilized large-scale multimodal\nmodels to enable intuitive, natural instruction-driven interactions. However,\nconventional methods still face significant challenges, particularly in spatial\nreasoning, precise region segmentation, and maintaining semantic consistency,\nespecially in complex scenes. To overcome these challenges, we introduce\nSmartFreeEdit, a novel end-to-end framework that integrates a multimodal large\nlanguage model (MLLM) with a hypergraph-enhanced inpainting architecture,\nenabling precise, mask-free image editing guided exclusively by natural\nlanguage instructions. The key innovations of SmartFreeEdit include:(1)the\nintroduction of region aware tokens and a mask embedding paradigm that enhance\nthe spatial understanding of complex scenes;(2) a reasoning segmentation\npipeline designed to optimize the generation of editing masks based on natural\nlanguage instructions;and (3) a hypergraph-augmented inpainting module that\nensures the preservation of both structural integrity and semantic coherence\nduring complex edits, overcoming the limitations of local-based image\ngeneration. Extensive experiments on the Reason-Edit benchmark demonstrate that\nSmartFreeEdit surpasses current state-of-the-art methods across multiple\nevaluation metrics, including segmentation accuracy, instruction adherence, and\nvisual quality preservation, while addressing the issue of local information\nfocus and improving global consistency in the edited image. Our project will be\navailable at https://github.com/smileformylove/SmartFreeEdit.\n","authors":["Qianqian Sun","Jixiang Luo","Dell Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2504.12704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.13218v1","updated":"2025-04-17T06:35:01Z","published":"2025-04-17T06:35:01Z","title":"Harmony: A Unified Framework for Modality Incremental Learning","summary":"  Incremental learning aims to enable models to continuously acquire knowledge\nfrom evolving data streams while preserving previously learned capabilities.\nWhile current research predominantly focuses on unimodal incremental learning\nand multimodal incremental learning where the modalities are consistent,\nreal-world scenarios often present data from entirely new modalities, posing\nadditional challenges. This paper investigates the feasibility of developing a\nunified model capable of incremental learning across continuously evolving\nmodal sequences. To this end, we introduce a novel paradigm called Modality\nIncremental Learning (MIL), where each learning stage involves data from\ndistinct modalities. To address this task, we propose a novel framework named\nHarmony, designed to achieve modal alignment and knowledge retention, enabling\nthe model to reduce the modal discrepancy and learn from a sequence of distinct\nmodalities, ultimately completing tasks across multiple modalities within a\nunified framework. Our approach introduces the adaptive compatible feature\nmodulation and cumulative modal bridging. Through constructing historical modal\nfeatures and performing modal knowledge accumulation and alignment, the\nproposed components collaboratively bridge modal differences and maintain\nknowledge retention, even with solely unimodal data available at each learning\nphase.These components work in concert to establish effective modality\nconnections and maintain knowledge retention, even when only unimodal data is\navailable at each learning stage. Extensive experiments on the MIL task\ndemonstrate that our proposed method significantly outperforms existing\nincremental learning methods, validating its effectiveness in MIL scenarios.\n","authors":["Yaguang Song","Xiaoshan Yang","Dongmei Jiang","Yaowei Wang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2504.13218v1.pdf","comment":null}]},"2025-04-16T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.05667v3","updated":"2025-04-16T23:13:25Z","published":"2024-08-11T01:14:13Z","title":"PhishLang: A Real-Time, Fully Client-Side Phishing Detection Framework\n  Using MobileBERT","summary":"  In this paper, we introduce PhishLang, the first fully client-side\nanti-phishing framework built on a lightweight ensemble framework that utilizes\nadvanced language models to analyze the contextual features of a website's\nsource code and URL. Unlike traditional heuristic or machine learning\napproaches that rely on static features and struggle to adapt to evolving\nthreats, or deep learning models that are computationally intensive, our\napproach utilizes MobileBERT, a fast and memory-efficient variant of the BERT\narchitecture, to capture nuanced features indicative of phishing attacks. To\nfurther enhance detection accuracy, PhishLang employs a multi-modal ensemble\napproach, combining both the URL and Source detection models. This architecture\nensures robustness by allowing one model to compensate for scenarios where the\nother may fail, or if both models provide ambiguous inferences. As a result,\nPhishLang excels at detecting both regular and evasive phishing threats,\nincluding zero-day attacks, outperforming popular anti-phishing tools, while\noperating without relying on external blocklists and safeguarding user privacy\nby ensuring that browser history remains entirely local and unshared. We\nrelease PhishLang as a Chromium browser extension and also open-source the\nframework to aid the research community.\n","authors":["Sayak Saha Roy","Shirin Nilizadeh"],"pdf_url":"https://arxiv.org/pdf/2408.05667v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10816v2","updated":"2025-04-16T21:45:16Z","published":"2025-04-15T02:31:34Z","title":"CSPLADE: Learned Sparse Retrieval with Causal Language Models","summary":"  In recent years, dense retrieval has been the focus of information retrieval\n(IR) research. While effective, dense retrieval produces uninterpretable dense\nvectors, and suffers from the drawback of large index size. Learned sparse\nretrieval (LSR) has emerged as promising alternative, achieving competitive\nretrieval performance while also being able to leverage the classical inverted\nindex data structure for efficient retrieval. However, limited works have\nexplored scaling LSR beyond BERT scale. In this work, we identify two\nchallenges in training large language models (LLM) for LSR: (1) training\ninstability during the early stage of contrastive training; (2) suboptimal\nperformance due to pre-trained LLM's unidirectional attention. To address these\nchallenges, we propose two corresponding techniques: (1) a lightweight\nadaptation training phase to eliminate training instability; (2) two model\nvariants to enable bidirectional information. With these techniques, we are\nable to train LSR models with 8B scale LLM, and achieve competitive retrieval\nperformance with reduced index size. Furthermore, we are among the first to\nanalyze the performance-efficiency tradeoff of LLM-based LSR model through the\nlens of model quantization. Our findings provide insights into adapting LLMs\nfor efficient retrieval modeling.\n","authors":["Zhichao Xu","Aosong Feng","Yijun Tian","Haibo Ding","Lin Lee Cheong"],"pdf_url":"https://arxiv.org/pdf/2504.10816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08672v3","updated":"2025-04-16T18:59:52Z","published":"2024-04-05T05:14:46Z","title":"Taxonomy and Analysis of Sensitive User Queries in Generative AI Search","summary":"  Although there has been a growing interest among industries in integrating\ngenerative LLMs into their services, limited experience and scarcity of\nresources act as a barrier in launching and servicing large-scale LLM-based\nservices. In this paper, we share our experiences in developing and operating\ngenerative AI models within a national-scale search engine, with a specific\nfocus on the sensitiveness of user queries. We propose a taxonomy for sensitive\nsearch queries, outline our approaches, and present a comprehensive analysis\nreport on sensitive queries from actual users. We believe that our experiences\nin launching generative AI search systems can contribute to reducing the\nbarrier in building generative LLM-based services.\n","authors":["Hwiyeol Jo","Taiwoo Park","Hyunwoo Lee","Nayoung Choi","Changbong Kim","Ohjoon Kwon","Donghyeon Jeon","Eui-Hyeon Lee","Kyoungho Shin","Sun Suk Lim","Kyungmi Kim","Jihye Lee","Sun Kim"],"pdf_url":"https://arxiv.org/pdf/2404.08672v3.pdf","comment":"NAACL2025(Findings), corrected typo in co-corresponding authors"},{"id":"http://arxiv.org/abs/2410.20056v2","updated":"2025-04-16T18:17:24Z","published":"2024-10-26T03:07:22Z","title":"Multi-Field Adaptive Retrieval","summary":"  Document retrieval for tasks such as search and retrieval-augmented\ngeneration typically involves datasets that are unstructured: free-form text\nwithout explicit internal structure in each document. However, documents can\nhave a structured form, consisting of fields such as an article title, message\nbody, or HTML header. To address this gap, we introduce Multi-Field Adaptive\nRetrieval (MFAR), a flexible framework that accommodates any number of and any\ntype of document indices on structured data. Our framework consists of two main\nsteps: (1) the decomposition of an existing document into fields, each indexed\nindependently through dense and lexical methods, and (2) learning a model which\nadaptively predicts the importance of a field by conditioning on the document\nquery, allowing on-the-fly weighting of the most likely field(s). We find that\nour approach allows for the optimized use of dense versus lexical\nrepresentations across field types, significantly improves in document ranking\nover a number of existing retrievers, and achieves state-of-the-art performance\nfor multi-field structured data.\n","authors":["Millicent Li","Tongfei Chen","Benjamin Van Durme","Patrick Xia"],"pdf_url":"https://arxiv.org/pdf/2410.20056v2.pdf","comment":"ICLR 2025, Spotlight"},{"id":"http://arxiv.org/abs/2504.12408v1","updated":"2025-04-16T18:17:19Z","published":"2025-04-16T18:17:19Z","title":"A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based\n  Relevance Judgment","summary":"  Large Language Models (LLMs) are increasingly used to automate relevance\njudgments for information retrieval (IR) tasks, often demonstrating agreement\nwith human labels that approaches inter-human agreement. To assess the\nrobustness and reliability of LLM-based relevance judgments, we systematically\ninvestigate impact of prompt sensitivity on the task. We collected prompts for\nrelevance assessment from 15 human experts and 15 LLMs across three tasks~ --\n~binary, graded, and pairwise~ -- ~yielding 90 prompts in total. After\nfiltering out unusable prompts from three humans and three LLMs, we employed\nthe remaining 72 prompts with three different LLMs as judges to label\ndocument/query pairs from two TREC Deep Learning Datasets (2020 and 2021). We\ncompare LLM-generated labels with TREC official human labels using Cohen's\n$\\kappa$ and pairwise agreement measures. In addition to investigating the\nimpact of prompt variations on agreement with human labels, we compare human-\nand LLM-generated prompts and analyze differences among different LLMs as\njudges. We also compare human- and LLM-generated prompts with the standard\nUMBRELA prompt used for relevance assessment by Bing and TREC 2024 Retrieval\nAugmented Generation (RAG) Track. To support future research in LLM-based\nevaluation, we release all data and prompts at\nhttps://github.com/Narabzad/prompt-sensitivity-relevance-judgements/.\n","authors":["Negar Arabzadeh","Charles L. A . Clarke"],"pdf_url":"https://arxiv.org/pdf/2504.12408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.12113v1","updated":"2025-04-16T14:21:02Z","published":"2025-04-16T14:21:02Z","title":"Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting\n  Methods for Clarification Generation","summary":"  In information retrieval (IR), providing appropriate clarifications to better\nunderstand users' information needs is crucial for building a proactive\nsearch-oriented dialogue system. Due to the strong in-context learning ability\nof large language models (LLMs), recent studies investigate prompting methods\nto generate clarifications using few-shot or Chain of Thought (CoT) prompts.\nHowever, vanilla CoT prompting does not distinguish the characteristics of\ndifferent information needs, making it difficult to understand how LLMs resolve\nambiguities in user queries. In this work, we focus on the concept of ambiguity\nfor clarification, seeking to model and integrate ambiguities in the\nclarification process. To this end, we comprehensively study the impact of\nprompting schemes based on reasoning and ambiguity for clarification. The idea\nis to enhance the reasoning abilities of LLMs by limiting CoT to predict first\nambiguity types that can be interpreted as instructions to clarify, then\ncorrespondingly generate clarifications. We name this new prompting scheme\nAmbiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various\ndatasets containing human-annotated clarifying questions to compare AT-CoT with\nmultiple baselines. We also perform user simulations to implicitly measure the\nquality of generated clarifications under various IR scenarios.\n","authors":["Anfu Tang","Laure Soulier","Vincent Guigue"],"pdf_url":"https://arxiv.org/pdf/2504.12113v1.pdf","comment":"11 pages, 3 figures. Accepted at SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.12063v1","updated":"2025-04-16T13:18:16Z","published":"2025-04-16T13:18:16Z","title":"Optimizing Compound Retrieval Systems","summary":"  Modern retrieval systems do not rely on a single ranking model to construct\ntheir rankings. Instead, they generally take a cascading approach where a\nsequence of ranking models are applied in multiple re-ranking stages. Thereby,\nthey balance the quality of the top-K ranking with computational costs by\nlimiting the number of documents each model re-ranks. However, the cascading\napproach is not the only way models can interact to form a retrieval system.\n  We propose the concept of compound retrieval systems as a broader class of\nretrieval systems that apply multiple prediction models. This encapsulates\ncascading models but also allows other types of interactions than top-K\nre-ranking. In particular, we enable interactions with large language models\n(LLMs) which can provide relative relevance comparisons. We focus on the\noptimization of compound retrieval system design which uniquely involves\nlearning where to apply the component models and how to aggregate their\npredictions into a final ranking. This work shows how our compound approach can\ncombine the classic BM25 retrieval model with state-of-the-art (pairwise) LLM\nrelevance predictions, while optimizing a given ranking metric and efficiency\ntarget. Our experimental results show optimized compound retrieval systems\nprovide better trade-offs between effectiveness and efficiency than cascading\napproaches, even when applied in a self-supervised manner.\n  With the introduction of compound retrieval systems, we hope to inspire the\ninformation retrieval field to more out-of-the-box thinking on how prediction\nmodels can interact to form rankings.\n","authors":["Harrie Oosterhuis","Rolf Jagerman","Zhen Qin","Xuanhui Wang"],"pdf_url":"https://arxiv.org/pdf/2504.12063v1.pdf","comment":"SIGIR 2025"},{"id":"http://arxiv.org/abs/2504.12007v1","updated":"2025-04-16T12:01:03Z","published":"2025-04-16T12:01:03Z","title":"Generative Recommendation with Continuous-Token Diffusion","summary":"  In recent years, there has been a significant trend toward using large\nlanguage model (LLM)-based recommender systems (RecSys). Current research\nprimarily focuses on representing complex user-item interactions within a\ndiscrete space to align with the inherent discrete nature of language models.\nHowever, this approach faces limitations due to its discrete nature: (i)\ninformation is often compressed during discretization; (ii) the tokenization\nand generation for the vast number of users and items in real-world scenarios\nare constrained by a limited vocabulary. Embracing continuous data presents a\npromising alternative to enhance expressive capabilities, though this approach\nis still in its early stages. To address this gap, we propose a novel\nframework, DeftRec, which incorporates \\textbf{de}noising di\\textbf{f}fusion\nmodels to enable LLM-based RecSys to seamlessly support continuous\n\\textbf{t}oken as input and target. First, we introduce a robust tokenizer with\na masking operation and an additive K-way architecture to index users and\nitems, capturing their complex collaborative relationships into continuous\ntokens. Crucially, we develop a denoising diffusion model to process user\npreferences within continuous domains by conditioning on reasoning content from\npre-trained large language model. During the denoising process, we reformulate\nthe objective to include negative interactions, building a comprehensive\nunderstanding of user preferences for effective and accurate recommendation\ngeneration. Finally, given a continuous token as output, recommendations can be\neasily generated through score-based retrieval. Extensive experiments\ndemonstrate the effectiveness of the proposed methods, showing that DeftRec\nsurpasses competitive benchmarks, including both traditional and emerging\nLLM-based RecSys.\n","authors":["Haohao Qu","Wenqi Fan","Shanru Lin"],"pdf_url":"https://arxiv.org/pdf/2504.12007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22675v2","updated":"2025-04-16T10:20:11Z","published":"2025-03-28T17:59:03Z","title":"Think Before Recommend: Unleashing the Latent Reasoning Power for\n  Sequential Recommendation","summary":"  Sequential Recommendation (SeqRec) aims to predict the next item by capturing\nsequential patterns from users' historical interactions, playing a crucial role\nin many real-world recommender systems. However, existing approaches\npredominantly adopt a direct forward computation paradigm, where the final\nhidden state of the sequence encoder serves as the user representation. We\nargue that this inference paradigm, due to its limited computational depth,\nstruggles to model the complex evolving nature of user preferences and lacks a\nnuanced understanding of long-tail items, leading to suboptimal performance. To\naddress this issue, we propose \\textbf{ReaRec}, the first inference-time\ncomputing framework for recommender systems, which enhances user\nrepresentations through implicit multi-step reasoning. Specifically, ReaRec\nautoregressively feeds the sequence's last hidden state into the sequential\nrecommender while incorporating special reasoning position embeddings to\ndecouple the original item encoding space from the multi-step reasoning space.\nMoreover, we introduce two lightweight reasoning-based learning methods,\nEnsemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to\nfurther effectively exploit ReaRec's reasoning potential. Extensive experiments\non five public real-world datasets and different SeqRec architectures\ndemonstrate the generality and effectiveness of our proposed ReaRec.\nRemarkably, post-hoc analyses reveal that ReaRec significantly elevates the\nperformance ceiling of multiple sequential recommendation backbones by\napproximately 30\\%-50\\%. Thus, we believe this work can open a new and\npromising avenue for future research in inference-time computing for sequential\nrecommendation.\n","authors":["Jiakai Tang","Sunhao Dai","Teng Shi","Jun Xu","Xu Chen","Wen Chen","Wu Jian","Yuning Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.22675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11889v1","updated":"2025-04-16T09:17:45Z","published":"2025-04-16T09:17:45Z","title":"Rethinking LLM-Based Recommendations: A Query Generation-Based,\n  Training-Free Approach","summary":"  Existing large language model LLM-based recommendation methods face several\nchallenges, including inefficiency in handling large candidate pools,\nsensitivity to item order within prompts (\"lost in the middle\" phenomenon) poor\nscalability, and unrealistic evaluation due to random negative sampling. To\naddress these issues, we propose a Query-to-Recommendation approach that\nleverages LLMs to generate personalized queries for retrieving relevant items\nfrom the entire candidate pool, eliminating the need for candidate\npre-selection. This method can be integrated into an ID-based recommendation\nsystem without additional training, enhances recommendation performance and\ndiversity through LLMs' world knowledge, and performs well even for less\npopular item groups. Experiments on three datasets show up to 57 percent\nimprovement, with an average gain of 31 percent, demonstrating strong zero-shot\nperformance and further gains when ensembled with existing models.\n","authors":["Donghee Han","Hwanjun Song","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2504.11889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.08754v3","updated":"2025-04-16T07:59:48Z","published":"2025-03-28T15:49:52Z","title":"Towards Personalized Conversational Sales Agents : Contextual User\n  Profiling for Strategic Action","summary":"  Conversational Recommender Systems (CRSs) aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To bridge this gap, we introduce Conversational Sales\n(CSales), a novel task that unifies preference elicitation, recommendation, and\npersuasion to better support user decision-making. For a realistic evaluation\nof CSales, we present CSUser, an LLM-based user simulator constructed from\nreal-world data, modeling diverse user profiles with needs and personalities.\nAdditionally, we propose CSI, a conversational sales agent that proactively\ninfers contextual profiles through dialogue for personalized action planning.\nExtensive experiments demonstrate that CSUser effectively replicates real-world\nusers and emphasize the importance of contextual profiling for strategic action\nselection, ultimately driving successful purchases in e-commerce.\n","authors":["Tongyoung Kim","Jeongeun Lee","Soojin Yoon","Sunghwan Kim","Dongha Lee"],"pdf_url":"https://arxiv.org/pdf/2504.08754v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11803v1","updated":"2025-04-16T06:24:49Z","published":"2025-04-16T06:24:49Z","title":"Résumé abstractif à partir d'une transcription audio","summary":"  Currently, large language models are gaining popularity, their achievements\nare used in many areas, ranging from text translation to generating answers to\nqueries. However, the main problem with these new machine learning algorithms\nis that training such models requires large computing resources that only large\nIT companies have. To avoid this problem, a number of methods (LoRA,\nquantization) have been proposed so that existing models can be effectively\nfine-tuned for specific tasks. In this paper, we propose an E2E (end to end)\naudio summarization model using these techniques. In addition, this paper\nexamines the effectiveness of these approaches to the problem under\nconsideration and draws conclusions about the applicability of these methods.\n","authors":["Ilia Derkach"],"pdf_url":"https://arxiv.org/pdf/2504.11803v1.pdf","comment":"35 pages, in French language, 8 tables, 6 figures"},{"id":"http://arxiv.org/abs/2504.08738v2","updated":"2025-04-16T05:59:02Z","published":"2025-03-20T18:56:22Z","title":"AI-Driven Sentiment Analytics: Unlocking Business Value in the\n  E-Commerce Landscape_v1","summary":"  The rapid growth of e-commerce has led to an overwhelming volume of customer\nfeedback, from product reviews to service interactions. Extracting meaningful\ninsights from this data is crucial for businesses aiming to improve customer\nsatisfaction and optimize decision-making. This paper presents an AI-driven\nsentiment analysis system designed specifically for e-commerce applications,\nbalancing accuracy with interpretability. Our approach integrates traditional\nmachine learning techniques with modern deep learning models, allowing for a\nmore nuanced understanding of customer sentiment while ensuring transparency in\ndecision-making. Experimental results show that our system outperforms standard\nsentiment analysis methods, achieving an accuracy of 89.7% on diverse,\nlarge-scale datasets. Beyond technical performance, real-world implementation\nacross multiple e-commerce platforms demonstrates tangible improvements in\ncustomer engagement and operational efficiency. This study highlights both the\npotential and the challenges of applying AI to sentiment analysis in a\ncommercial setting, offering insights into practical deployment strategies and\nareas for future refinement.\n","authors":["Qianye Wu","Chengxuan Xia","Sixuan Tian"],"pdf_url":"https://arxiv.org/pdf/2504.08738v2.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2504.11197v2","updated":"2025-04-16T03:32:23Z","published":"2025-04-15T13:53:08Z","title":"Efficient Distributed Retrieval-Augmented Generation for Enhancing\n  Language Model Performance","summary":"  Small language models (SLMs) support efficient deployments on\nresource-constrained edge devices, but their limited capacity compromises\ninference performance. Retrieval-augmented generation (RAG) is a promising\nsolution to enhance model performance by integrating external databases,\nwithout requiring intensive on-device model retraining. However, large-scale\npublic databases and user-specific private contextual documents are typically\nlocated on the cloud and the device separately, while existing RAG\nimplementations are primarily centralized. To bridge this gap, we propose\nDRAGON, a distributed RAG framework to enhance on-device SLMs through both\ngeneral and personal knowledge without the risk of leaking document privacy.\nSpecifically, DRAGON decomposes multi-document RAG into multiple parallel token\ngeneration processes performed independently and locally on the cloud and the\ndevice, and employs a newly designed Speculative Aggregation, a dual-side\nspeculative algorithm to avoid frequent output synchronization between the\ncloud and device. A new scheduling algorithm is further introduced to identify\nthe optimal aggregation side based on real-time network conditions. Evaluations\non real-world hardware testbed demonstrate a significant performance\nimprovement of DRAGON-up to 1.9x greater gains over standalone SLM compared to\nthe centralized RAG, substantial reduction in per-token latency, and negligible\nTime to First Token (TTFT) overhead.\n","authors":["Shangyu Liu","Zhenzhe Zheng","Xiaoyao Huang","Fan Wu","Guihai Chen","Jie Wu"],"pdf_url":"https://arxiv.org/pdf/2504.11197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11696v1","updated":"2025-04-16T01:43:36Z","published":"2025-04-16T01:43:36Z","title":"A New Paradigm of User-Centric Wireless Communication Driven by Large\n  Language Models","summary":"  The next generation of wireless communications seeks to deeply integrate\nartificial intelligence (AI) with user-centric communication networks, with the\ngoal of developing AI-native networks that more accurately address user\nrequirements. The rapid development of large language models (LLMs) offers\nsignificant potential in realizing these goals. However, existing efforts that\nleverage LLMs for wireless communication often overlook the considerable gap\nbetween human natural language and the intricacies of real-world communication\nsystems, thus failing to fully exploit the capabilities of LLMs. To address\nthis gap, we propose a novel LLM-driven paradigm for wireless communication\nthat innovatively incorporates the nature language to structured query language\n(NL2SQL) tool. Specifically, in this paradigm, user personal requirements is\nthe primary focus. Upon receiving a user request, LLMs first analyze the user\nintent in terms of relevant communication metrics and system parameters.\nSubsequently, a structured query language (SQL) statement is generated to\nretrieve the specific parameter values from a high-performance real-time\ndatabase. We further utilize LLMs to formulate and solve an optimization\nproblem based on the user request and the retrieved parameters. The solution to\nthis optimization problem then drives adjustments in the communication system\nto fulfill the user's requirements. To validate the feasibility of the proposed\nparadigm, we present a prototype system. In this prototype, we consider\nuser-request centric semantic communication (URC-SC) system in which a dynamic\nsemantic representation network at the physical layer adapts its encoding depth\nto meet user requirements. Additionally, two LLMs are employed to analyze user\nrequests and generate SQL statements, respectively. Simulation results\ndemonstrate the effectiveness.\n","authors":["Kuiyuan Ding","Caili Guo","Yang Yang","Wuxia Hu","Yonina C. Eldar"],"pdf_url":"https://arxiv.org/pdf/2504.11696v1.pdf","comment":"8 pages, 5 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2406.19388v4","updated":"2025-04-16T17:40:22Z","published":"2024-06-27T17:58:54Z","title":"Taming Data and Transformers for Audio Generation","summary":"  The scalability of ambient sound generators is hindered by data scarcity,\ninsufficient caption quality, and limited scalability in model architecture.\nThis work addresses these challenges by advancing both data and model scaling.\nFirst, we propose an efficient and scalable dataset collection pipeline\ntailored for ambient audio generation, resulting in AutoReCap-XL, the largest\nambient audio-text dataset with over 47 million clips. To provide high-quality\ntextual annotations, we propose AutoCap, a high-quality automatic audio\ncaptioning model. By adopting a Q-Former module and leveraging audio metadata,\nAutoCap substantially enhances caption quality, reaching a CIDEr score of\n$83.2$, a $3.2\\%$ improvement over previous captioning models. Finally, we\npropose GenAu, a scalable transformer-based audio generation architecture that\nwe scale up to 1.25B parameters. We demonstrate its benefits from data scaling\nwith synthetic captions as well as model size scaling. When compared to\nbaseline audio generators trained at similar size and data scale, GenAu obtains\nsignificant improvements of $4.7\\%$ in FAD score, $11.1\\%$ in IS, and $13.5\\%$\nin CLAP score. Our code, model checkpoints, and dataset are publicly available.\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Guha Balakrishnan","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2406.19388v4.pdf","comment":"Project Webpage: https://snap-research.github.io/GenAU/"},{"id":"http://arxiv.org/abs/2504.12204v1","updated":"2025-04-16T15:53:53Z","published":"2025-04-16T15:53:53Z","title":"Towards Realistic Low-Light Image Enhancement via ISP Driven Data\n  Modeling","summary":"  Deep neural networks (DNNs) have recently become the leading method for\nlow-light image enhancement (LLIE). However, despite significant progress,\ntheir outputs may still exhibit issues such as amplified noise, incorrect white\nbalance, or unnatural enhancements when deployed in real world applications. A\nkey challenge is the lack of diverse, large scale training data that captures\nthe complexities of low-light conditions and imaging pipelines. In this paper,\nwe propose a novel image signal processing (ISP) driven data synthesis pipeline\nthat addresses these challenges by generating unlimited paired training data.\nSpecifically, our pipeline begins with easily collected high-quality\nnormal-light images, which are first unprocessed into the RAW format using a\nreverse ISP. We then synthesize low-light degradations directly in the RAW\ndomain. The resulting data is subsequently processed through a series of ISP\nstages, including white balance adjustment, color space conversion, tone\nmapping, and gamma correction, with controlled variations introduced at each\nstage. This broadens the degradation space and enhances the diversity of the\ntraining data, enabling the generated data to capture a wide range of\ndegradations and the complexities inherent in the ISP pipeline. To demonstrate\nthe effectiveness of our synthetic pipeline, we conduct extensive experiments\nusing a vanilla UNet model consisting solely of convolutional layers, group\nnormalization, GeLU activation, and convolutional block attention modules\n(CBAMs). Extensive testing across multiple datasets reveals that the vanilla\nUNet model trained with our data synthesis pipeline delivers high fidelity,\nvisually appealing enhancement results, surpassing state-of-the-art (SOTA)\nmethods both quantitatively and qualitatively.\n","authors":["Zhihua Wang","Yu Long","Qinghua Lin","Kai Zhang","Yazhu Zhang","Yuming Fang","Li Liu","Xiaochun Cao"],"pdf_url":"https://arxiv.org/pdf/2504.12204v1.pdf","comment":"17 pages, 11 tables, 10 figures"},{"id":"http://arxiv.org/abs/2309.12029v3","updated":"2025-04-16T15:12:05Z","published":"2023-09-21T12:51:11Z","title":"Exploring Self-supervised Skeleton-based Action Recognition in Occluded\n  Environments","summary":"  To integrate action recognition into autonomous robotic systems, it is\nessential to address challenges such as person occlusions-a common yet often\noverlooked scenario in existing self-supervised skeleton-based action\nrecognition methods. In this work, we propose IosPSTL, a simple and effective\nself-supervised learning framework designed to handle occlusions. IosPSTL\ncombines a cluster-agnostic KNN imputer with an Occluded Partial\nSpatio-Temporal Learning (OPSTL) strategy. First, we pre-train the model on\noccluded skeleton sequences. Then, we introduce a cluster-agnostic KNN imputer\nthat performs semantic grouping using k-means clustering on sequence\nembeddings. It imputes missing skeleton data by applying K-Nearest Neighbors in\nthe latent space, leveraging nearby sample representations to restore occluded\njoints. This imputation generates more complete skeleton sequences, which\nsignificantly benefits downstream self-supervised models. To further enhance\nlearning, the OPSTL module incorporates Adaptive Spatial Masking (ASM) to make\nbetter use of intact, high-quality skeleton sequences during training. Our\nmethod achieves state-of-the-art performance on the occluded versions of the\nNTU-60 and NTU-120 datasets, demonstrating its robustness and effectiveness\nunder challenging conditions. Code is available at\nhttps://github.com/cyfml/OPSTL.\n","authors":["Yifei Chen","Kunyu Peng","Alina Roitberg","David Schneider","Jiaming Zhang","Junwei Zheng","Yufan Chen","Ruiping Liu","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2309.12029v3.pdf","comment":"Accepted to IJCNN 2025. Code is available at\n  https://github.com/cyfml/OPSTL"},{"id":"http://arxiv.org/abs/2410.21169v4","updated":"2025-04-16T15:01:20Z","published":"2024-10-28T16:11:35Z","title":"Document Parsing Unveiled: Techniques, Challenges, and Prospects for\n  Structured Information Extraction","summary":"  Document parsing is essential for converting unstructured and semi-structured\ndocuments such as contracts, academic papers, and invoices into structured,\nmachine-readable data. Document parsing reliable structured data from\nunstructured inputs, providing huge convenience for numerous applications.\nEspecially with recent achievements in Large Language Models, document parsing\nplays an indispensable role in both knowledge base construction and training\ndata generation. This survey presents a comprehensive review of the current\nstate of document parsing, covering key methodologies, from modular pipeline\nsystems to end-to-end models driven by large vision-language models. Core\ncomponents such as layout detection, content extraction (including text,\ntables, and mathematical expressions), and multi-modal data integration are\nexamined in detail. Additionally, this paper discusses the challenges faced by\nmodular document parsing systems and vision-language models in handling complex\nlayouts, integrating multiple modules, and recognizing high-density text. It\noutlines future research directions and emphasizes the importance of developing\nlarger and more diverse datasets.\n","authors":["Qintong Zhang","Bin Wang","Victor Shea-Jay Huang","Junyuan Zhang","Zhengren Wang","Hao Liang","Conghui He","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21169v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14773v2","updated":"2025-04-16T13:38:58Z","published":"2024-03-21T18:27:29Z","title":"StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text","summary":"  Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V\n","authors":["Roberto Henschel","Levon Khachatryan","Hayk Poghosyan","Daniil Hayrapetyan","Vahram Tadevosyan","Zhangyang Wang","Shant Navasardyan","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2403.14773v2.pdf","comment":"https://github.com/Picsart-AI-Research/StreamingT2V"},{"id":"http://arxiv.org/abs/2504.11101v2","updated":"2025-04-16T03:22:14Z","published":"2025-04-15T11:51:18Z","title":"Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and\n  Self-Improving OCR","summary":"  The Optical Character Recognition (OCR) task is important for evaluating\nVision-Language Models (VLMs) and providing high-quality data sources for LLM\ntraining data. While state-of-the-art VLMs show improved average OCR accuracy,\nthey still struggle with sample-level quality degradation and lack reliable\nautomatic detection of low-quality outputs. We introduce Consensus Entropy\n(CE), a training-free post-inference method that quantifies OCR uncertainty by\naggregating outputs from multiple VLMs. Our approach exploits a key insight:\ncorrect VLM OCR predictions converge in output space while errors diverge. We\ndevelop a lightweight multi-model framework that effectively identifies\nproblematic samples, selects the best outputs and combines model strengths.\nExperiments across multiple OCR benchmarks and VLMs demonstrate that CE\noutperforms VLM-as-judge approaches and single-model baselines at the same cost\nand achieves state-of-the-art results across multiple metrics. For instance,\nour solution demonstrates: achieving 15.2% higher F1 scores than VLM-as-judge\nmethods in quality verification, delivering 6.0% accuracy gains on mathematical\ncalculation tasks, and requiring rephrasing only 7.3% of inputs while\nmaintaining overall performance. Notably, the entire process requires neither\ntraining nor supervision while maintaining plug-and-play functionality\nthroughout.\n","authors":["Yulong Zhang","Tianyi Liang","Xinyue Huang","Erfei Cui","Xu Guo","Pei Chu","Chenhui Li","Ru Zhang","Wenhai Wang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11101v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11695v1","updated":"2025-04-16T01:40:06Z","published":"2025-04-16T01:40:06Z","title":"Interpreting the Linear Structure of Vision-language Model Embedding\n  Spaces","summary":"  Vision-language models encode images and text in a joint space, minimizing\nthe distance between corresponding image and text pairs. How are language and\nimages organized in this joint space, and how do the models encode meaning and\nmodality? To investigate this, we train and release sparse autoencoders (SAEs)\non the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2,\nand AIMv2). SAEs approximate model embeddings as sparse linear combinations of\nlearned directions, or \"concepts\". We find that, compared to other methods of\nlinear feature learning, SAEs are better at reconstructing the real embeddings,\nwhile also able to retain the most sparsity. Retraining SAEs with different\nseeds or different data diet leads to two findings: the rare, specific concepts\ncaptured by the SAEs are liable to change drastically, but we also show that\nthe key commonly-activating concepts extracted by SAEs are remarkably stable\nacross runs. Interestingly, while most concepts are strongly unimodal in\nactivation, we find they are not merely encoding modality per se. Many lie\nclose to - but not entirely within - the subspace defining modality, suggesting\nthat they encode cross-modal semantics despite their unimodal usage. To\nquantify this bridging behavior, we introduce the Bridge Score, a metric that\nidentifies concept pairs which are both co-activated across aligned image-text\ninputs and geometrically aligned in the shared space. This reveals that even\nunimodal concepts can collaborate to support cross-modal integration. We\nrelease interactive demos of the SAEs for all models, allowing researchers to\nexplore the organization of the concept spaces. Overall, our findings uncover a\nsparse linear structure within VLM embedding spaces that is shaped by modality,\nyet stitched together through latent bridges-offering new insight into how\nmultimodal meaning is constructed.\n","authors":["Isabel Papadimitriou","Huangyuan Su","Thomas Fel","Naomi Saphra","Sham Kakade","Stephanie Gil"],"pdf_url":"https://arxiv.org/pdf/2504.11695v1.pdf","comment":null}]},"2025-04-15T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.19955v2","updated":"2025-04-15T23:36:25Z","published":"2024-10-25T20:25:22Z","title":"Bridging Stepwise Lab-Informed Pretraining and Knowledge-Guided Learning\n  for Diagnostic Reasoning","summary":"  Despite the growing use of Electronic Health Records (EHR) for AI-assisted\ndiagnosis prediction, most data-driven models struggle to incorporate\nclinically meaningful medical knowledge. They often rely on limited ontologies,\nlacking structured reasoning capabilities and comprehensive coverage. This\nraises an important research question: Will medical knowledge improve\npredictive models to support stepwise clinical reasoning as performed by human\ndoctors? To address this problem, we propose DuaLK, a dual-expertise framework\nthat combines two complementary sources of information. For external knowledge,\nwe construct a Diagnosis Knowledge Graph (KG) that encodes both hierarchical\nand semantic relations enriched by large language models (LLM). To align with\npatient data, we further introduce a lab-informed proxy task that guides the\nmodel to follow a clinically consistent, stepwise reasoning process based on\nlab test signals. Experimental results on two public EHR datasets demonstrate\nthat DuaLK consistently outperforms existing baselines across four clinical\nprediction tasks. These findings highlight the potential of combining\nstructured medical knowledge with individual-level clinical signals to achieve\nmore accurate and interpretable diagnostic predictions. The source code is\npublicly available on https://github.com/humphreyhuu/DuaLK.\n","authors":["Pengfei Hu","Chang Lu","Fei Wang","Yue Ning"],"pdf_url":"https://arxiv.org/pdf/2410.19955v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11658v1","updated":"2025-04-15T23:03:53Z","published":"2025-04-15T23:03:53Z","title":"Improving LLM Interpretability and Performance via Guided Embedding\n  Refinement for Sequential Recommendation","summary":"  The fast development of Large Language Models (LLMs) offers growing\nopportunities to further improve sequential recommendation systems. Yet for\nsome practitioners, integrating LLMs to their existing base recommendation\nsystems raises questions about model interpretability, transparency and related\nsafety. To partly alleviate challenges from these questions, we propose guided\nembedding refinement, a method that carries out a guided and interpretable\nusage of LLM to enhance the embeddings associated with the base recommendation\nsystem. Instead of directly using LLMs as the backbone of sequential\nrecommendation systems, we utilize them as auxiliary tools to emulate the sales\nlogic of recommendation and generate guided embeddings that capture\ndomain-relevant semantic information on interpretable attributes. Benefiting\nfrom the strong generalization capabilities of the guided embedding, we\nconstruct refined embedding by using the guided embedding and reduced-dimension\nversion of the base embedding. We then integrate the refined embedding into the\nrecommendation module for training and inference. A range of numerical\nexperiments demonstrate that guided embedding is adaptable to various given\nexisting base embedding models, and generalizes well across different\nrecommendation tasks. The numerical results show that the refined embedding not\nonly improves recommendation performance, achieving approximately $10\\%$ to\n$50\\%$ gains in Mean Reciprocal Rank (MRR), Recall rate, and Normalized\nDiscounted Cumulative Gain (NDCG), but also enhances interpretability, as\nevidenced by case studies.\n","authors":["Nanshan Jia","Chenfei Yuan","Yuhang Wu","Zeyu Zheng"],"pdf_url":"https://arxiv.org/pdf/2504.11658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11284v1","updated":"2025-04-15T15:25:27Z","published":"2025-04-15T15:25:27Z","title":"Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation","summary":"  Bipartite ranking is a fundamental supervised learning problem, with the goal\nof learning a ranking over instances with maximal area under the ROC curve\n(AUC) against a single binary target label. However, one may often observe\nmultiple binary target labels, e.g., from distinct human annotators. How can\none synthesize such labels into a single coherent ranking? In this work, we\nformally analyze two approaches to this problem -- loss aggregation and label\naggregation -- by characterizing their Bayes-optimal solutions. Based on this,\nwe show that while both methods can yield Pareto-optimal solutions, loss\naggregation can exhibit label dictatorship: one can inadvertently (and\nundesirably) favor one label over others. This suggests that label aggregation\ncan be preferable to loss aggregation, which we empirically verify.\n","authors":["Michal Lukasik","Lin Chen","Harikrishna Narasimhan","Aditya Krishna Menon","Wittawat Jitkrittum","Felix X. Yu","Sashank J. Reddi","Gang Fu","Mohammadhossein Bateni","Sanjiv Kumar"],"pdf_url":"https://arxiv.org/pdf/2504.11284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11510v1","updated":"2025-04-15T10:24:37Z","published":"2025-04-15T10:24:37Z","title":"RAID: An In-Training Defense against Attribute Inference Attacks in\n  Recommender Systems","summary":"  In various networks and mobile applications, users are highly susceptible to\nattribute inference attacks, with particularly prevalent occurrences in\nrecommender systems. Attackers exploit partially exposed user profiles in\nrecommendation models, such as user embeddings, to infer private attributes of\ntarget users, such as gender and political views. The goal of defenders is to\nmitigate the effectiveness of these attacks while maintaining recommendation\nperformance. Most existing defense methods, such as differential privacy and\nattribute unlearning, focus on post-training settings, which limits their\ncapability of utilizing training data to preserve recommendation performance.\nAlthough adversarial training extends defenses to in-training settings, it\noften struggles with convergence due to unstable training processes. In this\npaper, we propose RAID, an in-training defense method against attribute\ninference attacks in recommender systems. In addition to the recommendation\nobjective, we define a defensive objective to ensure that the distribution of\nprotected attributes becomes independent of class labels, making users\nindistinguishable from attribute inference attacks. Specifically, this\ndefensive objective aims to solve a constrained Wasserstein barycenter problem\nto identify the centroid distribution that makes the attribute\nindistinguishable while complying with recommendation performance constraints.\nTo optimize our proposed objective, we use optimal transport to align users\nwith the centroid distribution. We conduct extensive experiments on four\nreal-world datasets to evaluate RAID. The experimental results validate the\neffectiveness of RAID and demonstrate its significant superiority over existing\nmethods in multiple aspects.\n","authors":["Xiaohua Feng","Yuyuan Li","Fengyuan Yu","Ke Xiong","Junjie Fang","Li Zhang","Tianyu Du","Chaochao Chen"],"pdf_url":"https://arxiv.org/pdf/2504.11510v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2306.08929v3","updated":"2025-04-15T10:24:14Z","published":"2023-06-15T08:02:07Z","title":"Inferring Communities of Interest in Collaborative Learning-based\n  Recommender Systems","summary":"  Collaborative-learning-based recommender systems, such as those employing\nFederated Learning (FL) and Gossip Learning (GL), allow users to train models\nwhile keeping their history of liked items on their devices. While these\nmethods were seen as promising for enhancing privacy, recent research has shown\nthat collaborative learning can be vulnerable to various privacy attacks. In\nthis paper, we propose a novel attack called Community Inference Attack (CIA),\nwhich enables an adversary to identify community members based on a set of\ntarget items. What sets CIA apart is its efficiency: it operates at low\ncomputational cost by eliminating the need for training surrogate models.\nInstead, it uses a comparison-based approach, inferring sensitive information\nby comparing users' models rather than targeting any specific individual model.\nTo evaluate the effectiveness of CIA, we conduct experiments on three\nreal-world recommendation datasets using two recommendation models under both\nFederated and Gossip-like settings. The results demonstrate that CIA can be up\nto 10 times more accurate than random guessing. Additionally, we evaluate two\nmitigation strategies: Differentially Private Stochastic Gradient Descent\n(DP-SGD) and a Share less policy, which involves sharing fewer, less sensitive\nmodel parameters. Our findings suggest that the Share less strategy offers a\nbetter privacy-utility trade-off, especially in GL.\n","authors":["Yacine Belal","Sonia Ben Mokhtar","Mohamed Maouche","Anthony Simonet-Boulogne"],"pdf_url":"https://arxiv.org/pdf/2306.08929v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11011v1","updated":"2025-04-15T09:32:57Z","published":"2025-04-15T09:32:57Z","title":"Document Quality Scoring for Web Crawling","summary":"  The internet contains large amounts of low-quality content, yet users expect\nweb search engines to deliver high-quality, relevant results. The abundant\npresence of low-quality pages can negatively impact retrieval and crawling\nprocesses by wasting resources on these documents. Therefore, search engines\ncan greatly benefit from techniques that leverage efficient quality estimation\nmethods to mitigate these negative impacts. Quality scoring methods for web\npages are useful for many processes typical for web search systems, including\nstatic index pruning, index tiering, and crawling. Building on work by Chang et\nal.~\\cite{chang2024neural}, who proposed using neural estimators of semantic\nquality for static index pruning, we extend their approach and apply their\nneural quality scorers to assess the semantic quality of web pages in crawling\nprioritisation tasks. In our experimental analysis, we found that prioritising\nsemantically high-quality pages over low-quality ones can improve downstream\nsearch effectiveness. Our software contribution consists of a Docker container\nthat computes an effective quality score for a given web page, allowing the\nquality scorer to be easily included and used in other components of web search\nsystems.\n","authors":["Francesca Pezzuti","Ariane Mueller","Sean MacAvaney","Nicola Tonellotto"],"pdf_url":"https://arxiv.org/pdf/2504.11011v1.pdf","comment":"Presented at WOWS2025"},{"id":"http://arxiv.org/abs/2504.11000v1","updated":"2025-04-15T09:16:17Z","published":"2025-04-15T09:16:17Z","title":"Why am I seeing this? Towards recognizing social media recommender\n  systems with missing recommendations","summary":"  Social media plays a crucial role in shaping society, often amplifying\npolarization and spreading misinformation. These effects stem from complex\ndynamics involving user interactions, individual traits, and recommender\nalgorithms driving content selection. Recommender systems, which significantly\nshape the content users see and decisions they make, offer an opportunity for\nintervention and regulation. However, assessing their impact is challenging due\nto algorithmic opacity and limited data availability. To effectively model user\ndecision-making, it is crucial to recognize the recommender system adopted by\nthe platform.\n  This work introduces a method for Automatic Recommender Recognition using\nGraph Neural Networks (GNNs), based solely on network structure and observed\nbehavior. To infer the hidden recommender, we first train a Recommender Neutral\nUser model (RNU) using a GNN and an adapted hindsight academic network\nrecommender, aiming to reduce reliance on the actual recommender in the data.\nWe then generate several Recommender Hypothesis-specific Synthetic Datasets\n(RHSD) by combining the RNU with different known recommenders, producing ground\ntruths for testing. Finally, we train Recommender Hypothesis-specific User\nmodels (RHU) under various hypotheses and compare each candidate with the\noriginal used to generate the RHSD.\n  Our approach enables accurate detection of hidden recommenders and their\ninfluence on user behavior. Unlike audit-based methods, it captures system\nbehavior directly, without ad hoc experiments that often fail to reflect real\nplatforms. This study provides insights into how recommenders shape behavior,\naiding efforts to reduce polarization and misinformation.\n","authors":["Sabrina Guidotti","Sabrina Patania","Giuseppe Vizzari","Dimitri Ognibene","Gregor Donabauer","Udo Kruschwitz","Davide Taibi"],"pdf_url":"https://arxiv.org/pdf/2504.11000v1.pdf","comment":"Accepted at RLDM 2025"},{"id":"http://arxiv.org/abs/2411.13212v3","updated":"2025-04-15T09:11:18Z","published":"2024-11-20T11:19:35Z","title":"Limitations of Automatic Relevance Assessments with Large Language\n  Models for Fair and Reliable Retrieval Evaluation","summary":"  Offline evaluation of search systems depends on test collections. These\nbenchmarks provide the researchers with a corpus of documents, topics and\nrelevance judgements indicating which documents are relevant for each topic.\nWhile test collections are an integral part of Information Retrieval (IR)\nresearch, their creation involves significant efforts in manual annotation.\nLarge language models (LLMs) are gaining much attention as tools for automatic\nrelevance assessment. Recent research has shown that LLM-based assessments\nyield high systems ranking correlation with human-made judgements. These\ncorrelations are helpful in large-scale experiments but less informative if we\nwant to focus on top-performing systems. Moreover, these correlations ignore\nwhether and how LLM-based judgements impact the statistically significant\ndifferences among systems with respect to human assessments. In this work, we\nlook at how LLM-generated judgements preserve ranking differences among\ntop-performing systems and also how they preserve pairwise significance\nevaluation as human judgements. Our results show that LLM-based judgements are\nunfair at ranking top-performing systems. Moreover, we observe an exceedingly\nhigh rate of false positives regarding statistical differences. Our work\nrepresents a step forward in the evaluation of the reliability of using\nLLMs-based judgements for IR evaluation. We hope this will serve as a basis for\nother researchers to develop more reliable models for automatic relevance\nassessment.\n","authors":["David Otero","Javier Parapar","Álvaro Barreiro"],"pdf_url":"https://arxiv.org/pdf/2411.13212v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22303v2","updated":"2025-04-15T08:10:39Z","published":"2025-03-28T10:26:49Z","title":"Preference-based Learning with Retrieval Augmented Generation for\n  Conversational Question Answering","summary":"  Conversational Question Answering (ConvQA) involves multiple subtasks, i) to\nunderstand incomplete questions in their context, ii) to retrieve relevant\ninformation, and iii) to generate answers. This work presents PRAISE, a\npipeline-based approach for ConvQA that trains LLM adapters for each of the\nthree subtasks. As labeled training data for individual subtasks is unavailable\nin practice, PRAISE learns from its own generations using the final answering\nperformance as feedback signal without human intervention and treats\nintermediate information, like relevant evidence, as weakly labeled data. We\napply Direct Preference Optimization by contrasting successful and unsuccessful\nsamples for each subtask. In our experiments, we show the effectiveness of this\ntraining paradigm: PRAISE shows improvements per subtask and achieves new\nstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5\npercentage points increase in precision over baselines.\n","authors":["Magdalena Kaiser","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2503.22303v2.pdf","comment":"WWW 2025 Short Paper, 5 pages"},{"id":"http://arxiv.org/abs/2504.08780v2","updated":"2025-04-15T07:59:02Z","published":"2025-04-05T06:16:43Z","title":"How Relevance Emerges: Interpreting LoRA Fine-Tuning in Reranking LLMs","summary":"  We conduct a behavioral exploration of LoRA fine-tuned LLMs for Passage\nReranking to understand how relevance signals are learned and deployed by Large\nLanguage Models. By fine-tuning Mistral-7B, LLaMA3.1-8B, and Pythia-6.9B on MS\nMARCO under diverse LoRA configurations, we investigate how relevance modeling\nevolves across checkpoints, the impact of LoRA rank (1, 2, 8, 32), and the\nrelative importance of updated MHA vs. MLP components. Our ablations reveal\nwhich layers and projections within LoRA transformations are most critical for\nreranking accuracy. These findings offer fresh explanations into LoRA's\nadaptation mechanisms, setting the stage for deeper mechanistic studies in\nInformation Retrieval. All models used in this study have been shared.\n","authors":["Atharva Nijasure","Tanya Chowdhury","James Allan"],"pdf_url":"https://arxiv.org/pdf/2504.08780v2.pdf","comment":"Extended Abstract"},{"id":"http://arxiv.org/abs/2411.04677v5","updated":"2025-04-15T07:46:44Z","published":"2024-11-07T13:03:21Z","title":"Lightning IR: Straightforward Fine-tuning and Inference of\n  Transformer-based Language Models for Information Retrieval","summary":"  A wide range of transformer-based language models have been proposed for\ninformation retrieval tasks. However, including transformer-based models in\nretrieval pipelines is often complex and requires substantial engineering\neffort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch\nLightning-based framework for applying transformer-based language models in\nretrieval scenarios. Lightning IR provides a modular and extensible\narchitecture that supports all stages of a retrieval pipeline: from fine-tuning\nand indexing to searching and re-ranking. Designed to be scalable and\nreproducible, Lightning IR is available as open-source:\nhttps://github.com/webis-de/lightning-ir.\n","authors":["Ferdinand Schlatt","Maik Fröbe","Matthias Hagen"],"pdf_url":"https://arxiv.org/pdf/2411.04677v5.pdf","comment":"Accepted as a demo at WSDM'25"},{"id":"http://arxiv.org/abs/2504.10921v1","updated":"2025-04-15T07:05:22Z","published":"2025-04-15T07:05:22Z","title":"MSCRS: Multi-modal Semantic Graph Prompt Learning Framework for\n  Conversational Recommender Systems","summary":"  Conversational Recommender Systems (CRSs) aim to provide personalized\nrecommendations by interacting with users through conversations. Most existing\nstudies of CRS focus on extracting user preferences from conversational\ncontexts. However, due to the short and sparse nature of conversational\ncontexts, it is difficult to fully capture user preferences by conversational\ncontexts only. We argue that multi-modal semantic information can enrich user\npreference expressions from diverse dimensions (e.g., a user preference for a\ncertain movie may stem from its magnificent visual effects and compelling\nstoryline). In this paper, we propose a multi-modal semantic graph prompt\nlearning framework for CRS, named MSCRS. First, we extract textual and image\nfeatures of items mentioned in the conversational contexts. Second, we capture\nhigher-order semantic associations within different semantic modalities\n(collaborative, textual, and image) by constructing modality-specific graph\nstructures. Finally, we propose an innovative integration of multi-modal\nsemantic graphs with prompt learning, harnessing the power of large language\nmodels to comprehensively explore high-dimensional semantic relationships.\nExperimental results demonstrate that our proposed method significantly\nimproves accuracy in item recommendation, as well as generates more natural and\ncontextually relevant content in response generation. We have released the code\nand the expanded multi-modal CRS datasets to facilitate further exploration in\nrelated research\\footnote{https://github.com/BIAOBIAO12138/MSCRS-main}.\n","authors":["Yibiao Wei","Jie Zou","Weikang Guo","Guoqing Wang","Xing Xu","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2504.10921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.21188v2","updated":"2025-04-15T05:19:42Z","published":"2025-03-27T06:10:22Z","title":"Are We Solving a Well-Defined Problem? A Task-Centric Perspective on\n  Recommendation Tasks","summary":"  Recommender systems (RecSys) leverage user interaction history to predict and\nsuggest relevant items, shaping user experiences across various domains. While\nmany studies adopt a general problem definition, i.e., to recommend preferred\nitems to users based on past interactions, such abstraction often lacks the\ndomain-specific nuances necessary for practical deployment. However, models are\nfrequently evaluated using datasets from online recommender platforms, which\ninherently reflect these specificities. In this paper, we analyze RecSys task\nformulations, emphasizing key components such as input-output structures,\ntemporal dynamics, and candidate item selection. All these factors directly\nimpact offline evaluation. We further examine the complexities of user-item\ninteractions, including decision-making costs, multi-step engagements, and\nunobservable interactions, which may influence model design and loss functions.\nAdditionally, we explore the balance between task specificity and model\ngeneralizability, highlighting how well-defined task formulations serve as the\nfoundation for robust evaluation and effective solution development. By\nclarifying task definitions and their implications, this work provides a\nstructured perspective on RecSys research. The goal is to help researchers\nbetter navigate the field, particularly in understanding specificities of the\nRecSys tasks and ensuring fair and meaningful evaluations.\n","authors":["Aixin Sun"],"pdf_url":"https://arxiv.org/pdf/2503.21188v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2502.08132v2","updated":"2025-04-15T01:35:23Z","published":"2025-02-12T05:28:08Z","title":"SS4Rec: Continuous-Time Sequential Recommendation with State Space\n  Models","summary":"  Sequential recommendation is a key area in the field of recommendation\nsystems aiming to model user interest based on historical interaction sequences\nwith irregular intervals. While previous recurrent neural network-based and\nattention-based approaches have achieved significant results, they have\nlimitations in capturing system continuity due to the discrete characteristics.\nIn the context of continuous-time modeling, state space model (SSM) offers a\npotential solution, as it can effectively capture the dynamic evolution of user\ninterest over time. However, existing SSM-based approaches ignore the impact of\nirregular time intervals within historical user interactions, making it\ndifficult to model complexed user-item transitions in sequences. To address\nthis issue, we propose a hybrid SSM-based model called SS4Rec for\ncontinuous-time sequential recommendation. SS4Rec integrates a time-aware SSM\nto handle irregular time intervals and a relation-aware SSM to model contextual\ndependencies, enabling it to infer user interest from both temporal and\nsequential perspectives. In the training process, the time-aware SSM and the\nrelation-aware SSM are discretized by variable stepsizes according to user\ninteraction time intervals and input data, respectively. This helps capture the\ncontinuous dependency from irregular time intervals and provides time-specific\npersonalized recommendations. Experimental studies on five benchmark datasets\ndemonstrate the superiority and effectiveness of SS4Rec.\n","authors":["Wei Xiao","Huiying Wang","Qifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.08132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05862v2","updated":"2025-04-15T01:14:43Z","published":"2025-04-08T09:41:03Z","title":"Are Generative AI Agents Effective Personalized Financial Advisors?","summary":"  Large language model-based agents are becoming increasingly popular as a\nlow-cost mechanism to provide personalized, conversational advice, and have\ndemonstrated impressive capabilities in relatively simple scenarios, such as\nmovie recommendations. But how do these agents perform in complex high-stakes\ndomains, where domain expertise is essential and mistakes carry substantial\nrisk? This paper investigates the effectiveness of LLM-advisors in the finance\ndomain, focusing on three distinct challenges: (1) eliciting user preferences\nwhen users themselves may be unsure of their needs, (2) providing personalized\nguidance for diverse investment preferences, and (3) leveraging advisor\npersonality to build relationships and foster trust. Via a lab-based user study\nwith 64 participants, we show that LLM-advisors often match human advisor\nperformance when eliciting preferences, although they can struggle to resolve\nconflicting user needs. When providing personalized advice, the LLM was able to\npositively influence user behavior, but demonstrated clear failure modes. Our\nresults show that accurate preference elicitation is key, otherwise, the\nLLM-advisor has little impact, or can even direct the investor toward\nunsuitable assets. More worryingly, users appear insensitive to the quality of\nadvice being given, or worse these can have an inverse relationship. Indeed,\nusers reported a preference for and increased satisfaction as well as emotional\ntrust with LLMs adopting an extroverted persona, even though those agents\nprovided worse advice.\n","authors":["Takehiro Takayanagi","Kiyoshi Izumi","Javier Sanz-Cruzado","Richard McCreadie","Iadh Ounis"],"pdf_url":"https://arxiv.org/pdf/2504.05862v2.pdf","comment":"Accepted for presentation at SIGIR 2025"}],"Multimedia":[{"id":"http://arxiv.org/abs/2504.11331v1","updated":"2025-04-15T16:05:09Z","published":"2025-04-15T16:05:09Z","title":"Dependency Structure Augmented Contextual Scoping Framework for\n  Multimodal Aspect-Based Sentiment Analysis","summary":"  Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract\nfine-grained information from image-text pairs to identify aspect terms and\ndetermine their sentiment polarity. However, existing approaches often fall\nshort in simultaneously addressing three core challenges: Sentiment Cue\nPerception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise\nElimination (SNE). To overcome these limitations, we propose DASCO\n(\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework),\na fine-grained scope-oriented framework that enhances aspect-level sentiment\nreasoning by leveraging dependency parsing trees. First, we designed a\nmulti-task pretraining strategy for MABSA on our base model, combining\naspect-oriented enhancement, image-text matching, and aspect-level\nsentiment-sensitive cognition. This improved the model's perception of aspect\nterms and sentiment cues while achieving effective image-text alignment,\naddressing key challenges like SCP and MIM. Furthermore, we incorporate\ndependency trees as syntactic branch combining with semantic branch, guiding\nthe model to selectively attend to critical contextual elements within a\ntarget-specific scope while effectively filtering out irrelevant noise for\naddressing SNE problem. Extensive experiments on two benchmark datasets across\nthree subtasks demonstrate that DASCO achieves state-of-the-art performance in\nMABSA, with notable gains in JMASA (+3.1\\% F1 and +5.4\\% precision on\nTwitter2015).\n","authors":["Hao Liu","Lijun He","Jiaxi Liang","Zhihan Ren","Fan Li"],"pdf_url":"https://arxiv.org/pdf/2504.11331v1.pdf","comment":"submitted to ACM MM2025"},{"id":"http://arxiv.org/abs/2504.11232v1","updated":"2025-04-15T14:33:25Z","published":"2025-04-15T14:33:25Z","title":"Leveraging multimodal explanatory annotations for video interpretation\n  with Modality Specific Dataset","summary":"  We examine the impact of concept-informed supervision on multimodal video\ninterpretation models using MOByGaze, a dataset containing human-annotated\nexplanatory concepts. We introduce Concept Modality Specific Datasets (CMSDs),\nwhich consist of data subsets categorized by the modality (visual, textual, or\naudio) of annotated concepts. Models trained on CMSDs outperform those using\ntraditional legacy training in both early and late fusion approaches. Notably,\nthis approach enables late fusion models to achieve performance close to that\nof early fusion models. These findings underscore the importance of\nmodality-specific annotations in developing robust, self-explainable video\nmodels and contribute to advancing interpretable multimodal learning in complex\nvideo analysis.\n","authors":["Elisa Ancarani","Julie Tores","Lucile Sassatelli","Rémy Sun","Hui-Yin Wu","Frédéric Precioso"],"pdf_url":"https://arxiv.org/pdf/2504.11232v1.pdf","comment":"6 pages, 8 Figures"},{"id":"http://arxiv.org/abs/2504.11515v1","updated":"2025-04-15T14:26:12Z","published":"2025-04-15T14:26:12Z","title":"Graph-Driven Multimodal Feature Learning Framework for Apparent\n  Personality Assessment","summary":"  Predicting personality traits automatically has become a challenging problem\nin computer vision. This paper introduces an innovative multimodal feature\nlearning framework for personality analysis in short video clips. For visual\nprocessing, we construct a facial graph and design a Geo-based two-stream\nnetwork incorporating an attention mechanism, leveraging both Graph\nConvolutional Networks (GCN) and Convolutional Neural Networks (CNN) to capture\nstatic facial expressions. Additionally, ResNet18 and VGGFace networks are\nemployed to extract global scene and facial appearance features at the frame\nlevel. To capture dynamic temporal information, we integrate a BiGRU with a\ntemporal attention module for extracting salient frame representations. To\nenhance the model's robustness, we incorporate the VGGish CNN for audio-based\nfeatures and XLM-Roberta for text-based features. Finally, a multimodal channel\nattention mechanism is introduced to integrate different modalities, and a\nMulti-Layer Perceptron (MLP) regression model is used to predict personality\ntraits. Experimental results confirm that our proposed framework surpasses\nexisting state-of-the-art approaches in performance.\n","authors":["Kangsheng Wang","Chengwei Ye","Huanzhen Zhang","Linuo Xu","Shuyan Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09353v2","updated":"2025-04-15T10:14:34Z","published":"2024-12-12T15:22:03Z","title":"Causal Graphical Models for Vision-Language Compositional Understanding","summary":"  Recent work has empirically shown that Vision-Language Models (VLMs) struggle\nto fully understand the compositional properties of the human language, usually\nmodeling an image caption as a \"bag of words\". As a result, they perform poorly\non compositional tasks, which require a deeper understanding of the different\nentities of a sentence (subject, verb, etc.) jointly with their mutual\nrelationships in order to be solved. In this paper, we model the dependency\nrelations among textual and visual tokens using a Causal Graphical Model (CGM),\nbuilt using a dependency parser, and we train a decoder conditioned by the VLM\nvisual encoder. Differently from standard autoregressive or parallel\npredictions, our decoder's generative process is partially-ordered following\nthe CGM structure. This structure encourages the decoder to learn only the main\ncausal dependencies in a sentence discarding spurious correlations. Using\nextensive experiments on five compositional benchmarks, we show that our method\nsignificantly outperforms all the state-of-the-art compositional approaches by\na large margin, and it also improves over methods trained using much larger\ndatasets.\n","authors":["Fiorenzo Parascandolo","Nicholas Moratelli","Enver Sangineto","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2412.09353v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2504.11009v1","updated":"2025-04-15T09:29:08Z","published":"2025-04-15T09:29:08Z","title":"MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal\n  Critique","summary":"  Visual language models (VLMs) have demonstrated strong performance across\ndiverse multimodal reasoning tasks but still face challenges such as\nhallucinations, resulting in incorrect reasoning outcomes. Inspired by recent\nresearch on external feedback mechanisms in large language models (LLMs), we\npropose a multimodal actor-critic framework to enhance VLM reasoning\ncapabilities. Specifically, the actor model generates step-by-step reasoning\npaths based on image and text inputs, while the critic model evaluates these\nreasoning paths and provides corrective feedback. The actor model iteratively\nrefines its reasoning based on the feedback until the reasoning outcome is\ndeemed satisfactory by the critic model. To reduce reliance on costly manual\nannotations, we introduce an automated method for constructing multimodal\ncritique datasets. By leveraging Monte Carlo Tree Search (MCTS), we\nsystematically guide the actor model to explore diverse reasoning paths. To\nobtain critique data for correcting erroneous reasoning steps, we prompt an\nannotator model to compare pairs of reasoning paths diverging from a shared\nancestor node - one leading to a correct conclusion and the other to an\nincorrect one. This approach enables us to construct the MMC (MCTS-based\nMultimodal Critique) dataset, upon which we further develop a comprehensive\ntraining and inference pipeline. Extensive experiments conducted on several\npublic benchmark datasets and mainstream VLMs demonstrate that our approach\nsignificantly improves the performance of VLM on complex multimodal reasoning\ntasks, underscoring its effectiveness and wide applicability.\n","authors":["Shuhang Liu","Zhenrong Zhang","Pengfei Hu","Jiefeng Ma","Jun Du","Qing Wang","Jianshu Zhang","Quan Liu","Jianqing Gao","Feng Ma"],"pdf_url":"https://arxiv.org/pdf/2504.11009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11002v1","updated":"2025-04-15T09:19:44Z","published":"2025-04-15T09:19:44Z","title":"Dopamine Audiobook: A Training-free MLLM Agent for Emotional and\n  Human-like Audiobook Generation","summary":"  Audiobook generation, which creates vivid and emotion-rich audio works, faces\nchallenges in conveying complex emotions, achieving human-like qualities, and\naligning evaluations with human preferences. Existing text-to-speech (TTS)\nmethods are often limited to specific scenarios, struggle with emotional\ntransitions, and lack automatic human-aligned evaluation benchmarks, instead\nrelying on either misaligned automated metrics or costly human assessments. To\naddress these issues, we propose Dopamine Audiobook, a new unified\ntraining-free system leveraging a multimodal large language model (MLLM) as an\nAI agent for emotional and human-like audiobook generation and evaluation.\nSpecifically, we first design a flow-based emotion-enhanced framework that\ndecomposes complex emotional speech synthesis into controllable sub-tasks.\nThen, we propose an adaptive model selection module that dynamically selects\nthe most suitable TTS methods from a set of existing state-of-the-art (SOTA)\nTTS methods for diverse scenarios. We further enhance emotional expressiveness\nthrough paralinguistic augmentation and prosody retrieval at word and utterance\nlevels. For evaluation, we propose a novel GPT-based evaluation framework\nincorporating self-critique, perspective-taking, and psychological MagicEmo\nprompts to ensure human-aligned and self-aligned assessments. Experiments show\nthat our method generates long speech with superior emotional expression to\nSOTA TTS models in various metrics. Importantly, our evaluation framework\ndemonstrates better alignment with human preferences and transferability across\naudio tasks. Project website with audio samples can be found at\nhttps://dopamine-audiobook.github.io.\n","authors":["Yan Rong","Shan Yang","Guangzhi Lei","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2504.11002v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18416v2","updated":"2025-04-15T08:43:07Z","published":"2024-12-24T13:08:34Z","title":"Muse: A Multimodal Conversational Recommendation Dataset with\n  Scenario-Grounded User Profiles","summary":"  Current conversational recommendation systems focus predominantly on text.\nHowever, real-world recommendation settings are generally multimodal, causing a\nsignificant gap between existing research and practical applications. To\naddress this issue, we propose Muse, the first multimodal conversational\nrecommendation dataset. Muse comprises 83,148 utterances from 7,000\nconversations centered around the Clothing domain. Each conversation contains\ncomprehensive multimodal interactions, rich elements, and natural dialogues.\nData in Muse are automatically synthesized by a multi-agent framework powered\nby multimodal large language models (MLLMs). It innovatively derives user\nprofiles from real-world scenarios rather than depending on manual design and\nhistory data for better scalability, and then it fulfills conversation\nsimulation and optimization. Both human and LLM evaluations demonstrate the\nhigh quality of conversations in Muse. Additionally, fine-tuning experiments on\nthree MLLMs demonstrate Muse's learnable patterns for recommendations and\nresponses, confirming its value for multimodal conversational recommendation.\nOur dataset and codes are available at\nhttps://anonymous.4open.science/r/Muse-0086.\n","authors":["Zihan Wang","Xiaocui Yang","Yongkang Liu","Shi Feng","Daling Wang","Yifei Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.18416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03897v4","updated":"2025-04-15T06:53:12Z","published":"2025-02-06T09:18:30Z","title":"UniForm: A Unified Multi-Task Diffusion Transformer for Audio-Video\n  Generation","summary":"  With the rise of diffusion models, audio-video generation has been\nrevolutionized. However, most existing methods rely on separate modules for\neach modality, with limited exploration of unified generative architectures. In\naddition, many are confined to a single task and small-scale datasets. To\naddress these limitations, we first propose UniForm, a unified multi-task\ndiffusion transformer that jointly generates audio and visual modalities in a\nshared latent space. A single diffusion process models both audio and video,\ncapturing the inherent correlations between sound and vision. Second, we\nintroduce task-specific noise schemes and task tokens, enabling a single model\nto support multiple tasks, including text-to-audio-video, audio-to-video, and\nvideo-to-audio generation. Furthermore, by leveraging large language models and\na large-scale text-audio-video combined dataset, UniForm achieves greater\ngenerative diversity than prior approaches. Extensive experiments show that\nUniForm achieves the state-of-the-art performance across audio-video generation\ntasks, producing content that is both well-aligned and close to real-world data\ndistributions. Our demos are available at https://uniform-t2av.github.io/.\n","authors":["Lei Zhao","Linfeng Feng","Dongxu Ge","Rujin Chen","Fangqiu Yi","Chi Zhang","Xiao-Lei Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2502.03897v4.pdf","comment":"Our demos are available at https://uniform-t2av.github.io/"},{"id":"http://arxiv.org/abs/2504.10849v1","updated":"2025-04-15T04:17:08Z","published":"2025-04-15T04:17:08Z","title":"Real-Time Word-Level Temporal Segmentation in Streaming Speech\n  Recognition","summary":"  Rich-text captions are essential to help communication for Deaf and\nhard-of-hearing (DHH) people, second-language learners, and those with autism\nspectrum disorder (ASD). They also preserve nuances when converting speech to\ntext, enhancing the realism of presentation scripts and conversation or speech\nlogs. However, current real-time captioning systems lack the capability to\nalter text attributes (ex. capitalization, sizes, and fonts) at the word level,\nhindering the accurate conveyance of speaker intent that is expressed in the\ntones or intonations of the speech. For example, ''YOU should do this'' tends\nto be considered as indicating ''You'' as the focus of the sentence, whereas\n''You should do THIS'' tends to be ''This'' as the focus. This paper proposes a\nsolution that changes the text decorations at the word level in real time. As a\nprototype, we developed an application that adjusts word size based on the\nloudness of each spoken word. Feedback from users implies that this system\nhelped to convey the speaker's intent, offering a more engaging and accessible\ncaptioning experience.\n","authors":["Naoto Nishida","Hirotaka Hiraki","Jun Rekimoto","Yoshio Ishiguro"],"pdf_url":"https://arxiv.org/pdf/2504.10849v1.pdf","comment":"3 pages, 1 figures"},{"id":"http://arxiv.org/abs/2504.10848v1","updated":"2025-04-15T04:16:48Z","published":"2025-04-15T04:16:48Z","title":"Ichiyo: Fragile and Transient Interaction in Neighborhood","summary":"  As the Internet develops, social networking and other communication tools\nhave transformed people's relationships into something fast, visible, and\ngeographically huge. However, these communication tools have not expanded\nopportunities for acquainting oneself with neighbors outside one's social\nnetwork; rather, they have comparatively diminished occasions for interacting\nwith unfamiliar neighbors by prioritizing communication with existing friends.\nTherefore, we invented the medium Ichiyo to increase the opportunities to think\nof neighbors walking along the same street or in the same neighborhood and to\nexpand the imagination of those who pass by and those who used to be there.\nThus, users can engage in indirect interaction. We used commercially available\nlaser cutters to engrave QR codes on leaves that are naturally found in our\nliving space to prevent environmental invasion. The QR codes lead to a communal\nspace on the web where users can freely leave messages. By engraving QR codes,\ninformation can be virtually expanded to be presented. To get the feedback of\nIchiyo, we let a total of several thousand people experience a new way of\ncommunication as a part of the exhibition ''iii Exhibition 2022'', an art\nexhibition at the University of Tokyo. A total of more than 1,000 leaves\nengraved with QR codes were prepared and scattered at the exhibition site and\nalong the road from the nearest station to the venue.\n","authors":["Hirofumi Shibata","Ayako Yogo","Naoto Nishida","Yu Shimada","Toma Ishii"],"pdf_url":"https://arxiv.org/pdf/2504.10848v1.pdf","comment":"3 pages, 3 figures"},{"id":"http://arxiv.org/abs/2504.10322v2","updated":"2025-04-15T04:02:00Z","published":"2025-04-14T15:30:49Z","title":"Efficient Prompt Tuning for Hierarchical Ingredient Recognition","summary":"  Fine-grained ingredient recognition presents a significant challenge due to\nthe diverse appearances of ingredients, resulting from different cutting and\ncooking methods. While existing approaches have shown promising results, they\nstill require extensive training costs and focus solely on fine-grained\ningredient recognition. In this paper, we address these limitations by\nintroducing an efficient prompt-tuning framework that adapts pretrained\nvisual-language models (VLMs), such as CLIP, to the ingredient recognition task\nwithout requiring full model finetuning. Additionally, we introduce three-level\ningredient hierarchies to enhance both training performance and evaluation\nrobustness. Specifically, we propose a hierarchical ingredient recognition\ntask, designed to evaluate model performance across different hierarchical\nlevels (e.g., chicken chunks, chicken, meat), capturing recognition\ncapabilities from coarse- to fine-grained categories. Our method leverages\nhierarchical labels, training prompt-tuned models with both fine-grained and\ncorresponding coarse-grained labels. Experimental results on the VireoFood172\ndataset demonstrate the effectiveness of prompt-tuning with hierarchical\nlabels, achieving superior performance. Moreover, the hierarchical ingredient\nrecognition task provides valuable insights into the model's ability to\ngeneralize across different levels of ingredient granularity.\n","authors":["Yinxuan Gui","Bin Zhu","Jingjing Chen","Chong-Wah Ngo"],"pdf_url":"https://arxiv.org/pdf/2504.10322v2.pdf","comment":"Accepted by IEEE International Conference on Multimedia and Expo\n  (ICME) 2025"},{"id":"http://arxiv.org/abs/2504.10826v1","updated":"2025-04-15T03:08:09Z","published":"2025-04-15T03:08:09Z","title":"SteerMusic: Enhanced Musical Consistency for Zero-shot Text-Guided and\n  Personalized Music Editing","summary":"  Music editing is an important step in music production, which has broad\napplications, including game development and film production. Most existing\nzero-shot text-guided methods rely on pretrained diffusion models by involving\nforward-backward diffusion processes for editing. However, these methods often\nstruggle to maintain the music content consistency. Additionally, text\ninstructions alone usually fail to accurately describe the desired music. In\nthis paper, we propose two music editing methods that enhance the consistency\nbetween the original and edited music by leveraging score distillation. The\nfirst method, SteerMusic, is a coarse-grained zero-shot editing approach using\ndelta denoising score. The second method, SteerMusic+, enables fine-grained\npersonalized music editing by manipulating a concept token that represents a\nuser-defined musical style. SteerMusic+ allows for the editing of music into\nany user-defined musical styles that cannot be achieved by the text\ninstructions alone. Experimental results show that our methods outperform\nexisting approaches in preserving both music content consistency and editing\nfidelity. User studies further validate that our methods achieve superior music\nediting quality. Audio examples are available on https://steermusic.pages.dev/.\n","authors":["Xinlei Niu","Kin Wai Cheuk","Jing Zhang","Naoki Murata","Chieh-Hsin Lai","Michele Mancusi","Woosung Choi","Giorgio Fabbro","Wei-Hsiang Liao","Charles Patrick Martin","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2504.10826v1.pdf","comment":null}]},"2025-04-14T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2504.10753v1","updated":"2025-04-14T23:04:35Z","published":"2025-04-14T23:04:35Z","title":"Epistemic Uncertainty-aware Recommendation Systems via Bayesian Deep\n  Ensemble Learning","summary":"  Recommending items to users has long been a fundamental task, and studies\nhave tried to improve it ever since. Most well-known models commonly employ\nrepresentation learning to map users and items into a unified embedding space\nfor matching assessment. These approaches have primary limitations, especially\nwhen dealing with explicit feedback and sparse data contexts. Two primary\nlimitations are their proneness to overfitting and failure to incorporate\nepistemic uncertainty in predictions. To address these problems, we propose a\nnovel Bayesian Deep Ensemble Collaborative Filtering method named BDECF. To\nimprove model generalization and quality, we utilize Bayesian Neural Networks,\nwhich incorporate uncertainty within their weight parameters. In addition, we\nintroduce a new interpretable non-linear matching approach for the user and\nitem embeddings, leveraging the advantages of the attention mechanism.\nFurthermore, we endorse the implementation of an ensemble-based supermodel to\ngenerate more robust and reliable predictions, resulting in a more complete\nmodel. Empirical evaluation through extensive experiments and ablation studies\nacross a range of publicly accessible real-world datasets with differing\nsparsity characteristics confirms our proposed method's effectiveness and the\nimportance of its components.\n","authors":["Radin Cheraghi","Amir Mohammad Mahfoozi","Sepehr Zolfaghari","Mohammadshayan Shabani","Maryam Ramezani","Hamid R. Rabiee"],"pdf_url":"https://arxiv.org/pdf/2504.10753v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2502.19596v2","updated":"2025-04-14T20:00:15Z","published":"2025-02-26T22:20:08Z","title":"Trustworthy Answers, Messier Data: Bridging the Gap in Low-Resource\n  Retrieval-Augmented Generation for Domain Expert Systems","summary":"  RAG has become a key technique for enhancing LLMs by reducing hallucinations,\nespecially in domain expert systems where LLMs may lack sufficient inherent\nknowledge. However, developing these systems in low-resource settings\nintroduces several challenges: (1) handling heterogeneous data sources, (2)\noptimizing retrieval phase for trustworthy answers, and (3) evaluating\ngenerated answers across diverse aspects. To address these, we introduce a data\ngeneration pipeline that transforms raw multi-modal data into structured corpus\nand Q&A pairs, an advanced re-ranking phase improving retrieval precision, and\na reference matching algorithm enhancing answer traceability. Applied to the\nautomotive engineering domain, our system improves factual correctness (+1.94),\ninformativeness (+1.16), and helpfulness (+1.67) over a non-RAG baseline, based\non a 1-5 scale by an LLM judge. These results highlight the effectiveness of\nour approach across distinct aspects, with strong answer grounding and\ntransparency.\n","authors":["Nayoung Choi","Grace Byun","Andrew Chung","Ellie S. Paek","Shinsun Lee","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2502.19596v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06064v2","updated":"2025-04-14T19:09:19Z","published":"2024-11-09T04:23:58Z","title":"Snippet-based Conversational Recommender System","summary":"  Conversational Recommender Systems (CRS) engage users in interactive\ndialogues to gather preferences and provide personalized recommendations. While\nexisting studies have advanced conversational strategies, they often rely on\npredefined attributes or expensive, domain-specific annotated datasets, which\nlimits their flexibility in handling diverse user preferences and adaptability\nacross domains. We propose SnipRec, a novel resource-efficient approach that\nleverages user-generated content, such as customer reviews, to capture a\nbroader range of user expressions. By employing large language models to map\nreviews and user responses into concise snippets, SnipRec represents user\npreferences and retrieves relevant items without the need for intensive manual\ndata collection or fine-tuning. Experiments across the restaurant, book, and\nclothing domains show that snippet-based representations outperform document-\nand sentence-based representations, achieving Hits@10 of 0.25-0.55 with 3,000\nto 10,000 candidate items while successfully handling free-form user responses.\n","authors":["Haibo Sun","Naoki Otani","Hannah Kim","Dan Zhang","Nikita Bhutani"],"pdf_url":"https://arxiv.org/pdf/2411.06064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10613v1","updated":"2025-04-14T18:11:53Z","published":"2025-04-14T18:11:53Z","title":"Enhancing Document Retrieval for Curating N-ary Relations in Knowledge\n  Bases","summary":"  Curation of biomedical knowledge bases (KBs) relies on extracting accurate\nmulti-entity relational facts from the literature - a process that remains\nlargely manual and expert-driven. An essential step in this workflow is\nretrieving documents that can support or complete partially observed n-ary\nrelations. We present a neural retrieval model designed to assist KB curation\nby identifying documents that help fill in missing relation arguments and\nprovide relevant contextual evidence.\n  To reduce dependence on scarce gold-standard training data, we exploit\nexisting KB records to construct weakly supervised training sets. Our approach\nintroduces two key technical contributions: (i) a layered contrastive loss that\nenables learning from noisy and incomplete relational structures, and (ii) a\nbalanced sampling strategy that generates high-quality negatives from diverse\nKB records. On two biomedical retrieval benchmarks, our approach achieves\nstate-of-the-art performance, outperforming strong baselines in NDCG@10 by 5.7\nand 3.7 percentage points, respectively.\n","authors":["Xing David Wang","Ulf Leser"],"pdf_url":"https://arxiv.org/pdf/2504.10613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12008v6","updated":"2025-04-14T17:40:38Z","published":"2024-04-18T08:59:32Z","title":"How Do Recommendation Models Amplify Popularity Bias? An Analysis from\n  the Spectral Perspective","summary":"  Recommendation Systems (RS) are often plagued by popularity bias. When\ntraining a recommendation model on a typically long-tailed dataset, the model\ntends to not only inherit this bias but often exacerbate it, resulting in\nover-representation of popular items in the recommendation lists. This study\nconducts comprehensive empirical and theoretical analyses to expose the root\ncauses of this phenomenon, yielding two core insights: 1) Item popularity is\nmemorized in the principal spectrum of the score matrix predicted by the\nrecommendation model; 2) The dimension collapse phenomenon amplifies the\nrelative prominence of the principal spectrum, thereby intensifying the\npopularity bias. Building on these insights, we propose a novel debiasing\nstrategy that leverages a spectral norm regularizer to penalize the magnitude\nof the principal singular value. We have developed an efficient algorithm to\nexpedite the calculation of the spectral norm by exploiting the spectral\nproperty of the score matrix. Extensive experiments across seven real-world\ndatasets and three testing paradigms have been conducted to validate the\nsuperiority of the proposed method.\n","authors":["Siyi Lin","Chongming Gao","Jiawei Chen","Sheng Zhou","Binbin Hu","Yan Feng","Chun Chen","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12008v6.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2504.10432v1","updated":"2025-04-14T17:20:48Z","published":"2025-04-14T17:20:48Z","title":"Invariance Matters: Empowering Social Recommendation via Graph Invariant\n  Learning","summary":"  Graph-based social recommendation systems have shown significant promise in\nenhancing recommendation performance, particularly in addressing the issue of\ndata sparsity in user behaviors. Typically, these systems leverage Graph Neural\nNetworks (GNNs) to capture user preferences by incorporating high-order social\ninfluences from observed social networks. However, existing graph-based social\nrecommendations often overlook the fact that social networks are inherently\nnoisy, containing task-irrelevant relationships that can hinder accurate user\npreference learning. The removal of these redundant social relations is\ncrucial, yet it remains challenging due to the lack of ground truth. In this\npaper, we approach the social denoising problem from the perspective of graph\ninvariant learning and propose a novel method, Social Graph Invariant\nLearning(SGIL). Specifically,SGIL aims to uncover stable user preferences\nwithin the input social graph, thereby enhancing the robustness of graph-based\nsocial recommendation systems. To achieve this goal, SGIL first simulates\nmultiple noisy social environments through graph generators. It then seeks to\nlearn environment-invariant user preferences by minimizing invariant risk\nacross these environments. To further promote diversity in the generated social\nenvironments, we employ an adversarial training strategy to simulate more\npotential social noisy distributions. Extensive experimental results\ndemonstrate the effectiveness of the proposed SGIL. The code is available at\nhttps://github.com/yimutianyang/SIGIR2025-SGIL.\n","authors":["Yonghui Yang","Le Wu","Yuxin Liao","Zhuangzhuang He","Pengyang Shao","Richang Hong","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2504.10432v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10371v1","updated":"2025-04-14T16:18:30Z","published":"2025-04-14T16:18:30Z","title":"Brain-Machine Interfaces & Information Retrieval Challenges and\n  Opportunities","summary":"  The fundamental goal of Information Retrieval (IR) systems lies in their\ncapacity to effectively satisfy human information needs - a challenge that\nencompasses not just the technical delivery of information, but the nuanced\nunderstanding of human cognition during information seeking. Contemporary IR\nplatforms rely primarily on observable interaction signals, creating a\nfundamental gap between system capabilities and users' cognitive processes.\nBrain-Machine Interface (BMI) technologies now offer unprecedented potential to\nbridge this gap through direct measurement of previously inaccessible aspects\nof information-seeking behaviour. This perspective paper offers a broad\nexamination of the IR landscape, providing a comprehensive analysis of how BMI\ntechnology could transform IR systems, drawing from advances at the\nintersection of both neuroscience and IR research. We present our analysis\nthrough three identified fundamental vertices: (1) understanding the neural\ncorrelates of core IR concepts to advance theoretical models of search\nbehaviour, (2) enhancing existing IR systems through contextual integration of\nneurophysiological signals, and (3) developing proactive IR capabilities\nthrough direct neurophysiological measurement. For each vertex, we identify\nspecific research opportunities and propose concrete directions for developing\nBMI-enhanced IR systems. We conclude by examining critical technical and\nethical challenges in implementing these advances, providing a structured\nroadmap for future research at the intersection of neuroscience and IR.\n","authors":["Yashar Moshfeghi","Niall McGuire"],"pdf_url":"https://arxiv.org/pdf/2504.10371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10326v1","updated":"2025-04-14T15:34:26Z","published":"2025-04-14T15:34:26Z","title":"AlayaDB: The Data Foundation for Efficient and Effective Long-context\n  LLM Inference","summary":"  AlayaDB is a cutting-edge vector database system natively architected for\nefficient and effective long-context inference for Large Language Models (LLMs)\nat AlayaDB AI. Specifically, it decouples the KV cache and attention\ncomputation from the LLM inference systems, and encapsulates them into a novel\nvector database system. For the Model as a Service providers (MaaS), AlayaDB\nconsumes fewer hardware resources and offers higher generation quality for\nvarious workloads with different kinds of Service Level Objectives (SLOs), when\ncomparing with the existing alternative solutions (e.g., KV cache\ndisaggregation, retrieval-based sparse attention). The crux of AlayaDB is that\nit abstracts the attention computation and cache management for LLM inference\ninto a query processing procedure, and optimizes the performance via a native\nquery optimizer. In this work, we demonstrate the effectiveness of AlayaDB via\n(i) three use cases from our industry partners, and (ii) extensive experimental\nresults on LLM inference benchmarks.\n","authors":["Yangshen Deng","Zhengxin You","Long Xiang","Qilong Li","Peiqi Yuan","Zhaoyang Hong","Yitao Zheng","Wanting Li","Runzhong Li","Haotian Liu","Kyriakos Mouratidis","Man Lung Yiu","Huan Li","Qiaomu Shen","Rui Mao","Bo Tang"],"pdf_url":"https://arxiv.org/pdf/2504.10326v1.pdf","comment":"14 pages, 12 figures, conference"},{"id":"http://arxiv.org/abs/2504.10307v1","updated":"2025-04-14T15:14:59Z","published":"2025-04-14T15:14:59Z","title":"CROSSAN: Towards Efficient and Effective Adaptation of Multiple\n  Multimodal Foundation Models for Sequential Recommendation","summary":"  Multimodal Foundation Models (MFMs) excel at representing diverse raw\nmodalities (e.g., text, images, audio, videos, etc.). As recommender systems\nincreasingly incorporate these modalities, leveraging MFMs to generate better\nrepresentations has great potential. However, their application in sequential\nrecommendation remains largely unexplored. This is primarily because mainstream\nadaptation methods, such as Fine-Tuning and even Parameter-Efficient\nFine-Tuning (PEFT) techniques (e.g., Adapter and LoRA), incur high\ncomputational costs, especially when integrating multiple modality encoders,\nthus hindering research progress. As a result, it remains unclear whether we\ncan efficiently and effectively adapt multiple (>2) MFMs for the sequential\nrecommendation task.\n  To address this, we propose a plug-and-play Cross-modal Side Adapter Network\n(CROSSAN). Leveraging the fully decoupled side adapter-based paradigm, CROSSAN\nachieves high efficiency while enabling cross-modal learning across diverse\nmodalities. To optimize the final stage of multimodal fusion across diverse\nmodalities, we adopt the Mixture of Modality Expert Fusion (MOMEF) mechanism.\nCROSSAN achieves superior performance on the public datasets for adapting four\nfoundation models with raw modalities. Performance consistently improves as\nmore MFMs are adapted. We will release our code and datasets to facilitate\nfuture research.\n","authors":["Junchen Fu","Yongxin Ni","Joemon M. Jose","Ioannis Arapakis","Kaiwen Zheng","Youhua Li","Xuri Ge"],"pdf_url":"https://arxiv.org/pdf/2504.10307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.08393v2","updated":"2025-04-14T14:55:02Z","published":"2025-03-11T12:57:24Z","title":"Weighted Tensor Decompositions for Context-aware Collaborative Filtering","summary":"  Over recent years it has become well accepted that user interest is not\nstatic or immutable. There are a variety of contextual factors, such as time of\nday, the weather or the user's mood, that influence the current interests of\nthe user. Modelling approaches need to take these factors into account if they\nwant to succeed at finding the most relevant content to recommend given the\nsituation.\n  A popular method for context-aware recommendation is to encode context\nattributes as extra dimensions of the classic user-item interaction matrix,\neffectively turning it into a tensor, followed by applying the appropriate\ntensor decomposition methods to learn missing values. However, unlike with\nmatrix factorization, where all decompositions are essentially a product of\nmatrices, there exist many more options for decomposing tensors by combining\nvector, matrix and tensor products. We study the most successful decomposition\nmethods that use weighted square loss and categorize them based on their tensor\nstructure and regularization strategy. Additionally, we further extend the pool\nof methods by filling in the missing combinations.\n  In this paper we provide an overview of the properties of the different\ndecomposition methods, such as their complexity, scalability, and modelling\ncapacity. These benefits are then contrasted with the performances achieved in\noffline experiments to gain more insight into which method to choose depending\non a specific situation and constraints.\n","authors":["Joey De Pauw","Bart Goethals"],"pdf_url":"https://arxiv.org/pdf/2503.08393v2.pdf","comment":"Workshop on Context-Aware Recommender Systems, September 18, 2023,\n  Singapore"},{"id":"http://arxiv.org/abs/2310.14379v3","updated":"2025-04-14T14:33:47Z","published":"2023-10-22T18:22:35Z","title":"Can Offline Metrics Measure Explanation Goals? A Comparative Survey\n  Analysis of Offline Explanation Metrics in Recommender Systems","summary":"  Explanations in a Recommender System (RS) provide reasons for recommendations\nto users and can enhance transparency, persuasiveness, engagement, and\ntrust-known as explanation goals. Evaluating the effectiveness of explanation\nalgorithms offline remains challenging due to subjectivity. Initially, we\nconducted a literature review on current offline metrics, revealing that\nalgorithms are often assessed with anecdotal evidence, offering convincing\nexamples, or with metrics that don't align with human perception. We\ninvestigated whether, in explanations connecting interacted and recommended\nitems based on shared content, the selection of item attributes and interacted\nitems affects explanation goals. Metrics measuring the diversity and popularity\nof attributes and the recency of item interactions were used to evaluate\nexplanations from three state-of-the-art agnostic algorithms across six\nrecommendation systems. These offline metrics were compared with results from\nan online user study. Our findings reveal a trade-off: transparency and trust\nrelate to popular properties, while engagement and persuasiveness are linked to\ndiversified properties. This study contributes to the development of more\nrobust evaluation methods for explanation algorithms in recommender systems.\n","authors":["André Levi Zanon","Marcelo Garcia Manzato","Leonardo Rocha"],"pdf_url":"https://arxiv.org/pdf/2310.14379v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10250v1","updated":"2025-04-14T14:13:03Z","published":"2025-04-14T14:13:03Z","title":"MURR: Model Updating with Regularized Replay for Searching a Document\n  Stream","summary":"  The Internet produces a continuous stream of new documents and user-generated\nqueries. These naturally change over time based on events in the world and the\nevolution of language. Neural retrieval models that were trained once on a\nfixed set of query-document pairs will quickly start misrepresenting\nnewly-created content and queries, leading to less effective retrieval.\nTraditional statistical sparse retrieval can update collection statistics to\nreflect these changes in the use of language in documents and queries. In\ncontrast, continued fine-tuning of the language model underlying neural\nretrieval approaches such as DPR and ColBERT creates incompatibility with\npreviously-encoded documents. Re-encoding and re-indexing all\npreviously-processed documents can be costly. In this work, we explore updating\na neural dual encoder retrieval model without reprocessing past documents in\nthe stream. We propose MURR, a model updating strategy with regularized replay,\nto ensure the model can still faithfully search existing documents without\nreprocessing, while continuing to update the model for the latest topics. In\nour simulated streaming environments, we show that fine-tuning models using\nMURR leads to more effective and more consistent retrieval results than other\nstrategies as the stream of documents and queries progresses.\n","authors":["Eugene Yang","Nicola Tonellotto","Dawn Lawrie","Sean MacAvaney","James Mayfield","Douglas W. Oard","Scott Miller"],"pdf_url":"https://arxiv.org/pdf/2504.10250v1.pdf","comment":"Published at ECIR 2025. 16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2504.10208v1","updated":"2025-04-14T13:21:29Z","published":"2025-04-14T13:21:29Z","title":"From Prompting to Alignment: A Generative Framework for Query\n  Recommendation","summary":"  In modern search systems, search engines often suggest relevant queries to\nusers through various panels or components, helping refine their information\nneeds. Traditionally, these recommendations heavily rely on historical search\nlogs to build models, which suffer from cold-start or long-tail issues.\nFurthermore, tasks such as query suggestion, completion or clarification are\nstudied separately by specific design, which lacks generalizability and hinders\nadaptation to novel applications. Despite recent attempts to explore the use of\nLLMs for query recommendation, these methods mainly rely on the inherent\nknowledge of LLMs or external sources like few-shot examples, retrieved\ndocuments, or knowledge bases, neglecting the importance of the calibration and\nalignment with user feedback, thus limiting their practical utility. To address\nthese challenges, we first propose a general Generative Query Recommendation\n(GQR) framework that aligns LLM-based query generation with user preference.\nSpecifically, we unify diverse query recommendation tasks by a universal prompt\nframework, leveraging the instruct-following capability of LLMs for effective\ngeneration. Secondly, we align LLMs with user feedback via presenting a\nCTR-alignment framework, which involves training a query-wise CTR predictor as\na process reward model and employing list-wise preference alignment to maximize\nthe click probability of the generated query list. Furthermore, recognizing the\ninconsistency between LLM knowledge and proactive search intents arising from\nthe separation of user-initiated queries from models, we align LLMs with user\ninitiative via retrieving co-occurrence queries as side information when\nhistorical logs are available.\n","authors":["Erxue Min","Hsiu-Yuan Huang","Min Yang","Xihong Yang","Xin Jia","Yunfang Wu","Hengyi Cai","Shuaiqiang Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2504.10208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10150v1","updated":"2025-04-14T12:01:11Z","published":"2025-04-14T12:01:11Z","title":"HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation\n  with User History Encoding and Compression","summary":"  While large language models (LLMs) have proven effective in leveraging\ntextual data for recommendations, their application to multimodal\nrecommendation tasks remains relatively underexplored. Although LLMs can\nprocess multimodal information through projection functions that map visual\nfeatures into their semantic space, recommendation tasks often require\nrepresenting users' history interactions through lengthy prompts combining text\nand visual elements, which not only hampers training and inference efficiency\nbut also makes it difficult for the model to accurately capture user\npreferences from complex and extended prompts, leading to reduced\nrecommendation performance. To address this challenge, we introduce HistLLM, an\ninnovative multimodal recommendation framework that integrates textual and\nvisual features through a User History Encoding Module (UHEM), compressing\nmultimodal user history interactions into a single token representation,\neffectively facilitating LLMs in processing user preferences. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmechanism.\n","authors":["Chen Zhang","Bo Hu","Weidong Chen","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2504.10150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10147v1","updated":"2025-04-14T11:57:52Z","published":"2025-04-14T11:57:52Z","title":"A Survey of Personalization: From RAG to Agent","summary":"  Personalization has become an essential capability in modern AI systems,\nenabling customized interactions that align with individual user preferences,\ncontexts, and goals. Recent research has increasingly concentrated on\nRetrieval-Augmented Generation (RAG) frameworks and their evolution into more\nadvanced agent-based architectures within personalized settings to enhance user\nsatisfaction. Building on this foundation, this survey systematically examines\npersonalization across the three core stages of RAG: pre-retrieval, retrieval,\nand generation. Beyond RAG, we further extend its capabilities into the realm\nof Personalized LLM-based Agents, which enhance traditional RAG systems with\nagentic functionalities, including user understanding, personalized planning\nand execution, and dynamic generation. For both personalization in RAG and\nagent-based personalization, we provide formal definitions, conduct a\ncomprehensive review of recent literature, and summarize key datasets and\nevaluation metrics. Additionally, we discuss fundamental challenges,\nlimitations, and promising research directions in this evolving field. Relevant\npapers and resources are continuously updated at\nhttps://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent.\n","authors":["Xiaopeng Li","Pengyue Jia","Derong Xu","Yi Wen","Yingyi Zhang","Wenlin Zhang","Wanyu Wang","Yichao Wang","Zhaocheng Du","Xiangyang Li","Yong Liu","Huifeng Guo","Ruiming Tang","Xiangyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2504.10147v1.pdf","comment":"18 pages, 5 figures"},{"id":"http://arxiv.org/abs/2504.10113v1","updated":"2025-04-14T11:22:41Z","published":"2025-04-14T11:22:41Z","title":"Unveiling Contrastive Learning's Capability of Neighborhood Aggregation\n  for Collaborative Filtering","summary":"  Personalized recommendation is widely used in the web applications, and graph\ncontrastive learning (GCL) has gradually become a dominant approach in\nrecommender systems, primarily due to its ability to extract self-supervised\nsignals from raw interaction data, effectively alleviating the problem of data\nsparsity. A classic GCL-based method typically uses data augmentation during\ngraph convolution to generates more contrastive views, and performs contrast on\nthese new views to obtain rich self-supervised signals. Despite this paradigm\nis effective, the reasons behind the performance gains remain a mystery. In\nthis paper, we first reveal via theoretical derivation that the gradient\ndescent process of the CL objective is formally equivalent to graph\nconvolution, which implies that CL objective inherently supports neighborhood\naggregation on interaction graphs. We further substantiate this capability\nthrough experimental validation and identify common misconceptions in the\nselection of positive samples in previous methods, which limit the potential of\nCL objective. Based on this discovery, we propose the Light Contrastive\nCollaborative Filtering (LightCCF) method, which introduces a novel\nneighborhood aggregation objective to bring users closer to all interacted\nitems while pushing them away from other positive pairs, thus achieving\nhigh-quality neighborhood aggregation with very low time complexity. On three\nhighly sparse public datasets, the proposed method effectively aggregate\nneighborhood information while preventing graph over-smoothing, demonstrating\nsignificant improvements over existing GCL-based counterparts in both training\nefficiency and recommendation accuracy. Our implementations are publicly\naccessible.\n","authors":["Yu Zhang","Yiwen Zhang","Yi Zhang","Lei Sang","Yun Yang"],"pdf_url":"https://arxiv.org/pdf/2504.10113v1.pdf","comment":"Accepted by SIGIR2025"},{"id":"http://arxiv.org/abs/2504.10107v1","updated":"2025-04-14T11:15:30Z","published":"2025-04-14T11:15:30Z","title":"Enhancing LLM-based Recommendation through Semantic-Aligned\n  Collaborative Knowledge","summary":"  Large Language Models (LLMs) demonstrate remarkable capabilities in\nleveraging comprehensive world knowledge and sophisticated reasoning mechanisms\nfor recommendation tasks. However, a notable limitation lies in their inability\nto effectively model sparse identifiers (e.g., user and item IDs), unlike\nconventional collaborative filtering models (Collabs.), thus hindering LLM to\nlearn distinctive user-item representations and creating a performance\nbottleneck. Prior studies indicate that integrating collaborative knowledge\nfrom Collabs. into LLMs can mitigate the above limitations and enhance their\nrecommendation performance. Nevertheless, the significant discrepancy in\nknowledge distribution and semantic space between LLMs and Collab. presents\nsubstantial challenges for effective knowledge transfer. To tackle these\nchallenges, we propose a novel framework, SeLLa-Rec, which focuses on achieving\nalignment between the semantic spaces of Collabs. and LLMs. This alignment\nfosters effective knowledge fusion, mitigating the influence of discriminative\nnoise and facilitating the deep integration of knowledge from diverse models.\nSpecifically, three special tokens with collaborative knowledge are embedded\ninto the LLM's semantic space through a hybrid projection layer and integrated\ninto task-specific prompts to guide the recommendation process. Experiments\nconducted on two public benchmark datasets (MovieLens-1M and Amazon Book)\ndemonstrate that SeLLa-Rec achieves state-of-the-art performance.\n","authors":["Zihan Wang","Jinghao Lin","Xiaocui Yang","Yongkang Liu","Shi Feng","Daling Wang","Yifei Zhang"],"pdf_url":"https://arxiv.org/pdf/2504.10107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10005v1","updated":"2025-04-14T09:08:40Z","published":"2025-04-14T09:08:40Z","title":"Session-based Recommender Systems: User Interest as a Stochastic Process\n  in the Latent Space","summary":"  This paper jointly addresses the problem of data uncertainty, popularity\nbias, and exposure bias in session-based recommender systems. We study the\nsymptoms of this bias both in item embeddings and in recommendations. We\npropose treating user interest as a stochastic process in the latent space and\nproviding a model-agnostic implementation of this mathematical concept. The\nproposed stochastic component consists of elements: debiasing item embeddings\nwith regularization for embedding uniformity, modeling dense user interest from\nsession prefixes, and introducing fake targets in the data to simulate extended\nexposure. We conducted computational experiments on two popular benchmark\ndatasets, Diginetica and YooChoose 1/64, as well as several modifications of\nthe YooChoose dataset with different ratios of popular items. The results show\nthat the proposed approach allows us to mitigate the challenges mentioned.\n","authors":["Klaudia Balcer","Piotr Lipinski"],"pdf_url":"https://arxiv.org/pdf/2504.10005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09984v1","updated":"2025-04-14T08:51:35Z","published":"2025-04-14T08:51:35Z","title":"On Precomputation and Caching in Information Retrieval Experiments with\n  Pipeline Architectures","summary":"  Modern information retrieval systems often rely on multiple components\nexecuted in a pipeline. In a research setting, this can lead to substantial\nredundant computations (e.g., retrieving the same query multiple times for\nevaluating different downstream rerankers). To overcome this, researchers take\ncached \"result\" files as inputs, which represent the output of another\npipeline. However, these result files can be brittle and can cause a disconnect\nbetween the conceptual design of the pipeline and its logical implementation.\nTo overcome both the redundancy problem (when executing complete pipelines) and\nthe disconnect problem (when relying on intermediate result files), we describe\nour recent efforts to improve the caching capabilities in the open-source\nPyTerrier IR platform. We focus on two main directions: (1) automatic implicit\ncaching of common pipeline prefixes when comparing systems and (2) explicit\ncaching of operations through a new extension package, pyterrier-caching. These\napproaches allow for the best of both worlds: pipelines can be fully expressed\nend-to-end, while also avoiding redundant computations between pipelines.\n","authors":["Sean MacAvaney","Craig Macdonald"],"pdf_url":"https://arxiv.org/pdf/2504.09984v1.pdf","comment":"WOWS @ ECIR 2025"},{"id":"http://arxiv.org/abs/2502.17494v5","updated":"2025-04-14T07:39:56Z","published":"2025-02-20T22:35:52Z","title":"External Large Foundation Model: How to Efficiently Serve Trillions of\n  Parameters for Online Ads Recommendation","summary":"  Ads recommendation is a prominent service of online advertising systems and\nhas been actively studied. Recent studies indicate that scaling-up and advanced\ndesign of the recommendation model can bring significant performance\nimprovement. However, with a larger model scale, such prior studies have a\nsignificantly increasing gap from industry as they often neglect two\nfundamental challenges in industrial-scale applications. First, training and\ninference budgets are restricted for the model to be served, exceeding which\nmay incur latency and impair user experience. Second, large-volume data arrive\nin a streaming mode with data distributions dynamically shifting, as new\nusers/ads join and existing users/ads leave the system. We propose the External\nLarge Foundation Model (ExFM) framework to address the overlooked challenges.\nSpecifically, we develop external distillation and a data augmentation system\n(DAS) to control the computational cost of training/inference while maintaining\nhigh performance. We design the teacher in a way like a foundation model (FM)\nthat can serve multiple students as vertical models (VMs) to amortize its\nbuilding cost. We propose Auxiliary Head and Student Adapter to mitigate the\ndata distribution gap between FM and VMs caused by the streaming data issue.\nComprehensive experiments on internal industrial-scale applications and public\ndatasets demonstrate significant performance gain by ExFM.\n","authors":["Mingfu Liang","Xi Liu","Rong Jin","Boyang Liu","Qiuling Suo","Qinghai Zhou","Song Zhou","Laming Chen","Hua Zheng","Zhiyuan Li","Shali Jiang","Jiyan Yang","Xiaozhen Xia","Fan Yang","Yasmine Badr","Ellie Wen","Shuyu Xu","Hansey Chen","Zhengyu Zhang","Jade Nie","Chunzhi Yang","Zhichen Zeng","Weilin Zhang","Xingliang Huang","Qianru Li","Shiquan Wang","Evelyn Lyu","Wenjing Lu","Rui Zhang","Wenjun Wang","Jason Rudy","Mengyue Hang","Kai Wang","Yinbin Ma","Shuaiwen Wang","Sihan Zeng","Tongyi Tang","Xiaohan Wei","Longhao Jin","Jamey Zhang","Marcus Chen","Jiayi Zhang","Angie Huang","Chi Zhang","Zhengli Zhao","Jared Yang","Qiang Jin","Xian Chen","Amit Anand Amlesahwaram","Lexi Song","Liang Luo","Yuchen Hao","Nan Xiao","Yavuz Yetim","Luoshang Pan","Gaoxiang Liu","Yuxi Hu","Yuzhen Huang","Jackie Xu","Rich Zhu","Xin Zhang","Yiqun Liu","Hang Yin","Yuxin Chen","Buyun Zhang","Xiaoyi Liu","Xingyuan Wang","Wenguang Mao","Zhijing Li","Zhehui Zhou","Feifan Gu","Qin Huang","Chonglin Sun","Nancy Yu","Shuo Gu","Shupin Mao","Benjamin Au","Jingzheng Qin","Peggy Yao","Jae-Woo Choi","Bin Gao","Ernest Wang","Lei Zhang","Wen-Yen Chen","Ted Lee","Jay Zha","Yi Meng","Alex Gong","Edison Gao","Alireza Vahdatpour","Yiping Han","Yantao Yao","Toshinari Kureha","Shuo Chang","Musharaf Sultan","John Bocharov","Sagar Chordia","Xiaorui Gan","Peng Sun","Rocky Liu","Bo Long","Wenlin Chen","Santanu Kolay","Huayu Li"],"pdf_url":"https://arxiv.org/pdf/2502.17494v5.pdf","comment":"Accepted by the ACM Web Conference (WWW) 2025 Industrial Track as\n  Oral Presentation"},{"id":"http://arxiv.org/abs/2504.09935v1","updated":"2025-04-14T06:54:49Z","published":"2025-04-14T06:54:49Z","title":"Constrained Auto-Regressive Decoding Constrains Generative Retrieval","summary":"  Generative retrieval seeks to replace traditional search index data\nstructures with a single large-scale neural network, offering the potential for\nimproved efficiency and seamless integration with generative large language\nmodels. As an end-to-end paradigm, generative retrieval adopts a learned\ndifferentiable search index to conduct retrieval by directly generating\ndocument identifiers through corpus-specific constrained decoding. The\ngeneralization capabilities of generative retrieval on out-of-distribution\ncorpora have gathered significant attention.\n  In this paper, we examine the inherent limitations of constrained\nauto-regressive generation from two essential perspectives: constraints and\nbeam search. We begin with the Bayes-optimal setting where the generative\nretrieval model exactly captures the underlying relevance distribution of all\npossible documents. Then we apply the model to specific corpora by simply\nadding corpus-specific constraints. Our main findings are two-fold: (i) For the\neffect of constraints, we derive a lower bound of the error, in terms of the KL\ndivergence between the ground-truth and the model-predicted step-wise marginal\ndistributions. (ii) For the beam search algorithm used during generation, we\nreveal that the usage of marginal distributions may not be an ideal approach.\nThis paper aims to improve our theoretical understanding of the generalization\ncapabilities of the auto-regressive decoding retrieval paradigm, laying a\nfoundation for its limitations and inspiring future advancements toward more\nrobust and generalizable generative retrieval.\n","authors":["Shiguang Wu","Zhaochun Ren","Xin Xin","Jiyuan Yang","Mengqi Zhang","Zhumin Chen","Maarten de Rijke","Pengjie Ren"],"pdf_url":"https://arxiv.org/pdf/2504.09935v1.pdf","comment":"13 pages, 6 figures, 2 tables, accepted by SIGIR 2025 (Proceedings of\n  the 48th International ACM SIGIR Conference on Research and Development in\n  Information Retrieval)"},{"id":"http://arxiv.org/abs/2504.09915v1","updated":"2025-04-14T06:24:56Z","published":"2025-04-14T06:24:56Z","title":"StePO-Rec: Towards Personalized Outfit Styling Assistant via\n  Knowledge-Guided Multi-Step Reasoning","summary":"  Advancements in Generative AI offers new opportunities for FashionAI,\nsurpassing traditional recommendation systems that often lack transparency and\nstruggle to integrate expert knowledge, leaving the potential for personalized\nfashion styling remain untapped. To address these challenges, we present PAFA\n(Principle-Aware Fashion), a multi-granular knowledge base that organizes\nprofessional styling expertise into three levels of metadata, domain\nprinciples, and semantic relationships. Using PAFA, we develop StePO-Rec, a\nknowledge-guided method for multi-step outfit recommendation. StePO-Rec\nprovides structured suggestions using a scenario-dimension-attribute framework,\nemploying recursive tree construction to align recommendations with both\nprofessional principles and individual preferences. A preference-trend\nre-ranking system further adapts to fashion trends while maintaining the\nconsistency of the user's original style. Experiments on the widely used\npersonalized outfit dataset IQON show a 28% increase in Recall@1 and 32.8% in\nMAP. Furthermore, case studies highlight improved explainability, traceability,\nresult reliability, and the seamless integration of expertise and\npersonalization.\n","authors":["Yuxi Bi","Yunfan Gao","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04843v2","updated":"2025-04-14T06:01:59Z","published":"2025-04-07T08:56:16Z","title":"Data Augmentation as Free Lunch: Exploring the Test-Time Augmentation\n  for Sequential Recommendation","summary":"  Data augmentation has become a promising method of mitigating data sparsity\nin sequential recommendation. Existing methods generate new yet effective data\nduring model training to improve performance. However, deploying them requires\nretraining, architecture modification, or introducing additional learnable\nparameters. The above steps are time-consuming and costly for well-trained\nmodels, especially when the model scale becomes large. In this work, we explore\nthe test-time augmentation (TTA) for sequential recommendation, which augments\nthe inputs during the model inference and then aggregates the model's\npredictions for augmented data to improve final accuracy. It avoids significant\ntime and cost overhead from loss calculation and backward propagation. We first\nexperimentally disclose the potential of existing augmentation operators for\nTTA and find that the Mask and Substitute consistently achieve better\nperformance. Further analysis reveals that these two operators are effective\nbecause they retain the original sequential pattern while adding appropriate\nperturbations. Meanwhile, we argue that these two operators still face\ntime-consuming item selection or interference information from mask tokens.\nBased on the analysis and limitations, we present TNoise and TMask. The former\ninjects uniform noise into the original representation, avoiding the\ncomputational overhead of item selection. The latter blocks mask token from\nparticipating in model calculations or directly removes interactions that\nshould have been replaced with mask tokens. Comprehensive experiments\ndemonstrate the effectiveness, efficiency, and generalizability of our method.\nWe provide an anonymous implementation at https://github.com/KingGugu/TTA4SR.\n","authors":["Yizhou Dang","Yuting Liu","Enneng Yang","Minhan Huang","Guibing Guo","Jianzhe Zhao","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2504.04843v2.pdf","comment":"Accepted by SIGIR 2025 Full Paper"},{"id":"http://arxiv.org/abs/2504.09877v1","updated":"2025-04-14T04:57:49Z","published":"2025-04-14T04:57:49Z","title":"Constructing Micro Knowledge Graphs from Technical Support Documents","summary":"  Short technical support pages such as IBM Technotes are quite common in\ntechnical support domain. These pages can be very useful as the knowledge\nsources for technical support applications such as chatbots, search engines and\nquestion-answering (QA) systems. Information extracted from documents to drive\ntechnical support applications is often stored in the form of Knowledge Graph\n(KG). Building KGs from a large corpus of documents poses a challenge of\ngranularity because a large number of entities and actions are present in each\npage. The KG becomes virtually unusable if all entities and actions from these\npages are stored in the KG. Therefore, only key entities and actions from each\npage are extracted and stored in the KG. This approach however leads to loss of\nknowledge represented by entities and actions left out of the KG as they are no\nlonger available to graph search and reasoning functions. We propose a set of\ntechniques to create micro knowledge graph (micrograph) for each of such web\npages. The micrograph stores all the entities and actions in a page and also\ntakes advantage of the structure of the page to represent exactly in which part\nof that page these entities and actions appeared, and also how they relate to\neach other. These micrographs can be used as additional knowledge sources by\ntechnical support applications. We define schemas for representing\nsemi-structured and plain text knowledge present in the technical support web\npages. Solutions in technical support domain include procedures made of steps.\nWe also propose a technique to extract procedures from these webpages and the\nschemas to represent them in the micrographs. We also discuss how technical\nsupport applications can take advantage of the micrographs.\n","authors":["Atul Kumar","Nisha Gupta","Saswati Dana"],"pdf_url":"https://arxiv.org/pdf/2504.09877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04400v2","updated":"2025-04-14T02:51:58Z","published":"2025-04-06T08:03:03Z","title":"Pre-training Generative Recommender with Multi-Identifier Item\n  Tokenization","summary":"  Generative recommendation autoregressively generates item identifiers to\nrecommend potential items. Existing methods typically adopt a one-to-one\nmapping strategy, where each item is represented by a single identifier.\nHowever, this scheme poses issues, such as suboptimal semantic modeling for\nlow-frequency items and limited diversity in token sequence data. To overcome\nthese limitations, we propose MTGRec, which leverages Multi-identifier item\nTokenization to augment token sequence data for Generative Recommender\npre-training. Our approach involves two key innovations: multi-identifier item\ntokenization and curriculum recommender pre-training. For multi-identifier item\ntokenization, we leverage the RQ-VAE as the tokenizer backbone and treat model\ncheckpoints from adjacent training epochs as semantically relevant tokenizers.\nThis allows each item to be associated with multiple identifiers, enabling a\nsingle user interaction sequence to be converted into several token sequences\nas different data groups. For curriculum recommender pre-training, we introduce\na curriculum learning scheme guided by data influence estimation, dynamically\nadjusting the sampling probability of each data group during recommender\npre-training. After pre-training, we fine-tune the model using a single\ntokenizer to ensure accurate item identification for recommendation. Extensive\nexperiments on three public benchmark datasets demonstrate that MTGRec\nsignificantly outperforms both traditional and generative recommendation\nbaselines in terms of effectiveness and scalability.\n","authors":["Bowen Zheng","Enze Liu","Zhongfu Chen","Zhongrui Ma","Yue Wang","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2504.04400v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04405v2","updated":"2025-04-14T02:50:20Z","published":"2025-04-06T08:07:49Z","title":"Universal Item Tokenization for Transferable Generative Recommendation","summary":"  Recently, generative recommendation has emerged as a promising paradigm,\nattracting significant research attention. The basic framework involves an item\ntokenizer, which represents each item as a sequence of codes serving as its\nidentifier, and a generative recommender that predicts the next item by\nautoregressively generating the target item identifier. However, in existing\nmethods, both the tokenizer and the recommender are typically domain-specific,\nlimiting their ability for effective transfer or adaptation to new domains. To\nthis end, we propose UTGRec, a Universal item Tokenization approach for\ntransferable Generative Recommendation. Specifically, we design a universal\nitem tokenizer for encoding rich item semantics by adapting a multimodal large\nlanguage model (MLLM). By devising tree-structured codebooks, we discretize\ncontent representations into corresponding codes for item tokenization. To\neffectively learn the universal item tokenizer on multiple domains, we\nintroduce two key techniques in our approach. For raw content reconstruction,\nwe employ dual lightweight decoders to reconstruct item text and images from\ndiscrete representations to capture general knowledge embedded in the content.\nFor collaborative knowledge integration, we assume that co-occurring items are\nsimilar and integrate collaborative signals through co-occurrence alignment and\nreconstruction. Finally, we present a joint learning framework to pre-train and\nadapt the transferable generative recommender across multiple domains.\nExtensive experiments on four public datasets demonstrate the superiority of\nUTGRec compared to both traditional and generative recommendation baselines.\n","authors":["Bowen Zheng","Hongyu Lu","Yu Chen","Wayne Xin Zhao","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2504.04405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09823v1","updated":"2025-04-14T02:47:23Z","published":"2025-04-14T02:47:23Z","title":"RAKG:Document-level Retrieval Augmented Knowledge Graph Construction","summary":"  With the rise of knowledge graph based retrieval-augmented generation (RAG)\ntechniques such as GraphRAG and Pike-RAG, the role of knowledge graphs in\nenhancing the reasoning capabilities of large language models (LLMs) has become\nincreasingly prominent. However, traditional Knowledge Graph Construction (KGC)\nmethods face challenges like complex entity disambiguation, rigid schema\ndefinition, and insufficient cross-document knowledge integration. This paper\nfocuses on the task of automatic document-level knowledge graph construction.\nIt proposes the Document-level Retrieval Augmented Knowledge Graph Construction\n(RAKG) framework. RAKG extracts pre-entities from text chunks and utilizes\nthese pre-entities as queries for RAG, effectively addressing the issue of\nlong-context forgetting in LLMs and reducing the complexity of Coreference\nResolution. In contrast to conventional KGC methods, RAKG more effectively\ncaptures global information and the interconnections among disparate nodes,\nthereby enhancing the overall performance of the model. Additionally, we\ntransfer the RAG evaluation framework to the KGC field and filter and evaluate\nthe generated knowledge graphs, thereby avoiding incorrectly generated entities\nand relationships caused by hallucinations in LLMs. We further developed the\nMINE dataset by constructing standard knowledge graphs for each article and\nexperimentally validated the performance of RAKG. The results show that RAKG\nachieves an accuracy of 95.91 % on the MINE dataset, a 6.2 % point improvement\nover the current best baseline, GraphRAG (89.71 %). The code is available at\nhttps://github.com/LMMApplication/RAKG.\n","authors":["Hairong Zhang","Jiaheng Si","Guohang Yan","Boyuan Qi","Pinlong Cai","Song Mao","Ding Wang","Botian Shi"],"pdf_url":"https://arxiv.org/pdf/2504.09823v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2504.09816v1","updated":"2025-04-14T02:35:00Z","published":"2025-04-14T02:35:00Z","title":"Augmented Relevance Datasets with Fine-Tuned Small LLMs","summary":"  Building high-quality datasets and labeling query-document relevance are\nessential yet resource-intensive tasks, requiring detailed guidelines and\nsubstantial effort from human annotators. This paper explores the use of small,\nfine-tuned large language models (LLMs) to automate relevance assessment, with\na focus on improving ranking models' performance by augmenting their training\ndataset. We fine-tuned small LLMs to enhance relevance assessments, thereby\nimproving dataset creation quality for downstream ranking model training. Our\nexperiments demonstrate that these fine-tuned small LLMs not only outperform\ncertain closed source models on our dataset but also lead to substantial\nimprovements in ranking model performance. These results highlight the\npotential of leveraging small LLMs for efficient and scalable dataset\naugmentation, providing a practical solution for search engine optimization.\n","authors":["Quentin Fitte-Rey","Matyas Amrouche","Romain Deveaud"],"pdf_url":"https://arxiv.org/pdf/2504.09816v1.pdf","comment":"10 pages, 3 figures, and 6 tables. Accepted and presented to LLM4EVAL\n  at WSDM '25"},{"id":"http://arxiv.org/abs/2402.13840v2","updated":"2025-04-14T02:13:08Z","published":"2024-02-21T14:38:02Z","title":"Multi-view Intent Learning and Alignment with Large Language Models for\n  Session-based Recommendation","summary":"  Session-based recommendation (SBR) methods often rely on user behavior data,\nwhich can struggle with the sparsity of session data, limiting performance.\nResearchers have identified that beyond behavioral signals, rich semantic\ninformation in item descriptions is crucial for capturing hidden user intent.\nWhile large language models (LLMs) offer new ways to leverage this semantic\ndata, the challenges of session anonymity, short-sequence nature, and high LLM\ntraining costs have hindered the development of a lightweight, efficient LLM\nframework for SBR.\n  To address the above challenges, we propose an LLM-enhanced SBR framework\nthat integrates semantic and behavioral signals from multiple views. This\ntwo-stage framework leverages the strengths of both LLMs and traditional SBR\nmodels while minimizing training costs. In the first stage, we use multi-view\nprompts to infer latent user intentions at the session semantic level,\nsupported by an intent localization module to alleviate LLM hallucinations. In\nthe second stage, we align and unify these semantic inferences with behavioral\nrepresentations, effectively merging insights from both large and small models.\nExtensive experiments on two real datasets demonstrate that the LLM4SBR\nframework can effectively improve model performance. We release our codes along\nwith the baselines at https://github.com/tsinghua-fib-lab/LLM4SBR.\n","authors":["Shutong Qiao","Wei Zhou","Junhao Wen","Chen Gao","Qun Luo","Peixuan Chen","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.13840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09795v1","updated":"2025-04-14T01:50:33Z","published":"2025-04-14T01:50:33Z","title":"VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents","summary":"  We aim to develop a retrieval-augmented generation (RAG) framework that\nanswers questions over a corpus of visually-rich documents presented in mixed\nmodalities (e.g., charts, tables) and diverse formats (e.g., PDF, PPTX). In\nthis paper, we introduce a new RAG framework, VDocRAG, which can directly\nunderstand varied documents and modalities in a unified image format to prevent\nmissing information that occurs by parsing documents to obtain text. To improve\nthe performance, we propose novel self-supervised pre-training tasks that adapt\nlarge vision-language models for retrieval by compressing visual information\ninto dense token representations while aligning them with textual content in\ndocuments. Furthermore, we introduce OpenDocVQA, the first unified collection\nof open-domain document visual question answering datasets, encompassing\ndiverse document types and formats. OpenDocVQA provides a comprehensive\nresource for training and evaluating retrieval and question answering models on\nvisually-rich documents in an open-domain setting. Experiments show that\nVDocRAG substantially outperforms conventional text-based RAG and has strong\ngeneralization capability, highlighting the potential of an effective RAG\nparadigm for real-world documents.\n","authors":["Ryota Tanaka","Taichi Iki","Taku Hasegawa","Kyosuke Nishida","Kuniko Saito","Jun Suzuki"],"pdf_url":"https://arxiv.org/pdf/2504.09795v1.pdf","comment":"Accepted by CVPR 2025; project page: https://vdocrag.github.io"},{"id":"http://arxiv.org/abs/2504.08256v2","updated":"2025-04-14T01:31:40Z","published":"2025-04-11T04:55:50Z","title":"RAG-VR: Leveraging Retrieval-Augmented Generation for 3D Question\n  Answering in VR Environments","summary":"  Recent advances in large language models (LLMs) provide new opportunities for\ncontext understanding in virtual reality (VR). However, VR contexts are often\nhighly localized and personalized, limiting the effectiveness of\ngeneral-purpose LLMs. To address this challenge, we present RAG-VR, the first\n3D question-answering system for VR that incorporates retrieval-augmented\ngeneration (RAG), which augments an LLM with external knowledge retrieved from\na localized knowledge database to improve the answer quality. RAG-VR includes a\npipeline for extracting comprehensive knowledge about virtual environments and\nuser conditions for accurate answer generation. To ensure efficient retrieval,\nRAG-VR offloads the retrieval process to a nearby edge server and uses only\nessential information during retrieval. Moreover, we train the retriever to\neffectively distinguish among relevant, irrelevant, and hard-to-differentiate\ninformation in relation to questions. RAG-VR improves answer accuracy by\n17.9%-41.8% and reduces end-to-end latency by 34.5%-47.3% compared with two\nbaseline systems.\n","authors":["Shiyi Ding","Ying Chen"],"pdf_url":"https://arxiv.org/pdf/2504.08256v2.pdf","comment":"GenAI-XR 2025 Workshop, co-located with 2025 IEEE Conference on\n  Virtual Reality and 3D User Interfaces (VR)"}],"Multimedia":[{"id":"http://arxiv.org/abs/2504.10739v1","updated":"2025-04-14T22:17:55Z","published":"2025-04-14T22:17:55Z","title":"HippoMM: Hippocampal-inspired Multimodal Memory for Long Audiovisual\n  Event Understanding","summary":"  Comprehending extended audiovisual experiences remains a fundamental\nchallenge for computational systems. Current approaches struggle with temporal\nintegration and cross-modal associations that humans accomplish effortlessly\nthrough hippocampal-cortical networks. We introduce HippoMM, a\nbiologically-inspired architecture that transforms hippocampal mechanisms into\ncomputational advantages for multimodal understanding. HippoMM implements three\nkey innovations: (i) hippocampus-inspired pattern separation and completion\nspecifically designed for continuous audiovisual streams, (ii) short-to-long\nterm memory consolidation that transforms perceptual details into semantic\nabstractions, and (iii) cross-modal associative retrieval pathways enabling\nmodality-crossing queries. Unlike existing retrieval systems with static\nindexing schemes, HippoMM dynamically forms integrated episodic representations\nthrough adaptive temporal segmentation and dual-process memory encoding.\nEvaluations on our challenging HippoVlog benchmark demonstrate that HippoMM\nsignificantly outperforms state-of-the-art approaches (78.2% vs. 64.2%\naccuracy) while providing substantially faster response times (20.4s vs.\n112.5s). Our results demonstrate that translating neuroscientific memory\nprinciples into computational architectures provides a promising foundation for\nnext-generation multimodal understanding systems. The code and benchmark\ndataset are publicly available at https://github.com/linyueqian/HippoMM.\n","authors":["Yueqian Lin","Qinsi Wang","Hancheng Ye","Yuzhe Fu","Hai \"Helen\" Li","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2504.10739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11491v1","updated":"2025-04-14T18:54:21Z","published":"2025-04-14T18:54:21Z","title":"Attention GhostUNet++: Enhanced Segmentation of Adipose Tissue and Liver\n  in CT Images","summary":"  Accurate segmentation of abdominal adipose tissue, including subcutaneous\n(SAT) and visceral adipose tissue (VAT), along with liver segmentation, is\nessential for understanding body composition and associated health risks such\nas type 2 diabetes and cardiovascular disease. This study proposes Attention\nGhostUNet++, a novel deep learning model incorporating Channel, Spatial, and\nDepth Attention mechanisms into the Ghost UNet++ bottleneck for automated,\nprecise segmentation. Evaluated on the AATTCT-IDS and LiTS datasets, the model\nachieved Dice coefficients of 0.9430 for VAT, 0.9639 for SAT, and 0.9652 for\nliver segmentation, surpassing baseline models. Despite minor limitations in\nboundary detail segmentation, the proposed model significantly enhances feature\nrefinement, contextual understanding, and computational efficiency, offering a\nrobust solution for body composition analysis. The implementation of the\nproposed Attention GhostUNet++ model is available\nat:https://github.com/MansoorHayat777/Attention-GhostUNetPlusPlus.\n","authors":["Mansoor Hayat","Supavadee Aramvith","Subrata Bhattacharjee","Nouman Ahmad"],"pdf_url":"https://arxiv.org/pdf/2504.11491v1.pdf","comment":"Accepted for presentation in the 47th Annual International Conference\n  of the IEEE Engineering in Medicine and Biology Society (EMBC 2025)"},{"id":"http://arxiv.org/abs/2504.10443v1","updated":"2025-04-14T17:34:06Z","published":"2025-04-14T17:34:06Z","title":"Multimodal Long Video Modeling Based on Temporal Dynamic Context","summary":"  Recent advances in Large Language Models (LLMs) have led to significant\nbreakthroughs in video understanding. However, existing models still struggle\nwith long video processing due to the context length constraint of LLMs and the\nvast amount of information within the video. Although some recent methods are\ndesigned for long video understanding, they often lose crucial information\nduring token compression and struggle with additional modality like audio. In\nthis work, we propose a dynamic long video encoding method utilizing the\ntemporal relationship between frames, named Temporal Dynamic Context (TDC).\nFirstly, we segment the video into semantically consistent scenes based on\ninter-frame similarities, then encode each frame into tokens using visual-audio\nencoders. Secondly, we propose a novel temporal context compressor to reduce\nthe number of tokens within each segment. Specifically, we employ a query-based\nTransformer to aggregate video, audio, and instruction text tokens into a\nlimited set of temporal context tokens. Finally, we feed the static frame\ntokens and the temporal context tokens into the LLM for video understanding.\nFurthermore, to handle extremely long videos, we propose a training-free\nchain-of-thought strategy that progressively extracts answers from multiple\nvideo segments. These intermediate answers serve as part of the reasoning\nprocess and contribute to the final answer. We conduct extensive experiments on\ngeneral video understanding and audio-video understanding benchmarks, where our\nmethod demonstrates strong performance. The code and models are available at\nhttps://github.com/Hoar012/TDC-Video.\n","authors":["Haoran Hao","Jiaming Han","Yiyuan Zhang","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2504.10443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10258v1","updated":"2025-04-14T14:19:57Z","published":"2025-04-14T14:19:57Z","title":"XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a\n  Novel Benchmark","summary":"  Document Reading Order Recovery is a fundamental task in document image\nunderstanding, playing a pivotal role in enhancing Retrieval-Augmented\nGeneration (RAG) and serving as a critical preprocessing step for large\nlanguage models (LLMs). Existing methods often struggle with complex\nlayouts(e.g., multi-column newspapers), high-overhead interactions between\ncross-modal elements (visual regions and textual semantics), and a lack of\nrobust evaluation benchmarks. We introduce XY-Cut++, an advanced layout\nordering method that integrates pre-mask processing, multi-granularity\nsegmentation, and cross-modal matching to address these challenges. Our method\nsignificantly enhances layout ordering accuracy compared to traditional XY-Cut\ntechniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8\nBLEU overall) while maintaining simplicity and efficiency. It outperforms\nexisting baselines by up to 24\\% and demonstrates consistent accuracy across\nsimple and complex layouts on the newly introduced DocBench-100 dataset. This\nadvancement establishes a reliable foundation for document structure recovery,\nsetting a new standard for layout ordering tasks and facilitating more\neffective RAG and LLM preprocessing.\n","authors":["Shuai Liu","Youmeng Li","Jizeng Wei"],"pdf_url":"https://arxiv.org/pdf/2504.10258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10222v1","updated":"2025-04-14T13:44:11Z","published":"2025-04-14T13:44:11Z","title":"PRM-BAS: Enhancing Multimodal Reasoning through PRM-guided Beam\n  Annealing Search","summary":"  Recent work increasingly focuses on improving the reasoning capabilities of\nMultimodal Large Language Models (MLLMs). Among existing methods, Process\nReward Models (PRMs) stand out for offering dense, step-wise supervision to\nguide intermediate reasoning. However, how to effectively integrate PRMs into\nsearch strategies remains an open question. In this paper, we introduce PRM-BAS\n(PRM-Guided Beam Annealing Search), a lightweight approach for PRM-guided\nreasoning that dynamically adjusts beam size -- starting with a broader search\nspace and gradually narrowing it as contextual information accumulates, thereby\nbalancing performance and efficiency. We further propose a unified framework\nfor data construction and PRM training. Specifically, we construct the\nPRM-BAS-300k dataset by selecting 300k questions from existing datasets and\nperforming rollouts at each step to estimate the probability of reaching a\ncorrect final answer. The PRM is then trained using a combination of value loss\nfor absolute action quality and rank loss for relative action quality.\nExtensive experiments on challenging multimodal reasoning benchmarks\ndemonstrate that PRM-BAS significantly improves reasoning performance while\nmaintaining low computational cost. Moreover, it generalizes well across\ndifferent model scales and architectures, showcasing strong robustness and\nplug-and-play capability.\n","authors":["Pengfei Hu","Zhenrong Zhang","Qikai Chang","Shuhang Liu","Jiefeng Ma","Jun Du","Jianshu Zhang","Quan Liu","Jianqing Gao","Feng Ma","Qingfeng Liu"],"pdf_url":"https://arxiv.org/pdf/2504.10222v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10166v1","updated":"2025-04-14T12:21:27Z","published":"2025-04-14T12:21:27Z","title":"Fact-Checking with Contextual Narratives: Leveraging Retrieval-Augmented\n  LLMs for Social Media Analysis","summary":"  We propose CRAVE (Cluster-based Retrieval Augmented Verification with\nExplanation); a novel framework that integrates retrieval-augmented Large\nLanguage Models (LLMs) with clustering techniques to address fact-checking\nchallenges on social media. CRAVE automatically retrieves multimodal evidence\nfrom diverse, often contradictory, sources. Evidence is clustered into coherent\nnarratives, and evaluated via an LLM-based judge to deliver fact-checking\nverdicts explained by evidence summaries. By synthesizing evidence from both\ntext and image modalities and incorporating agent-based refinement, CRAVE\nensures consistency and diversity in evidence representation. Comprehensive\nexperiments demonstrate CRAVE's efficacy in retrieval precision, clustering\nquality, and judgment accuracy, showcasing its potential as a robust\ndecision-support tool for fact-checkers.\n","authors":["Arka Ujjal Dey","Muhammad Junaid Awan","Georgia Channing","Christian Schroeder de Witt","John Collomosse"],"pdf_url":"https://arxiv.org/pdf/2504.10166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.10150v1","updated":"2025-04-14T12:01:11Z","published":"2025-04-14T12:01:11Z","title":"HistLLM: A Unified Framework for LLM-Based Multimodal Recommendation\n  with User History Encoding and Compression","summary":"  While large language models (LLMs) have proven effective in leveraging\ntextual data for recommendations, their application to multimodal\nrecommendation tasks remains relatively underexplored. Although LLMs can\nprocess multimodal information through projection functions that map visual\nfeatures into their semantic space, recommendation tasks often require\nrepresenting users' history interactions through lengthy prompts combining text\nand visual elements, which not only hampers training and inference efficiency\nbut also makes it difficult for the model to accurately capture user\npreferences from complex and extended prompts, leading to reduced\nrecommendation performance. To address this challenge, we introduce HistLLM, an\ninnovative multimodal recommendation framework that integrates textual and\nvisual features through a User History Encoding Module (UHEM), compressing\nmultimodal user history interactions into a single token representation,\neffectively facilitating LLMs in processing user preferences. Extensive\nexperiments demonstrate the effectiveness and efficiency of our proposed\nmechanism.\n","authors":["Chen Zhang","Bo Hu","Weidong Chen","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2504.10150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.15166v2","updated":"2025-04-14T08:38:46Z","published":"2025-03-19T12:47:37Z","title":"Machine Unlearning in Hyperbolic vs. Euclidean Multimodal Contrastive\n  Learning: Adapting Alignment Calibration to MERU","summary":"  Machine unlearning methods have become increasingly important for selective\nconcept removal in large pre-trained models. While recent work has explored\nunlearning in Euclidean contrastive vision-language models, the effectiveness\nof concept removal in hyperbolic spaces remains unexplored. This paper\ninvestigates machine unlearning in hyperbolic contrastive learning by adapting\nAlignment Calibration to MERU, a model that embeds images and text in\nhyperbolic space to better capture semantic hierarchies. Through systematic\nexperiments and ablation studies, we demonstrate that hyperbolic geometry\noffers distinct advantages for concept removal, achieving near perfect\nforgetting with reasonable performance on retained concepts, particularly when\nscaling to multiple concept removal. Our approach introduces\nhyperbolic-specific components including entailment calibration and norm\nregularization that leverage the unique properties of hyperbolic space.\nComparative analysis with Euclidean models reveals fundamental differences in\nunlearning dynamics, with hyperbolic unlearning reorganizing the semantic\nhierarchy while Euclidean approaches merely disconnect cross-modal\nassociations. These findings not only advance machine unlearning techniques but\nalso provide insights into the geometric properties that influence concept\nrepresentation and removal in multimodal models. Source code available at\nhttps://github.com/alex-pv01/HAC\n","authors":["Àlex Pujol Vidal","Sergio Escalera","Kamal Nasrollahi","Thomas B. Moeslund"],"pdf_url":"https://arxiv.org/pdf/2503.15166v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2504.09948v1","updated":"2025-04-14T07:18:32Z","published":"2025-04-14T07:18:32Z","title":"Omni-Dish: Photorealistic and Faithful Image Generation and Editing for\n  Arbitrary Chinese Dishes","summary":"  Dish images play a crucial role in the digital era, with the demand for\nculturally distinctive dish images continuously increasing due to the\ndigitization of the food industry and e-commerce. In general cases, existing\ntext-to-image generation models excel in producing high-quality images;\nhowever, they struggle to capture diverse characteristics and faithful details\nof specific domains, particularly Chinese dishes. To address this limitation,\nwe propose Omni-Dish, the first text-to-image generation model specifically\ntailored for Chinese dishes. We develop a comprehensive dish curation pipeline,\nbuilding the largest dish dataset to date. Additionally, we introduce a\nrecaption strategy and employ a coarse-to-fine training scheme to help the\nmodel better learn fine-grained culinary nuances. During inference, we enhance\nthe user's textual input using a pre-constructed high-quality caption library\nand a large language model, enabling more photorealistic and faithful image\ngeneration. Furthermore, to extend our model's capability for dish editing\ntasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish\nediting dataset and train a specialized editing model. Extensive experiments\ndemonstrate the superiority of our methods.\n","authors":["Huijie Liu","Bingcan Wang","Jie Hu","Xiaoming Wei","Guoliang Kang"],"pdf_url":"https://arxiv.org/pdf/2504.09948v1.pdf","comment":"10 pages, 10 figures, 3 tables"},{"id":"http://arxiv.org/abs/2504.09915v1","updated":"2025-04-14T06:24:56Z","published":"2025-04-14T06:24:56Z","title":"StePO-Rec: Towards Personalized Outfit Styling Assistant via\n  Knowledge-Guided Multi-Step Reasoning","summary":"  Advancements in Generative AI offers new opportunities for FashionAI,\nsurpassing traditional recommendation systems that often lack transparency and\nstruggle to integrate expert knowledge, leaving the potential for personalized\nfashion styling remain untapped. To address these challenges, we present PAFA\n(Principle-Aware Fashion), a multi-granular knowledge base that organizes\nprofessional styling expertise into three levels of metadata, domain\nprinciples, and semantic relationships. Using PAFA, we develop StePO-Rec, a\nknowledge-guided method for multi-step outfit recommendation. StePO-Rec\nprovides structured suggestions using a scenario-dimension-attribute framework,\nemploying recursive tree construction to align recommendations with both\nprofessional principles and individual preferences. A preference-trend\nre-ranking system further adapts to fashion trends while maintaining the\nconsistency of the user's original style. Experiments on the widely used\npersonalized outfit dataset IQON show a 28% increase in Recall@1 and 32.8% in\nMAP. Furthermore, case studies highlight improved explainability, traceability,\nresult reliability, and the seamless integration of expertise and\npersonalization.\n","authors":["Yuxi Bi","Yunfan Gao","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2504.09915v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09906v1","updated":"2025-04-14T06:02:41Z","published":"2025-04-14T06:02:41Z","title":"Plasticity-Aware Mixture of Experts for Learning Under QoE Shifts in\n  Adaptive Video Streaming","summary":"  Adaptive video streaming systems are designed to optimize Quality of\nExperience (QoE) and, in turn, enhance user satisfaction. However, differences\nin user profiles and video content lead to different weights for QoE factors,\nresulting in user-specific QoE functions and, thus, varying optimization\nobjectives. This variability poses significant challenges for neural networks,\nas they often struggle to generalize under evolving targets - a phenomenon\nknown as plasticity loss that prevents conventional models from adapting\neffectively to changing optimization objectives. To address this limitation, we\npropose the Plasticity-Aware Mixture of Experts (PA-MoE), a novel learning\nframework that dynamically modulates network plasticity by balancing memory\nretention with selective forgetting. In particular, PA-MoE leverages noise\ninjection to promote the selective forgetting of outdated knowledge, thereby\nendowing neural networks with enhanced adaptive capabilities. In addition, we\npresent a rigorous theoretical analysis of PA-MoE by deriving a regret bound\nthat quantifies its learning performance. Experimental evaluations demonstrate\nthat PA-MoE achieves a 45.5% improvement in QoE over competitive baselines in\ndynamic streaming environments. Further analysis reveals that the model\neffectively mitigates plasticity loss by optimizing neuron utilization.\nFinally, a parameter sensitivity study is performed by injecting varying levels\nof noise, and the results align closely with our theoretical predictions.\n","authors":["Zhiqiang He","Zhi Liu"],"pdf_url":"https://arxiv.org/pdf/2504.09906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.09835v1","updated":"2025-04-14T03:03:42Z","published":"2025-04-14T03:03:42Z","title":"Laugh at Your Own Pace: Basic Performance Evaluation of Language\n  Learning Assistance by Adjustment of Video Playback Speeds Based on Laughter\n  Detection","summary":"  Among various methods to learn a second language (L2), such as listening and\nshadowing, Extensive Viewing involves learning L2 by watching many videos.\nHowever, it is difficult for many L2 learners to smoothly and effortlessly\ncomprehend video contents made for native speakers at the original speed.\nTherefore, we developed a language learning assistance system that\nautomatically adjusts the playback speed according to the learner's\ncomprehension. Our system judges that learners understand the contents if they\nlaugh at the punchlines of comedy dramas, and vice versa. Experimental results\nshow that this system supports learners with relatively low L2 ability (under\n700 in TOEIC Score in the experimental condition) to understand video contents.\nOur system can widen learners' possible options of native speakers' videos as\nExtensive Viewing material.\n","authors":["Naoto Nishida","Hinako Nozaki","Buntarou Shizuki"],"pdf_url":"https://arxiv.org/pdf/2504.09835v1.pdf","comment":"6 pages, 5 figures"}]}}